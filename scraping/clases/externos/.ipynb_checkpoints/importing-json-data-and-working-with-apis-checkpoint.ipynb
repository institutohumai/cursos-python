{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "dhs_url = \"https://assets.datacamp.com/production/repositories/4412/datasets/29672336a81043615a204add894bb2bc1b289b16/dhs_counts.json\"\n",
    "urlretrieve(dhs_url, \"dhs_daily_report.json\")\n",
    "\n",
    "# Print JSON file to console\n",
    "with open(\"dhs_daily_report.json\") as f:\n",
    "  text = f.read()\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a data frame\n",
    "pop_in_shelters = pd.read_json(\"dhs_daily_report.json\")\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Many open data portals make available JSONs datasets that are particularly easy to parse. They can be accessed directly via URL. Each object is a record, all objects have the same set of attributes, and none of the values are nested objects that themselves need to be parsed.</p>\n",
    "<p>The New York City Department of Homeless Services Daily Report is such a dataset, containing years' worth of homeless shelter population counts. You can view it in the console before loading it to a data frame with <code>pandas</code>'s <code>read_json()</code> function.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Get a sense of the contents of <code>dhs_daily_report.json</code>, which are printed in the console.</li>\n",
    "<li>Load <code>pandas</code> as <code>pd</code>.</li>\n",
    "<li>Use <code>read_json()</code> to load <code>dhs_daily_report.json</code> to a data frame, <code>pop_in_shelters</code>.</li>\n",
    "<li>View summary statistics about <code>pop_in_shelters</code> with the data frame's <code>describe()</code> method.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Make sure the JSON file was correctly passed to <code>read_json()</code>, with no typos or keyword arguments.</li>\n",
    "<li><code>describe()</code> is a DataFrame method, not a <code>pandas</code> function -- be sure to use dot notation with the data frame name.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with JSON orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "dhs_split_url = \"https://assets.datacamp.com/production/repositories/4412/datasets/d58e20d4eff661e6c01d89a105c5e01a137c6b6d/dhs_counts_split.json\"\n",
    "urlretrieve(dhs_split_url, 'dhs_report_reformatted.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>JSON isn't a tabular format, so <code>pandas</code> makes assumptions about its orientation when loading data. Most JSON data you encounter will be in orientations that <code>pandas</code> can automatically transform into a data frame.</p>\n",
    "<p>Sometimes, like in this modified version of the Department of Homeless Services Daily Report, data is oriented differently. To reduce the file size, it has been <code>split</code> formatted. You'll see what happens when you try to load it normally versus with the <code>orient</code> keyword argument. The <code>try/except</code> block will alert you if there are errors loading the data.</p>\n",
    "<p><code>pandas</code> has been loaded as <code>pd</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import requests\n",
    "from urllib.request import urlretrieve\n",
    "url = \"https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt\"\n",
    "urlretrieve(url, \"yelp_api_key.txt\")\n",
    "\n",
    "with open(\"yelp_api_key.txt\", \"r\") as f:\n",
    "  api_key = f.readlines()[0]\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "params = {\"term\": \"cafe\",\n",
    "       \t  \"location\": \"NYC\"}\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/6960241321d7d91fbfe16f74d3216b58074427f9/nyc_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"nyc_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a data frame\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise, you'll use <code>requests.get()</code> to query the Yelp Business Search API for cafes in New York City. <code>requests.get()</code> needs a URL to get data from. The Yelp API also needs search parameters and authorization headers passed to the <code>params</code> and <code>headers</code> keyword arguments, respectively.</p>\n",
    "<p>You'll need to extract the data from the response with its <code>json()</code> method, and pass it to <code>pandas</code>'s <code>DataFrame()</code> function to make a data frame. Note that the necessary data is under the dictionary key <code>\"businesses\"</code>.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) and <code>requests</code> have been loaded. Authorization data is in the dictionary <code>headers</code>, and the needed API parameters are stored as <code>params</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Get data about New York City cafes from the Yelp API (<code>api_url</code>) with <code>requests.get()</code>. The necessary <code>params</code> and <code>headers</code> information has been provided.</li>\n",
    "<li>Extract the JSON data from the response with its <code>json()</code> method, and assign it to <code>data</code>.</li>\n",
    "<li>Load the cafe listings to the data frame <code>cafes</code> with <code>pandas</code>'s <code>DataFrame()</code> function. The listings are under the <code>\"businesses\"</code> key in <code>data</code>.</li>\n",
    "<li>Print the data frame's <code>dtypes</code> to see what information you're getting.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><code>json()</code> doesn't take any additional arguments.</li>\n",
    "<li>Remember to preface <code>DataFrame()</code> with its library alias, just as you would with a function like <code>read_json()</code>.</li>\n",
    "<li><code>DataFrame()</code> takes the data to load as an argument. Check that you're providing just the business data from the response.</li>\n",
    "<li>The syntax to get a dictionary value by its key is <code>dictionary[\"key\"]</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import requests\n",
    "from urllib.request import urlretrieve\n",
    "url = \"https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt\"\n",
    "urlretrieve(url, \"yelp_api_key.txt\")\n",
    "\n",
    "with open(\"yelp_api_key.txt\", \"r\") as f:\n",
    "  api_key = f.readlines()[0]\n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/6960241321d7d91fbfe16f74d3216b58074427f9/nyc_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_400_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/ff9363ccbabbd036db806f7279c4d2e54f0684a8/400_error.pkl\"\n",
    "err_404_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"nyc_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_400_path, \"400_error.pkl\")\n",
    "urllib.request.urlretrieve(err_404_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {\"term\": \"cafe\",\n",
    "          \t  \"location\": \"NYC\"}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=parameters)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print head\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Formatting parameters to get the data you need is an integral part of working with APIs. These parameters can be passed to the <code>get()</code> function's <code>params</code> keyword argument as a dictionary.</p>\n",
    "<p>The Yelp API requires the <code>location</code> parameter be set. It also lets users supply a <code>term</code> to search for. You'll use these parameters to get data about cafes in NYC, then process the result to create a data frame.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) and <code>requests</code> have been loaded. The API endpoint is stored in the variable <code>api_url</code>. Authorization data is stored in the dictionary <code>headers</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Create a dictionary, <code>parameters</code>, with the <code>term</code> and <code>location</code> parameters set to search for <code>\"cafe\"</code>s in <code>\"NYC\"</code>.</li>\n",
    "<li>Query the Yelp API (<code>api_url</code>) with <code>requests</code>'s <code>get()</code> function and the <code>headers</code> and <code>params</code> keyword arguments set. Save the result as <code>response</code>.</li>\n",
    "<li>Extract the JSON data from <code>response</code> with the appropriate method. Save the result as <code>data</code>.</li>\n",
    "<li>Load the <code>\"businesses\"</code> values in <code>data</code> to the data frame <code>cafes</code> and print the head.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The method to extract JSON data from a response is <code>json()</code>.</li>\n",
    "<li>Remember that only <code>data[\"businesses\"]</code> should be loaded to the data frame, not all of <code>data</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set request headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "url = \"https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt\"\n",
    "urlretrieve(url, \"yelp_api_key.txt\")\n",
    "\n",
    "with open(\"yelp_api_key.txt\", \"r\") as f:\n",
    "  api_key = f.readlines()[0]\n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "params = {\"term\": \"cafe\",\n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\"}\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/ef0fd6cedb81a1c6fa83240ab9337311d31150cc/top_nyc_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"top_nyc_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=params)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print names\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "print(cafes.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Many APIs require users provide an API key, obtained by registering for the service. Keys typically are passed in the request header, rather than as parameters.</p>\n",
    "<p>The <a href=\"https://www.yelp.com/developers/documentation/v3/authentication\">Yelp API documentation</a> says \"To authenticate API calls with the API Key, set the <code>Authorization</code> HTTP header value as <code>Bearer API_KEY</code>.\" </p>\n",
    "<p>You'll set up a dictionary to pass this information to <code>get()</code>, call the API for the highest-rated cafes in NYC, and parse the response.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) and <code>requests</code> have been loaded. The API endpoint is stored as <code>api_url</code>, and the key is <code>api_key</code>. Parameters are in the dictionary <code>params</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Create a dictionary, <code>headers</code>, that passes the formatted key string to the <code>\"Authorization\"</code> header value.</li>\n",
    "<li>Query the Yelp API (<code>api_url</code>) with <code>get()</code> and the necessary headers and parameters. Save the result as <code>response</code>.</li>\n",
    "<li>Extract the JSON data from <code>response</code>. Save the result as <code>data</code>.</li>\n",
    "<li>Load the <code>\"businesses\"</code> values in <code>data</code> to the data frame <code>cafes</code> and print the <code>names</code> column.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The string <code>format()</code> method is used to embed variable values, like <code>api_key</code>, in a string.</li>\n",
    "<li><code>response.get()</code> should have <code>headers</code> and <code>params</code> arguments specified.</li>\n",
    "<li>A response's <code>json()</code> method will return just the response data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten nested JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "url = \"https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt\"\n",
    "urlretrieve(url, \"yelp_api_key.txt\")\n",
    "\n",
    "with open(\"yelp_api_key.txt\", \"r\") as f:\n",
    "  api_key = f.readlines()[0]\n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "parameters = {\"term\": \"cafe\",\n",
    "          \t  \"location\": \"NYC\"}\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/6960241321d7d91fbfe16f74d3216b58074427f9/nyc_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"nyc_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################\n",
    "\n",
    "\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "                       sep=\"_\")\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A feature of JSON data is that it can be nested: an attribute's value can consist of attribute-value pairs. This nested data is more useful unpacked, or flattened, into its own data frame columns. The <code>pandas.io.json</code> submodule has a function, <code>json_normalize()</code>, that does exactly this.</p>\n",
    "<p>The Yelp API response data is nested. Your job is to flatten out the next level of data in the <code>coordinates</code> and <code>location</code> columns.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) and <code>requests</code> have been imported. The results of the API call are stored as <code>response</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Load the <code>json_normalize()</code> function from <code>pandas</code>' <code>io.json</code> submodule.</li>\n",
    "<li>Isolate the JSON data from <code>response</code> and assign it to <code>data</code>.</li>\n",
    "<li>Use <code>json_normalize()</code> to flatten and load the businesses data to a data frame, <code>cafes</code>. Set the <code>sep</code> argument to use underscores (<code>_</code>), rather than periods.</li>\n",
    "<li>Print the <code>data</code> head.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>When importing a function from a module, you do not need to include the parentheses in the name.</li>\n",
    "<li>The method to get JSON data from a <code>response</code> object is <code>json()</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle deeply nested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "from pandas.io.json import json_normalize\n",
    "url = \"https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt\"\n",
    "urlretrieve(url, \"yelp_api_key.txt\")\n",
    "\n",
    "with open(\"yelp_api_key.txt\", \"r\") as f:\n",
    "  api_key = f.readlines()[0]\n",
    "\n",
    "# Set variables for API call\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "parameters = {\"term\": \"cafe\",\n",
    "          \t  \"location\": \"NYC\"}\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/6960241321d7d91fbfe16f74d3216b58074427f9/nyc_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"nyc_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################\n",
    "\n",
    "# Get API data\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=parameters)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Last exercise, you flattened data nested down one level. Here, you'll unpack more deeply nested data.</p>\n",
    "<p>The <code>categories</code> attribute in the Yelp API response contains lists of objects. To flatten this data, you'll employ <code>json_normalize()</code> arguments to specify the path to <code>categories</code> and pick other attributes to include in the data frame. You should also change the separator to facilitate column selection and prefix the other attributes to prevent column name collisions. We'll work through this in steps.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) and <code>json_normalize()</code> have been imported. JSON-formatted Yelp data on cafes in NYC is stored as <code>data</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "url = 'https://assets.datacamp.com/production/repositories/4412/datasets/7b4d21edba07766a12341ed66b99d25670ad432c/yelp_api_key.txt'\n",
    "urlretrieve(url, 'yelp_api_key.txt')\n",
    "\n",
    "\n",
    "with open('yelp_api_key.txt', 'r') as f:\n",
    "  api_key = f.readlines()[0]\n",
    "api_url = 'https://api.yelp.com/v3/businesses/search'\n",
    "headers = {'Authorization': 'Bearer %s' % api_key}\n",
    "\n",
    "top_50_cafes = pd.read_json('https://assets.datacamp.com/production/repositories/4412/datasets/7b31d59aecf48bff27dd1743f6de83147fc3f23b/top_50_cafes.json')\n",
    "\n",
    "############################################################\n",
    "# Set up MockRequests\n",
    "#===========================================================\n",
    "import urllib.request\n",
    "from mock_request.requests import MockRequests\n",
    "\n",
    "# First, download the dat files relevant containing dictionaries\n",
    "exp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/1dd811c7b392767be29ca33877113f976afe3053/next_50_cafes.pkl\"\n",
    "avr_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/67a5ee3539b2916df0e6ebc63216c6f396693137/available_requests.dat\"\n",
    "err_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/cfc89bc158082f77e30cfa230890bc60b2e68df5/404_error.pkl\"\n",
    "erp_path = \"https://assets.datacamp.com/production/repositories/4412/datasets/aaf775650bf3386e156f27c8b0198fcc94b4d9f4/request_errors.dat\"\n",
    "\n",
    "# Download the file from URL path and save it locally\n",
    "urllib.request.urlretrieve(exp_path, \"next_50_cafes.pkl\")\n",
    "urllib.request.urlretrieve(avr_path, \"available_requests.dat\")\n",
    "urllib.request.urlretrieve(err_path, \"404_error.pkl\")\n",
    "urllib.request.urlretrieve(erp_path, \"request_errors.dat\")\n",
    "\n",
    "# Then, instantiate the MockRequests object and save it as requests\n",
    "requests = MockRequests(\"available_requests.dat\", \"request_errors.dat\")\n",
    "\n",
    "# Use requests.get as normal\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\": 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise, youâ€™ll practice appending records by creating a dataset of the 100 highest-rated cafes in New York City according to Yelp.</p>\n",
    "<p>APIs often limit the amount of data returned, since sending large datasets can be time- and resource-intensive. The Yelp Business Search API limits the results returned in a call to 50 records. However, the <code>offset</code> parameter lets a user retrieve results starting after a specified number. By modifying the offset, we can get results 1-50 in one call and 51-100 in another. Then, we can append the data frames.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>), <code>requests</code>, and <code>json_normalize()</code> have been imported. The 50 top-rated cafes are already in a data frame, <code>top_50_cafes</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Add an <code>\"offset\"</code> parameter to <code>params</code> so that the Yelp API call will get cafes 51-100.</li>\n",
    "<li>Append the results of the API call to <code>top_50_cafes</code>, setting <code>ignore_index</code> so rows will be renumbered.</li>\n",
    "<li>Print the shape of the resulting data frame, <code>cafes</code>, to confirm there are 100 records.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>If using an offset of 0 gets results 1-50, what offset will get results starting at 51?</li>\n",
    "<li>Recall that <code>append()</code> is a DataFrame method like  <code>head()</code>, not a function like <code>read_json()</code>.</li>\n",
    "<li><code>ignore_index</code> takes <code>True</code>/<code>False</code> as an argument. Did you set the right value?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "crosswalk_url = 'https://assets.datacamp.com/production/repositories/4412/datasets/ef15460ec3234617ebebb227119282bd44d49c8c/zip_to_puma.csv'\n",
    "pop_data_url = 'https://assets.datacamp.com/production/repositories/4412/datasets/e2a8074a29a987eacfe9ee49043094eefb909fcc/2016acs5yr_puma.xlsx'\n",
    "cafes_url = 'https://assets.datacamp.com/production/repositories/4412/datasets/2c7576fc1a19d7a62df027a8ed662b0db9b7f16d/cafes.json'\n",
    "urlretrieve(crosswalk_url, 'zip_to_puma.csv')\n",
    "urlretrieve(pop_data_url, 'pop_data.xlsx')\n",
    "urlretrieve(cafes_url, 'cafes.json')\n",
    "\n",
    "crosswalk = pd.read_csv('zip_to_puma.csv', dtype={'zipcode':'str', 'puma':'str'})\n",
    "pop_data = pd.read_excel('pop_data.xlsx', dtype={'puma': 'str'})\n",
    "cafes = pd.read_json('cafes.json',orient='records', dtype={'location_zip_code':'str'}, precise_float=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the last exercise, you built a dataset of the top 100 cafes in New York City according to Yelp. Now, you'll combine that with demographic data to investigate which neighborhood has the most good cafes per capita.</p>\n",
    "<p>To do this, you'll merge two datasets with the DataFrame <code>merge()</code> method. The first,<code>crosswalk</code>, is a crosswalk between ZIP codes and Public Use Micro Data Sample Areas (PUMAs), which are aggregates of census tracts and correspond roughly to NYC neighborhoods. Then, you'll merge in <code>pop_data</code>, which contains 2016 population estimates for each PUMA.</p>\n",
    "<p><code>pandas</code> (as <code>pd</code>) has been imported, as has the <code>cafes</code> data frame from last exercise.</p>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
