{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "35ea051b",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/GCP/05_Airflow/Airflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4dddb351",
            "metadata": {},
            "source": [
                "# 1. Airflow: desarrollo, scheduling y monitoring de trabajos.\n",
                "\n",
                "<a href=\"https://airflow.apache.org/\">\n",
                "<img alt=\"Logo de Airflow\" src=\"./images/AirflowLogo.png\" height=\"150px\"/>\n",
                "</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1f95d95",
            "metadata": {},
            "source": [
                "## 1.1. Introducci√≥n a conceptos b√°sicos"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a47a43b",
            "metadata": {},
            "source": [
                "### ¬øQu√© es Airflow?\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;Son las 3 de la ma√±ana y suena la alarma. Medio dormida, la persona a cargo del pipeline se levanta y se acerca su computadora. Con la mirada medio perdida, aprieta \"enter\" sobre el comando pre-escrito que ten√≠a sobre su terminal y que va a procesar los datos del dia requeridos para el dia siguiente a primera hora. Se da vuelta y se dirije de nuevo hacia la cama preguntadose si no habr√° otra manera de schedulear tareas... \n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;**Airflow** es una plataforma open-source que nos permite desarrollar, schedulear y monitorear trabajos, de manera que podemos organizarlas en flujos de tareas. Es un projecto comenzado por Airbnb haya por el 2014 para manejar los complejos flujos de trabajo que existia en una empresa de ese tama√±o y que desde su concepci√≥n fue hecho [open-source](https://github.com/apache/airflow) y que al momento de escribir este material tiene tan solo 28K estrellas en github...\n",
                "\n",
                "### ¬øP√°ra que sirve?\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Como ya mencionamos, Airflow sive para orquestar trabajos en batch en donde existe una interdependencia entre los mismos. Para esto se hace de operadores de diversos tipos que permiten conectar y utilizar una poco m√≥dica cantidad de frameworks y tecnolog√≠as.\n",
                "Esto quiere decir que si tu flujo de tareas tiene un principio y un fin bien definidos y que siempre se deben correr en un intervalo constantes, entonces se pueden **programar** en Airflow como un DAG. Hago √©nfasis en la palabra programar, porque es as√≠ como se orquestan las tareas, a trav√©s de c√≥digo python. Airflow es una herramienta que nos brinda una hermosa UI para monitoreo y gesti√≥n de los flujos de trabajo, as√≠ como de las tareas individuales que los compone, pero no est√° pensada para crear tus flujos de trabajo en la misma.\n",
                "\n",
                "| <img alt=\"Grafo de Hello World\" src=\"images/hello_world_graph_view.png\" /> |\n",
                "|:--:|\n",
                "|*Ejemplo de un simple flujo de trabajo en Airflow*|\n",
                "\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; El hecho de que los flujos de trabajo sean c√≥digo nos brinda la oportunidad de:\n",
                "* Versionar como cualquier otro c√≥digo con git.\n",
                "* Diferentes personas pueden trabajar en simultaneo en un mismo flujo de trabajo en diferentes etapas y colaborativamente en determinar el correcto flujo del mismo.\n",
                "* Se pueden escribir tests para las funcionalidad.\n",
                "* Los componentes son extensibles. Si necesit√°s agregar m√°s funcionalidad de la que ya brinda Airflow, pod√©s crear tus propios operadores.\n",
                "* El scheduling tambien es programable, por lo que si las reglas de \"cuando lanzar el flujo\" son extra√±as pero programables, se pueden schedulear en Airflow.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; El c√≥digo que genera el flujo que se mostr√≥ en la imagen anterior es el siguiente y en donde se muestra 2 maneras diferentes de especificar una tarea:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a36eee23",
            "metadata": {},
            "source": [
                "```python\n",
                "from datetime import datetime\n",
                "\n",
                "from airflow import DAG\n",
                "from airflow.decorators import task\n",
                "from airflow.operators.bash import BashOperator\n",
                "\n",
                "# Un DAG representa un flujo de trabajos, al menos una colecc√≠on \n",
                "with DAG(dag_id=\"demo\", start_date=datetime(2022, 1, 1), schedule=\"0 0 * * *\") as dag:\n",
                "\n",
                "    # Las tareas se representan con operadores\n",
                "    hello = BashOperator(task_id=\"hello\", bash_command=\"echo hola humai!\")\n",
                "\n",
                "    # Las tareas que se defininen con el decorador, son tareas de PythonOperator\n",
                "    @task()\n",
                "    def airflow():\n",
                "        print(\"hola airflow!\")\n",
                "\n",
                "    # Se establece el orden en el que deben dar las tareas del flujo\n",
                "    hello >> airflow()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "682a3c83",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b300f815",
            "metadata": {},
            "source": [
                "### 1.2 Instalaci√≥n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98df3314",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Para usar Airflow existen varias alternativas como usarlos adentro de Docker, adentro de Kubernetes o standalone de manera local.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Para instalar de manera local solamente el core (como mencionamos existe un buen n√∫mero de plugins y addons para Airflow), podemos correr el siguiente comando:\n",
                "\n",
                "```bash\n",
                "export AIRFLOW_HOME=~/airflow\n",
                "AIRFLOW_VERSION=2.4.2\n",
                "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
                "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
                "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Podemos corrobarad que todo se instal√≥ correctamente verificando la versi√≥n instalada:\n",
                "\n",
                "```console\n",
                "root@6ff3c3340c9b:/airflow# airflow version\n",
                "2.4.2\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Una vez verificado que est√° bien instalado, podemos levantar la plataforma con `airflow standalone`. Luego de inicializar el servidor web, podremos acceder a la direcci√≥n `localhost:8080` para acceder a la UI. En esa direccio√≥ encontraremos una pantalla de login.\n",
                "\n",
                "| <img alt=\"Login Airflow\" src=\"images/AirflowLogIn.png\" height=\"400px\"/> |\n",
                "|:--:|\n",
                "|*Login de Airflow*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Las credenciales para acceder se muestran en los logs de la terminal:\n",
                "\n",
                "| <img alt=\"Credenciales de login Airflow\" src=\"images/AirflowLonginCreds.png\" height=\"400px\"/> |\n",
                "|:--:|\n",
                "|*Logs de `airflow standalone`*|*\n",
                "\n",
                "**(La contrase√±a de `admin` tambien se puede encontrar en `$AIRFLOW_HOME/standalone_admin_password.txt`. Si no se toc√≥ `$AIRFLOW_HOME` -> `cat ~/airflow/standalone_admin_password.txt`)*\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Y voil√†! Estamos adentro:\n",
                "\n",
                "| <img alt=\"UI de Airflow\" src=\"images/AirflowUI.png\"/> |\n",
                "|:--:|\n",
                "|*UI de Airflow*|\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b7dbe6e",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "567be33a",
            "metadata": {},
            "source": [
                "### 1.3 Primera ejecuci√≥n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f092402",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Para empezar a generar una peque√±a intuici√≥n de como funciona Airflow, hagamos una primera ejecucion de un flujo de trabajo, en este caso `example_bash_operator`. Para esto tenemos 2 opciones:\n",
                "* O bien lanzamos una ejecucion para un flujo desde la UI: \n",
                "\n",
                "| <img alt=\"Ejecutar un Airflow\" src=\"images/ExecDag.png\"/> |\n",
                "|:--:|\n",
                "|*Ejecuci√≥n de un DAG de Airflow*|\n",
                "\n",
                "* O mismo desde la consola:\n",
                "\n",
                "```console\n",
                "root@6ff3c3340c9b:/airflow# airflow dags trigger example_bash_operator\n",
                "[2022-10-26 15:55:58,868] {__init__.py:42} INFO - Loaded API auth backend: airflow.api.auth.backend.session\n",
                "Created <DagRun example_bash_operator @ 2022-10-26T13:55:58+00:00: manual__2022-10-26T13:55:58+00:00, state:queued, queued_at: 2022-10-26 13:55:58.927668+00:00. externally triggered: True>\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Ahora si lo activamos desde la consola vamos a ver que la tarea est√° encolada para ejecuci√≥n: \n",
                "| <img alt=\"Cola Airflow\" src=\"images/AirflowQueue.png\"/> |\n",
                "|:--:|\n",
                "|*Flujo de ejecuci√≥n encolado*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Ajam... ¬øY por qu√© no se ejecuta?. Si notamos a la izquierda del flujo de trabajo veremos que est√° desactivado, es decir, no est√° siendo tenido en cuenta por el scheduler. De modo tal que si queremos que corrar debemos prenderlo, ya sea haciendo click en la UI o corriendo el siguiente comando\n",
                "\n",
                "```console\n",
                "root@6ff3c3340c9b:/airflow#  airflow dags unpause example_bash_operator\n",
                "Dag: example_bash_operator, paused: False\n",
                "```\n",
                "| <img alt=\"Ejecuci√≥n exitosa\" src=\"images/AirflowSuccess.png\"/> |\n",
                "|:--:|\n",
                "|*Ejecuciones exitosas*|\n",
                "\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Bien!! Ahora si pudimos correr nuestras... ¬ø2 ejecuciones?. ¬øPor qu√© hay 2 ejecuciones? Si clickeamos en la UI, veremos que las 2 ejecuciones exitosas fueron:\n",
                "* Una ejecuci√≥n manual, la que invocamos nosotros ya sea por CLI o por Web.\n",
                "* Una ejecuci√≥n scheduleada. Esta √∫ltima fue ejecutada por el **Airflow Scheduler** a la hora de activar un DAG. Este es un buen punto a tener en cuando a la hora de comenzar a trabajar con Airflow: siempre se va a realizar una ejecuci√≥n en el momento de activar el DAG correpondiente a la que deber√≠a haber sido la √∫ltima ejecuci√≥n scheduleada. En este caso podemos ver que era la correspondiente al dia de ayer a las 00:00.\n",
                "\n",
                "| <img alt=\"Doble ejecuci√≥n\" src=\"images/AirflowDouble.png\"/> |\n",
                "|:--:|\n",
                "|*Diferentes tipos de ejecuciones*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Y si clickeamos en alguna de las 2 ejecuciones, lo primero que nos mostrar√° ser√° el grafo de ejecuci√≥n junto con sus resultados:\n",
                "\n",
                "| <img alt=\"Resultados del flujo\" src=\"images/DagResults.png\"/> |\n",
                "|:--:|\n",
                "|*Resultados del flujo*|\n",
                " \n",
                " *(El c√≥digo del dag puede verse en la pesta√±a \"Code\" y [en este link](extras/example_bash_operator.py))*\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9ef5b639",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5e06f188",
            "metadata": {},
            "source": [
                "### 1.4 Conceptos principales y arquitectura"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9e938ebd",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Ahora que corrimos un flujo de trabajo y tuvimos un primer encuentro con como funciona Airflow, hagamos un paso para atr√°s y hablemos de la terminolog√≠a y conceptos que conlleva la plataforma y que ya venimos usando.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Dentro de Airflow, un flujo de trabajo es representado por un grafo ac√≠clico dirigido o DAG (del ingl√©s Directed Acyclic Graph), lo que quiere decir que no hay forma de comenzar en un punto *p* y encontrar un camino que vuelva a *p*. Estos DAGs estan compuesto de unidades de trabajo llamadas tareas (Tasks) de manera tal de que entre ellas se establezca el orden de prioridad en terminos de dependencias o flujo de datos. Por ejemplo, que hacer si todo va bien, como si no:\n",
                "\n",
                "| <img alt=\"Ejemplo de flujo\" src=\"images/edge_label_example.png\"/> |\n",
                "|:--:|\n",
                "|*Ejemplo de DAG*|"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "60eca179",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; En t√©rminos de arquitectura, Airflow consiste de una serie de componentes que trabajan en conjunto:\n",
                "\n",
                "* El scheduler que se encarga tanto de triggerear nuevos flujos como de enviarle al **executor** tareas a ejecutar (como hicimos en nuestro ejemplo).\n",
                "\n",
                "* El mencionado executor que se encarga de efectivamente ejecutar tareas. Para el entorno en el que estamos trabajando (desarrollo), el executor est√° embebido en el scheduler, pero en un ambiente productivo se puede (y debe) usar un executor propiamente dicho (Por ejemplo: [CeleryExecutor](https://github.com/celery/celery) o [KubernetesExecutor](https://kubernetes.io/es/)) y enviar las tareas a workers.\n",
                "\n",
                "* El webserver que nos presenta una herramienta visual para controlar el estado, triggerear y debugear nuestros DAGs y sus tareas.\n",
                "\n",
                "* Una carpeta en donde vamos a guardar nuestros DAGs. Esta carpeta va a ser leida tanto por el scheduler como por el executor y todos sus workers (De otro modo no sabr√≠an cual de nuestras obras maestras ejecutar).\n",
                "\n",
                "* Una base de datos para guardar la metadata. Es usada por el scheduler, el executor y el webserver. En nuestro caso de `standalone`, Airflow usa una base de datos [SQlite](https://www.sqlite.org/index.html) que se puede encontrar en `$AIRFLOW_HOME/airflow.db`\n",
                "\n",
                "| <img alt=\"Arquitectura de Airflow\" src=\"images/arch-diag-basic.png\"/> |\n",
                "|:--:|\n",
                "|*Arquitectura de Airflow*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Como ya vimos, un DAG es una serie de tareas que normalmente es de alguno de estos 3 tipos:\n",
                "* Operadores (Operators), tareas predefinidas que se pueden conectar facilmente para generar un flujo. Los m√°s comunes son BashOperator (que ejecuta comandos o scripts de bash), PythonOperator (que ejecuta una funci√≥n de Python) y EmailOperator (que manda un mail). Pero estos no son todos, uno puede crear sus propios operadores custom o usar alguno de 60+ paquetes creados por diferentes providers, como Google Calendar o AWS S3.\n",
                "* Sensores (Sensors), una subclase de operadores que se basan en esperar eventos externos para comenzar.\n",
                "* Tareas determinadas por el decorador `@task` que son funciones de Python, similares a las que se pueden obtener con un PythonOperator. Esta opci√≥n es similar a como se construyen flujos de datos en herramientas como [Metaflow](https://metaflow.org/) e internamente lleva el nombre de TaskFlow."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76924b5e",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Dado que ya sabemos como crear tareas, ahora nos toca organizarlas. Para Airflow, los DAGs est√°n dise√±ados para correrse muchas veces e incluso en paralelo, contemplando que la data que se va a procesar es la del intervalo que hay en su periodo de scheduling (Por ejemplo si se schedulea con `@daily`, la informaci√≥n a procesar deber√≠a ser aquella generada entre las 00:00 y las 24:00).\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Para representar la dependencia de una tarea con otra (o otras) se pueden usar 2 sintaxis:\n",
                "\n",
                "* A trav√©s de `>>` y `<<`:\n",
                "```python\n",
                "primera_tarea >> [segunda_tarea, tercera_tarea]\n",
                "cuarta_tarea << tercera_tarea\n",
                "```\n",
                "\n",
                "* A trav√©s de `set_upstream` y `set_downstream`:\n",
                "```python\n",
                "primera_tarea.set_downstream([segunda_tarea, tercera_tarea])\n",
                "cuarta_tarea.set_upstream(tercera_tarea)\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; En el contexto de los DAGs, las mencionadas dependencias representan los arcos. Por defecto, una tarea va a esperar a que todas las tareas que tiene debajo hayan terminados exitosamente antes de comenzar(*), pero eso se puede customizar para generar por ejemplo, diferentes ramas de tareas que tienen dependencias en com√∫n, que una tarea comience a solo si corresponde a la √∫ltima ejecuci√≥n de un scheduling o si queremos que dependa de una condic√≠on espec√≠fica de como terminaron las tareas m√°s abajo.\n",
                "\n",
                "**(Si las tareas fallan, por defecto Airflow tratar√© nuevamente de ejecutarlo con un delay configurable entre pruebas. Si se alcanza el limit√©, fallar√° la tarea y, si no se tiene contemplado, el DAG)*\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; ¬øC√≥mo se comunican las tareas? Para esto, de nuevo, las opciones son 3:\n",
                "* XComs (‚ÄúCross-communications‚Äù) que es el sistema por el cual se envian peque√±as cantidades de informaci√≥n.\n",
                "* A partir de subir y descargar archivos a un storage en la nube (Por ejemplo GCS). Pensemos que estos metodos no son excluyente; tranquilamente se puede subir un dataset en la tarea A y comunicarle a la tarea B, via Xcoms, que dataset es el que debe procesar.\n",
                "* Si configuramos las tareas mediante la API de TaskFlow, la informaci√≥n se va a pasar automaticamente mediante Xcoms.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Como podemos ver, Airflow es una plataforma con una enorme cantidad de features y conceptos. Solo para que no queden cosas en el tintero, pasar√© a listar un par que nos quedar√≥n:\n",
                "* Existen subDAGs que nos permiten definir un DAG adentro de otro. De la misma manera en la que codeamos una funci√≥n porque tener c√≥digo duplicado es una mala pr√°ctica, los subDAGs nos permiten instanciar varias veces una misma secuencia de pasos a seguir si as√≠ lo requeriese nuestro flujo.\n",
                "* Similar al concepto anterior, podemos agrupar tareas dentro de un grupo (TaskGroup). Esto no tiene ning√∫n efecto funcional, solo est√©tico (üíÖ). La idea es que si nuestro DAG es muy complejo, visualmente no nos sirve analizar los flujos si lo que estamos viendo es una mara√±a de tareas, por lo que visualmente podemos agruparlas (normalmente en terminos funcionales de conjunto) de modo tal de que nos sea m√°s facil distinguir el flujo entre procesos.\n",
                "\n",
                "| <img alt=\"Grupo de tareas\" src=\"images/task_group.gif\"/> |\n",
                "|:--:|\n",
                "|*Grupo de tareas*|\n",
                "\n",
                "* Tambien contamos con un feature en donde se pueden guardar variables de entorno, accesibles desde las tareas y que a su vez tambien pueden ser escritas en las mismas. Un caso particularmente similar es el de las conexiones con bases de datos. Estas pueden ser configuradas desde el CLI o en la secci√≥n `Admin` de la UI."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75b2a195",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5e06f188",
            "metadata": {},
            "source": [
                "### 1.5 Planteo de problema"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48e785f5",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Mucho ruido y pocas nueces! Hagamos nuestro propio DAG! Vimos que al iniciar nuestra UI, se nos brinda una serie de DAGs de ejemplo predefinidos. Est√°s m√°s que invitad@ a ver que contienen ya que todos muestran como funcionan muchos de los conceptos que nombramos m√°s arriba. Ahora, ¬øC√≥mo agregamos los propios?\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Como mencionamos en la secci√≥n de arquitectura, Airflow requiere que se especifique una carpeta de donde leer las definiciones de los DAGs. Dicha carpeta se especifica en el archivo de configuraci√≥n `$AIRFLOW_HOME/airflow.cfg`, bajo la entrada `dags_folder`. (Uno sabe que una plataforma tiene un par de features cuando su archivo de configuraci√≥n tiene m√°s de 300 entradas...)\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Para nuestro caso de estudio creemos el escenario. Vamos a crear un peque√±o ETL para un random walker que solo puede avanzar(?) con las siguientes condiciones:\n",
                "* Nuestro experimento deber√≠a haber comenzado en 2022-01-01 y deber√° ejecutarse a cada minuto (Ac√° podemos ver que evidentemente no podemos tener una persona iniciando nuestro script cada 1 minuto).\n",
                "* Primero vamos a tirar un dado y una moneda N veces: cara es avanzar y ceca es retrocedor, mientras que el dado nos dar√° la cantidad de pasos (Este paso simula la generaci√≥n de datos, normalmente puede ser una condici√≥n externa al proceso).\n",
                "* Luego de tener nuestras N muestras vamos a tomar una decision: si luego de las N veces, el random walker avanz√≥ entonces registramos el avance; ahora, si el random walker retrocedi√≥, simplemente lo dejamos en la posici√≥n en la que estaba al comienzo. (Est√° etapa simular√° tanto la transformaci√≥n de la data y que hacer en caso de \"fallo\").\n",
                "* Por √∫ltimo registraremos la posici√≥n final del random walker.\n",
                "\n",
                "| <img alt=\"Robot caminando\" src=\"images/robotWalking.jpg\"/> |\n",
                "|:--:|\n",
                "|*Robot caminando*|"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75b2a195",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37f7020e",
            "metadata": {},
            "source": [
                "### 1.6 Armado del esqueleto"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6872f615",
            "metadata": {},
            "source": [
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Genial! Comencemos! Lo primero que vamos a hacer es crear el archivo en donde escribiremos el DAG:\n",
                "```bash\n",
                "mkdir $AIRFLOW_HOME/dags \n",
                "touch $AIRFLOW_HOME/dags/cuasi_random_walker.py\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Abrimos el archivo con nuestro IDE preferido y creamos el contexto del DAG:\n",
                "\n",
                "```python\n",
                "import datetime\n",
                "\n",
                "from airflow import DAG\n",
                "from airflow.operators.empty import EmptyOperator\n",
                "\n",
                "with DAG(\n",
                "    dag_id='random_walker',\n",
                "    schedule='* * * * *',\n",
                "    start_date=datetime.datetime(2022, 1, 1),\n",
                "    catchup=False,\n",
                "    tags=['experimentos'],\n",
                "    params={\"N\": \"50\"},\n",
                ") as dag:\n",
                "    ejemplo = EmptyOperator(\n",
                "        task_id='prototipo',\n",
                "    )\n",
                "\n",
                "    ejemplo\n",
                "```\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; En este caso estamos creando un DAG con una sola tarea de ejemplo. Es una buena pr√°ctica crear los DAGs con un contexto pero sepamos que se puden generar, por ejemplo como resultado de una funci√≥n. Pasemos a explicar sus par√°metros:\n",
                "* **dag_id**: El nombre de nuestro DAG. Es el que veremos en nuestra UI.\n",
                "* **schedule**: Con que frecuencia se deber√≠a ejecutar nuestro DAG si tal cosa existe, puede que solo querramos que se ejecute de manera externa o en base a otros DAGs. Este  par√°metro soporta expresiones cron (Siempre se puede visitar [crontab.guru](https://crontab.guru/)) o valores preseteados como `@hourly` o `@daily`.\n",
                "* **start_date**: Desde que momento este DAG es valido. Existe un analog√≥ para establecer hasta que momento es valido.\n",
                "* **catchup**: Este  par√°metro establece si se deber√≠a hacer un fillback. Esto quiere decir que si estuviese activado lanzar√≠a todos los DAGs que se deber√≠an haber ejecutado entre 2022-01-01:00:00 y el momento en el que activemos el DAG. Es importante mencionar, como vimos en el ejemplo al comienzo, que indefectiblemente de si este par√°metro est√° activado o no, el scheduler har√° una ejecucion (DagRun) correspondiente al √∫ltimo schedule.\n",
                "* **tags**: Lista de tags que nos ayuda visualmente a identificar DAGs que sirve a un fin com√∫n o area.\n",
                "* **params**: Diccionario que podremos usar para acceder en nuestros templates.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Ahora que sabemos como generar DAGs y prototipos, borremos `ejemplo` y armemos el esqueleto de nuestra flujo:\n",
                "```python\n",
                "from airflow.operators.python import BranchPythonOperator\n",
                "from airflow.utils.edgemodifier import Label\n",
                "from airflow.utils.trigger_rule import TriggerRule\n",
                "\n",
                "    ...\n",
                "    # Lanzamos los dados y la moneda\n",
                "    tirar_dados = EmptyOperator(\n",
                "        task_id='tirar_dados',\n",
                "    )\n",
                "\n",
                "    # Dependiendo del resultado vamos a avanzar o permanecer quietos\n",
                "    resultados = ['avanzar', 'permanecer']\n",
                "\n",
                "    branching = BranchPythonOperator(\n",
                "        task_id='branching',\n",
                "        python_callable=lambda: random.choice(resultados),\n",
                "    )\n",
                "\n",
                "    # Vamos a tirar los dados antes de analizar sus resultados\n",
                "    tirar_dados >> branching\n",
                "\n",
                "    # No importa que camino se haya tomado, vamos a registrar en donde est√° el walker\n",
                "    loggear_posicion = EmptyOperator(\n",
                "        task_id='loggear_posici√≥n',\n",
                "        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n",
                "    )\n",
                "\n",
                "    # En este caso, para ambos caminos vamos a calcularla la nuevo posici√≥n\n",
                "    for option in resultados:\n",
                "        t = EmptyOperator(\n",
                "            task_id=option,\n",
                "        )\n",
                "\n",
                "        nueva_posicion = EmptyOperator(\n",
                "            task_id='nueva_posicion_' + option,\n",
                "        )\n",
                "\n",
                "        # Las Label son opcionales pero nos sirven para entender el flujo\n",
                "        branching >> Label(option) >> t >> nueva_posicion >> loggear_posicion\n",
                "```\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Si abrimos nuestro DAG en este momento en la UI, en la secci√≥n de grafo deberiamos ver lo siguiente:\n",
                "\n",
                "| <img alt=\"BOO!\" src=\"images/Skeleton.png\"/> |\n",
                "|:--:|\n",
                "|*Spooky skeleton*|\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75b2a195",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37f7020e",
            "metadata": {},
            "source": [
                "### 1.7 Implementamos tareas"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a598b59",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Para la implementaci√≥n de los pasos instalemos numpy como dependencia:\n",
                "```bash\n",
                "pip install numpy\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Creemos carpetas para guardar la data. En un escenario real esto podr√≠a ser un data lake o data warehouse.\n",
                "```bash\n",
                "mkdir -p /tmp/data/dados\n",
                "mkdir -p /tmp/data/monedas\n",
                "mkdir -p /tmp/data/posiciones\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Guardemos la posicion inicial\n",
                "```bash\n",
                "echo \"0\" >> /tmp/data/posiciones/2022-01-01_00-00-00.txt\n",
                "```\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Y ahora s√≠, implementemos los step:\n",
                "\n",
                "* ### `tirar_dados`: reemplazamos el EmptyOperator\n",
                "\n",
                "```python\n",
                "from airflow.operators.python import PythonOperator\n",
                "import numpy as np\n",
                "\n",
                "# Funciones\n",
                "def tirar_dados_func(N: str):\n",
                "\n",
                "    # Convertimos N a int\n",
                "    N = int(N)\n",
                "\n",
                "    # A partir de la fecha generamos una etiqueta\n",
                "    time_name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "\n",
                "    # Tiro dados\n",
                "    dados = np.random.randint(1, 7, size=N)\n",
                "\n",
                "    # Tiro monedas\n",
                "    monedas = np.random.choice([-1, 1], size=N)\n",
                "\n",
                "    # Guardamos valores\n",
                "    with open(f\"/tmp/data/dados/{time_name}.npy\", \"wb\") as f:\n",
                "        np.save(f, dados)\n",
                "\n",
                "    with open(f\"/tmp/data/monedas/{time_name}.npy\", \"wb\") as f:\n",
                "        np.save(f, monedas)\n",
                "\n",
                "...\n",
                "    # Lanzamos los dados y la moneda\n",
                "    tirar_dados = PythonOperator(\n",
                "        task_id=\"tirar_dados\",\n",
                "        op_args=[\n",
                "            \"{{ params.N }}\",  # Ac√° usamos el formato de templating\n",
                "        ],\n",
                "        python_callable=tirar_dados_func,\n",
                "    )\n",
                "```\n",
                "\n",
                "* ### `branching`:\n",
                "```python\n",
                "import glob\n",
                "import os\n",
                "\n",
                "def get_last_file(path: str) -> str:\n",
                "    \"\"\"Get the name of last created file in a given path\n",
                "\n",
                "    Args:\n",
                "        path (str): Path to the explore\n",
                "\n",
                "    Returns:\n",
                "        str: Name of the last created file\n",
                "    \"\"\"\n",
                "    list_of_files = glob.glob(f\"{path}*\")\n",
                "    latest_file = max(list_of_files, key=os.path.getctime)\n",
                "    return latest_file\n",
                "\n",
                "\n",
                "def branching_func(\n",
                "    **context,\n",
                "):  # Agregamos el contexto para poner mandar mensajes entre tareas\n",
                "\n",
                "    # Obtengo los ultimos dados y monedas\n",
                "    ultimos_dados = get_last_file(\"/tmp/data/dados/\")\n",
                "    ultimas_monedas = get_last_file(\"/tmp/data/monedas/\")\n",
                "\n",
                "    dados = np.load(ultimos_dados)\n",
                "    monedas = np.load(ultimas_monedas)\n",
                "\n",
                "    # Calculo la suma total de pasos en ambas direcciones\n",
                "    suma = (dados * monedas).sum()\n",
                "\n",
                "    # Mandamos el mensaje de cuanto fue la suma\n",
                "    task_instance = context[\"task_instance\"]\n",
                "    task_instance.xcom_push(key=\"suma_de_pasos\", value=str(suma))\n",
                "\n",
                "    # Si el walker debe avanzar\n",
                "    if suma >= 0:\n",
                "        return \"avanzar\"\n",
                "\n",
                "    # Caso contrario\n",
                "    return \"permanecer\"\n",
                "\n",
                "...\n",
                "    # Dependiendo del resultado vamos a avanzar o permanecer quietos\n",
                "    branching = BranchPythonOperator(\n",
                "        task_id=\"branching\",\n",
                "        python_callable=branching_func,\n",
                "        provide_context=True,  # Proveemos el contexto para enviar mensajes entre tareas\n",
                "    )\n",
                "\n",
                "    # Vamos a tirar los dados antes de analizar sus resultados\n",
                "    tirar_dados >> branching\n",
                "```\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e04a9e1",
            "metadata": {},
            "source": [
                "* ### Vamos a tener que separara el `avanzar` y `retroceder` ya que tienen diferente comportamiento:\n",
                "\n",
                "```python\n",
                "def avanzar_func(\n",
                "    **context,\n",
                "):\n",
                "\n",
                "    # Recibimos el mensaje de `branching`\n",
                "    task_instance = context[\"task_instance\"]\n",
                "    suma = task_instance.xcom_pull(task_ids=\"branching\", key=\"suma_de_pasos\")\n",
                "\n",
                "    # \"Procesamos\" la nueva suma.\n",
                "    print(f\"El walker avanza {suma} pasos!\")\n",
                "\n",
                "\n",
                "def nueva_posicion_avanzar_func(\n",
                "    **context,\n",
                "):\n",
                "\n",
                "    # Recibimos el mensaje de `branching`\n",
                "    task_instance = context[\"task_instance\"]\n",
                "    suma = task_instance.xcom_pull(task_ids=\"branching\", key=\"suma_de_pasos\")\n",
                "\n",
                "    # Ultima posicion\n",
                "    ultima_posicion = get_last_file(\"/tmp/data/posiciones/\")\n",
                "    with open(ultima_posicion, \"r+\") as f:\n",
                "        posicion = f.read()\n",
                "\n",
                "    # Calculamos la nueva posicion\n",
                "    nueva_posicion = int(suma) + int(posicion)\n",
                "\n",
                "    print(f\"{suma = }\")\n",
                "    print(f\"{posicion = }\")\n",
                "\n",
                "    # Guardamos la ultima posicion\n",
                "    time_name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "    with open(f\"/tmp/data/posiciones/{time_name}.txt\", \"w\") as f:\n",
                "        f.write(str(nueva_posicion))\n",
                "\n",
                "\n",
                "def permanecer_func(\n",
                "    **context,\n",
                "):\n",
                "\n",
                "    # Recibimos el mensaje de `branching`\n",
                "    task_instance = context[\"task_instance\"]\n",
                "    suma = task_instance.xcom_pull(task_ids=\"branching\", key=\"suma_de_pasos\")\n",
                "\n",
                "    # \"Procesamos\" la nueva suma.\n",
                "    print(f\"El walker no va a moverse porque la suma fue {suma}!\")\n",
                "\n",
                "\n",
                "def nueva_posicion_permanecer_func():\n",
                "\n",
                "    # Ultima posicion\n",
                "    ultima_posicion = get_last_file(\"/tmp/data/posiciones/\")\n",
                "    with open(ultima_posicion, \"r\") as f:\n",
                "        posicion = f.read()\n",
                "\n",
                "    # Calculamos la \"nueva\" posicion\n",
                "    nueva_posicion = int(posicion)\n",
                "\n",
                "    # Guardamos la \"nueva\" posicion\n",
                "    time_name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "    with open(f\"/tmp/data/posiciones/{time_name}.txt\", \"w\") as f:\n",
                "        f.write(str(nueva_posicion))\n",
                "...\n",
                "\n",
                "    avanzar = PythonOperator(\n",
                "        task_id=\"avanzar\",\n",
                "        python_callable=avanzar_func,\n",
                "        provide_context=True,\n",
                "    )\n",
                "\n",
                "    # Brancheamos y luego avanzamos\n",
                "    branching >> Label(\"avanzar\") >> avanzar\n",
                "\n",
                "    # Calculamos la nueva posicion\n",
                "    nueva_posicion_avanzar = PythonOperator(\n",
                "        task_id=\"nueva_posicion_avanzar\",\n",
                "        python_callable=nueva_posicion_avanzar_func,\n",
                "        provide_context=True,\n",
                "    )\n",
                "\n",
                "    # Luego de procesar el avance, calculamos la nuva posicion\n",
                "    avanzar >> nueva_posicion_avanzar\n",
                "\n",
                "    # Permanecemos quietos\n",
                "    permanecer = PythonOperator(\n",
                "        task_id=\"permanecer\",\n",
                "        python_callable=permanecer_func,\n",
                "        provide_context=True,\n",
                "    )\n",
                "\n",
                "    # Brancheamos y luego avanzamos\n",
                "    branching >> Label(\"permanecer\") >> permanecer\n",
                "\n",
                "    # Calculamos la nueva posicion\n",
                "    nueva_posicion_permanecer = PythonOperator(\n",
                "        task_id=\"nueva_posicion_permanecer\",\n",
                "        python_callable=nueva_posicion_permanecer_func,\n",
                "    )\n",
                "\n",
                "    # Luego de procesar el avance, calculamos la nuva posicion\n",
                "    permanecer >> nueva_posicion_permanecer\n",
                "```\n",
                "\n",
                "* ### Y por √∫ltimo `loggear_posicion`:\n",
                "\n",
                "```python \n",
                "def loggear_posicion_func():\n",
                "\n",
                "    # Obtengo la ultima posicion\n",
                "    ultima_posicion = get_last_file(\"/tmp/data/posiciones/\")\n",
                "    with open(ultima_posicion, \"r\") as f:\n",
                "        posicion = f.read()\n",
                "\n",
                "    # \"Proceso\" la √∫ltima posicion\n",
                "    print(f\"En este momento, el walker est√° en {posicion}!\")\n",
                "\n",
                "...\n",
                "\n",
                "    # No importa que camino se haya tomado, vamos a registrar en donde est√° el walker\n",
                "    loggear_posicion = PythonOperator(\n",
                "        task_id=\"loggear_posicion\",\n",
                "        python_callable=loggear_posicion_func,\n",
                "        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,\n",
                "    )\n",
                "\n",
                "    # Si se avanzo o no, se loggea\n",
                "    [nueva_posicion_avanzar, nueva_posicion_permanecer] >> loggear_posicion\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee1570bd",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Uff eso fue un mont√≥n, pero pudimos pasar por la mayoria de los conceptos que vimos anteriormente. El archivo final deber√≠a verse [asi](\"extras/cuasi_random_walker.py\").\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Encendamos motores y prendamos nuestro walker! Luego de unos minutos veremos que el DAG se corri√≥ varias veces; en algunos casos el walker avanz√≥ y en otros permaneci√≥ quieto. Para ver la posici√≥n del walker siempre podemos ver los logs del step `loggear_posicion`. \n",
                "\n",
                "| <img alt=\"Walker on\" src=\"images/Walker.png\"/> |\n",
                "|:--:|\n",
                "|*Walker corriendo*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; C√≥mo podemos ver, los DAGs no fallan, aunque el walker \"fallo\" y no avanz√≥ (los DAGs solo tienen 2 estados final: `success` o `failed`). Es importante mencionar que de la misma manera en donde en un programa no siempre todo va como querriamos, el programa finaliza bien pero con un comportamiento indeseado. Esto se puede traducir en que la excepcion estaba contemplada (otra muy distinta es que explote el programa...). De la misma manera, a la hora de generar nuestro pipelines o DAGs deberiamos tener en cuenta los casos \"no tan bonitos\" ,en donde se deben tomar acciones correspondientemente, y dejar que los `failed` de los pipelines nos digan que tenemos un nuevo caso no contemplado a trabajar.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1d30202",
            "metadata": {},
            "source": [
                "### 1.8 Ambiente productivo"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26752323",
            "metadata": {},
            "source": [
                "&nbsp;&nbsp;&nbsp;&nbsp; Ahora bien. Ya tenemos un ambiente de desarollo local en donde nuestro DAG funciona. Pero...\n",
                "| <img alt=\"Delivery\" src=\"images/delivery.png\"/> |\n",
                "|:--:|\n",
                "|*Delivery de flujos*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Por tanto, el paso siguiente es llevar nuestra soluci√≥n a la nube. Y que mejor lugar que [Cloud Composer](https://cloud.google.com/composer), un servicio de Airflow gestionado por Google Cloud.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Teniendo nuesto proyecto seleccionado, para acceder a Composer podemos buscarlo y acceder al mismo\n",
                "\n",
                "| <img alt=\"Composer\" src=\"images/ComposerSearch.png\"/> |\n",
                "|:--:|\n",
                "|*Composer en el buscador*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Una vez adentro del servicio el paso siguiente es crear un ambiente, para eso vamos a llenar los siguientes campos y crear el ambiente:\n",
                "* Nombre: develop\n",
                "* Ubicaci√≥n: indistinto (Ej: us-central-1)\n",
                "* Cantidad de nodos: lo dejamos en 3\n",
                "* Tama√±o del disco: dado que no vamos a necesitar demasiado, podemos reducir el valor por defecto (100GB) a 30GB.\n",
                "* Version de imagen: de nuevo, al ser experimental podemos seleccionar la √∫ltima version disponible. En mi caso fue `composer-1.19.12-airflow-2.3.3`.\n",
                "* Cantidad de schedulers: Con 1 solo estamos bien.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Ya ingresados todos los campos y clickeado el boton de `CREAR` nos queda esperar hasta que el ambiente se encuentre estable:\n",
                "| <img alt=\"Composer estable\" src=\"images/StableComposer.png\"/> |\n",
                "|:--:|\n",
                "|*Composer estable*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Genial! Ya tenemos un ambiente andando. \n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Cuando me refer√≠a a las diferencias entre un ambiente de desarollo y uno productivo es que, por ejemplo, si tratasemos de subir nuestro DAG a la plataforma no funcionar√≠a por incompatibilidades en la versi√≥n (en composer se usa Airflow 2.3.3 y en local 2.4.2), as√≠ como del sistema de archivos (que usamos para transferir informaci√≥n). Como solucion a esos problemas, podemos modificar nuestro DAG para:\n",
                "* Reemplazar `schedule` por el deprecado  par√°metro `schelude_interval` en la creaci√≥n del DAG.\n",
                "* Reemplazar la carpeta en donde se escribe la informaci√≥n de los DAGs de `/tmp/data` a `/home/airflow/data`, que en Composer convenientemente se sincroniza con el bucket que usa backend üëå.\n",
                "* Agregar funciones de fallback para el cold-start de carpetas y archivos.\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Una versi√≥n con esos problemas corregidos puede obtenerse [aca](\"extras/cuasi_random_walker_composer_version.py\").\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Antes de subir nuestro DAG, debemos indicarle a Composer que el ambiente en el que vamos a trabajar requiere que instalemos dependecias (`numpy`). Para eso podremos agregarla en la siguiente opcion:\n",
                "\n",
                "| <img alt=\"Agregar numpy\" src=\"images/AddNumpy.png\"/> |\n",
                "|:--:|\n",
                "|*Agregar numpy*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Una vez que agreguemos la dependencias deberemos esperar a que los cambios se repliquen en todo el ambiente. Cuando eso termine solo nos quedar√° subir nuestro DAG al bucket de nuestro ambiente:\n",
                "| <img alt=\"Carpeta de DAGS\" src=\"images/UploadDag.png\"/> |\n",
                "|:--:|\n",
                "|*Carpeta de DAGS*|\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Luego de subir nuestro DAG, Composer automaticamente lo agregar√° al scheduler y lo habilitar√°. Esto signifca que si abrimos la UI y esperamos de unos minutos... Voil√†! Tenemos nuestro DAG corriendo en la nube con Composer!\n",
                "\n",
                "| <img alt=\"Walking Composer\" src=\"images/ComposerWalking.png\"/> |\n",
                "|:--:|\n",
                "|*Walking Composer*|\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "88b31663",
            "metadata": {},
            "source": [
                "### 1.9 Conclusiones"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0be96680",
            "metadata": {},
            "source": [
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Uff que paseo! Airflow es una gran plataforma que nos permite sincronizar y orquestar tareas de casi cualquier manera. \n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Son un mont√≥n de conceptos y creame que no exploramos un mont√≥n de cosas (como otros executors u operadores custom), por lo que no te desanimes si ves abrumador la cantidad de conceptos. De a poco se avanza. Lo importante a saber es que siempre podemos contar con Airflow para que se ocupe de lanzar nuestros flujos de trabajo y poder dormir tranquilos. Al menos hasta que nos env√≠e un mail...üí®üìÆ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "vscode": {
            "interpreter": {
                "hash": "d27203dcce1218a35e5dd056f2e1158cfdf3a468b69181284abdce2479607453"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
