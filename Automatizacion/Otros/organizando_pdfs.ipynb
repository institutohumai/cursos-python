{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35818f2-6a79-464b-a5da-8d9142a1479d",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/Automatizacion/organizando_pdfs.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg'/> </a> <br> Recordá abrir en una nueva pestaña </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517e9e5-c4a4-49ec-96e9-2a6f211f9063",
   "metadata": {},
   "source": [
    "# Ejercicio: Organizando PDFs\n",
    "\n",
    "Supongamos que tenemos una carpeta con muchos artículos científicos bajados de ArXiv como [este](https://arxiv.org/abs/hep-th/9711200) de Maldacena, el renombrado físico argentino. En esta notebook vamos a usar [textract](https://textract.readthedocs.io/en/stable/) para leer textos de PDFs y así:\n",
    "\n",
    "- Reemplazar el título de algo como \"9711200.pdf\" al título del trabajo\n",
    "- Categorizar los trabajos\n",
    "- Crear carpetas por categoría\n",
    "- Mover los artículos a cada una de las carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd359f02-d87d-4fe7-8d67-a73bc6f0e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalamos la librería que vamos a utilizar\n",
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3fcc667f-ddd3-4da7-b16d-b53694300db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e9be677e-8314-4e94-8dee-e7c36257085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 19:35:31--  https://unket.s3.sa-east-1.amazonaws.com/data/papers.zip\n",
      "Resolving unket.s3.sa-east-1.amazonaws.com (unket.s3.sa-east-1.amazonaws.com)... 52.95.165.122\n",
      "Connecting to unket.s3.sa-east-1.amazonaws.com (unket.s3.sa-east-1.amazonaws.com)|52.95.165.122|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50053476 (48M) [application/zip]\n",
      "Saving to: ‘papers.zip’\n",
      "\n",
      "papers.zip          100%[===================>]  47,73M  3,56MB/s    in 24s     \n",
      "\n",
      "2021-10-06 19:35:55 (2,03 MB/s) - ‘papers.zip’ saved [50053476/50053476]\n",
      "\n",
      "Archive:  papers.zip\n",
      "   creating: papers/\n",
      "  inflating: papers/1907.12461.pdf   \n",
      "  inflating: papers/1909.06349.pdf   \n",
      "  inflating: papers/2002.07655.pdf   \n",
      "  inflating: papers/2009.11423.pdf   \n",
      "  inflating: papers/1508.06576.pdf   \n",
      "  inflating: papers/1601.00670.pdf   \n",
      "  inflating: papers/1603.07285v1.pdf  \n",
      "  inflating: papers/1606.05908.pdf   \n",
      "  inflating: papers/1606.07873.pdf   \n",
      "  inflating: papers/1611.03673.pdf   \n",
      "  inflating: papers/1611.08024 (1).pdf  \n",
      "  inflating: papers/1706.03762.pdf   \n",
      "  inflating: papers/1903.01458.pdf   \n",
      "  inflating: papers/barratt1959.pdf  \n",
      "  inflating: papers/eysenck1977.pdf  \n",
      "  inflating: papers/eysenck1985.pdf  \n",
      "  inflating: papers/fpsyg-02-00086.pdf  \n",
      "  inflating: papers/langenecker2007.pdf  \n",
      "  inflating: papers/li2009.pdf       \n",
      "  inflating: papers/luengo1991.pdf   \n",
      "  inflating: papers/patmethenysongbook.pdf  \n"
     ]
    }
   ],
   "source": [
    "# descargamos los papers ejemplo\n",
    "!wget https://unket.s3.sa-east-1.amazonaws.com/data/papers.zip\n",
    "# si ejecutan localmente o no tienen unzip en la terminal, descomprimir a mano\n",
    "!unzip papers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "89411de7-ced6-4267-bee0-8df88c6bbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la ruta a la carpeta con los trabajos\n",
    "pdf_dir = r\"./papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c3bbcca-008d-4e60-89c5-37a33c9f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listamos todos los archivos de esa carpeta\n",
    "files = os.listdir(pdf_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5b5ec-a435-4b28-8fd3-8783991f6237",
   "metadata": {},
   "source": [
    "Creamos un directorio para cada una de las 3 categorías, \"Neuro\", \"Deep\" (para Deep Learning) y \"Otros\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b31e54a4-599a-4091-9df8-973a1303046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_neuro = 'Neuro'\n",
    "os.makedirs(ruta_neuro, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "deb27a55-ffd8-4c23-8f8e-7e7a212e54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_deep = 'Deep'\n",
    "os.makedirs(ruta_deep, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca04b5fd-4287-4721-aafc-18cd5d489aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_otros = 'Otros'\n",
    "os.makedirs(ruta_otros, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072054b-ab6e-4630-8183-cb0aa1e91055",
   "metadata": {},
   "source": [
    "Vamos a necesitar una función para extraer el título. Como no queremos usar regex todavía (se ve próximamente), vamos a usar también esta siguiente función para limpiar un renglón para ver si tiene el largo suficiente para ser un título. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2487434c-5395-46b8-9fef-bb2db4712cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar(string):\n",
    "    \"\"\"Función para limpiar los renglones quitando los caracteres que no queremos considerar\"\"\"\n",
    "    \n",
    "    # \\w+ significa \"1 o más de un caracter alfanumérico\". Ver más en la clase \n",
    "    # de regex\n",
    "    \n",
    "    # versión sin regex:\n",
    "    # no_valen = '0123456789-:\\.'\n",
    "    # return ''.join([i for i in string if i not in no_valen])\n",
    "    \n",
    "    return ''.join(re.findall('\\w+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8e7e0204-3c73-448c-88f5-04dc6df44846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(texto, largo_min=20):\n",
    "    \"\"\"Función para extraer un título. Podríamos mejorar la lógica o usar distintos enfoques\"\"\"  \n",
    "    renglones = [t for t in texto.split('\\n') if len(limpiar(t)) > largo_min]\n",
    "    if len(renglones) > 0:\n",
    "        return renglones[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36a450-6f77-41e0-9d7e-261b81c1bb28",
   "metadata": {},
   "source": [
    "Ahora, recorremos cada archivo y para cada uno vamos a:\n",
    "- Leer el texto con textract\n",
    "- Extraer el título\n",
    "- Estimar la categoría\n",
    "- Mover y renombrarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a8b9c440-b174-4405-ad9a-d892fbcc6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Inference: A Review for Statisticians\n",
      "Neuro\n",
      "\n",
      "arXiv:1603.07285v1 [stat.ML] 23 Mar 2016\n",
      "Deep\n",
      "\n",
      "Person. indiuid. D# Vol. 12, No. 7. pp. 657-667. 1991\n",
      "Neuro\n",
      "\n",
      "Perceptual alzd Motor Skills, 1959, 9, 191-198. @ Southern Universities Press 1959\n",
      "Deep\n",
      "\n",
      "Deep Learning for Cognitive Neuroscience\n",
      "Neuro\n",
      "\n",
      "Under review as a conference paper at ICLR 2017\n",
      "Neuro\n",
      "\n",
      "Br. J. soc. d i n . Psychol. (1977), 16, 57-68\n",
      "Neuro\n",
      "\n",
      "patmethenysongbook\n",
      "Otros\n",
      "\n",
      "Task-Oriented Dialogue as Dataflow Synthesis\n",
      "Neuro\n",
      "\n",
      "This article was downloaded by: [University of Newcastle (Australia)]\n",
      "Neuro\n",
      "\n",
      "arXiv:2002.07655v1 [q-bio.NC] 18 Feb 2020\n",
      "Neuro\n",
      "\n",
      "Tutorial on Variational Autoencoders\n",
      "Deep\n",
      "\n",
      "A Neural Algorithm of Artistic Style\n",
      "Neuro\n",
      "\n",
      "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks\n",
      "Deep\n",
      "\n",
      "Person. indiuid. Off. Vol. 6. No. 5, pp. 613419,\n",
      "Neuro\n",
      "\n",
      "Slice-based Learning: A Programming Model for\n",
      "Deep\n",
      "\n",
      "arXiv:1706.03762v5 [cs.CL] 6 Dec 2017\n",
      "Neuro\n",
      "\n",
      "Hypothesis and Theory Article\n",
      "Neuro\n",
      "\n",
      "Human Brain Mapping 31:410–423 (2010)\n",
      "Neuro\n",
      "\n",
      "EEGNet: A Compact Convolutional Neural Network\n",
      "Neuro\n",
      "\n",
      "arXiv:1606.07873v1 [cs.CV] 25 Jun 2016\n",
      "Deep\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in files:\n",
    "    # ruta completa al archivo\n",
    "    full_name = os.path.join(pdf_dir, filename)\n",
    "    # de pdf a texto\n",
    "    text = textract.process(full_name, language='eng').decode()\n",
    "    # asumimos que el título está en los primeros 1000 caracteres\n",
    "    top = text[:1000]\n",
    "    \n",
    "    # vamos a intentar extraer el titulo con nuestra función\n",
    "    # si no encontramos un sub-string que cumpla con las condiciones, \n",
    "    # mantenemos el nombre original\n",
    "    title = get_title(top)\n",
    "    if title == None:\n",
    "        title = filename.replace('.pdf', '')\n",
    "        \n",
    "    # para categorizar los textos, vamos a usar esta lógica sencilla:\n",
    "    # nos fijamos si contiene alguno de los siguientes términos clave\n",
    "    is_deep = ('deep learning' in text.lower()) or ('statistic' in text.lower())\n",
    "    is_neuro = ('neuro' in text.lower()) or ('brain' in text.lower())\n",
    "    \n",
    "    # para cada texto, lo vamos a mover a su carpeta y vamos a mostrar su categoría\n",
    "    print(title)\n",
    "    if is_neuro:\n",
    "        print('Neuro')\n",
    "        copyfile(f\"{pdf_dir}/{filename}\", f\"{ruta_neuro}/{title}.pdf\")\n",
    "    elif is_deep:\n",
    "        print('Deep')\n",
    "        copyfile(f\"{pdf_dir}/{filename}\", f\"{ruta_deep}/{title}.pdf\")\n",
    "    else:\n",
    "        print('Otros')\n",
    "        copyfile(f\"{pdf_dir}/{filename}\", f\"{ruta_otros}/{title}.pdf\")    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
