{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVyEa-VZHBkc"
   },
   "source": [
    "# Sistemas de Recomendación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r25Ck3i3FnNe"
   },
   "source": [
    "<div align=\"center\"><a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/MachineLearning/10_Recomendacion/sistemas_recomendacion.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg'/> </a> <br> Recordá abrir en una nueva pestaña </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QjZzwtiVHC2o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDdCuUc-HFVc"
   },
   "source": [
    "## Similitud coseno\n",
    "\n",
    "$$sim(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}$$\n",
    "\n",
    "¿Cómo calcularla en Python?\n",
    "\n",
    "Supongamos que tenemos la siguiente matriz:\n",
    "\n",
    "|  \t| Libro A \t| Libro B \t| Libro C \t|\n",
    "|-------\t|---------\t|---------\t|---------\t|\n",
    "| Juan \t| 5 \t| 4 \t| 4 \t|\n",
    "| Diego \t| 4 \t| 5 \t| 5 \t|\n",
    "\n",
    "\n",
    "Podemos calcular la similitud coseno empleando sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBz04zsD-8F7",
    "outputId": "184216ea-f669-46cc-e852-27366a6aa71d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.97823198],\n",
       "       [0.97823198, 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "Juan = [5,4,4]\n",
    "Diego = [4,5,5]\n",
    "cosine_similarity([Juan, Diego])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x0cVhgjHIyH"
   },
   "source": [
    "También podemos calcular la similitud a mano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eShcK8FWHGvO",
    "outputId": "797a2f4a-c993-48e7-a993-1781524aa12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782319760890369"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*4 + 4*5 + 4*5)/(np.sqrt(5**2+4**2+4**2)*np.sqrt(4**2+5**2+5**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5IhuB1oHL4c"
   },
   "source": [
    "O empleando Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqtKORcEHNUB"
   },
   "source": [
    "Calcular la similitud coseno usando numpy (con np.dot y np.linalg.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jDaFAUXEHGxY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WpFW_ptHQLE"
   },
   "source": [
    "Ahora bien, cuando tenemos una matriz user-item de la vida real, tenemos muchos casos faltantes. En esta situación, no podremos calcular la similitud coseno tan fácilmente... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAbQ0-haHO4j",
    "outputId": "543c2364-4a27-4ee6-9e0e-d620120c666f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5., nan,  4.],\n",
       "       [ 4.,  3.,  5.],\n",
       "       [ 4.,  5.,  5.],\n",
       "       [nan,  5., nan],\n",
       "       [nan,  5.,  3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item = np.array([[5, np.nan, 4],[4,3,5],[4,5,5],[np.nan, 5, np.nan], [np.nan, 5, 3]])\n",
    "user_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKvEn5JwHUz7"
   },
   "source": [
    "## Surprise\n",
    "\n",
    "En esta notebook vamos a emplear la librería surprise. Esta es una librería que se basa en la API de scikit-learn y permite implementar varios algoritmos básicos de recomendación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDGfsEZrHXuo"
   },
   "source": [
    "Comencemos cargando un dataset clásico en sistemas de recomendación: MovieLens (https://movielens.org/). Esta es una página de recomendación de películas que abrió información histórica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS9o7coRHY7B",
    "outputId": "8eb5e300-e806-416b-c781-91d60966fd09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1619416 sha256=d302574156fc55e868203a66a2ea44148c537e35da283718662d6f804d287848\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise, surprise\n",
      "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n",
      "--2021-09-17 20:35:12--  https://files.grouplens.org/datasets/movielens/ml-100k/u.data\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1979173 (1.9M)\n",
      "Saving to: ‘u.data’\n",
      "\n",
      "u.data              100%[===================>]   1.89M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-09-17 20:35:12 (14.3 MB/s) - ‘u.data’ saved [1979173/1979173]\n",
      "\n",
      "--2021-09-17 20:35:12--  http://./\n",
      "Resolving . (.)... failed: No address associated with hostname.\n",
      "wget: unable to resolve host address ‘.’\n",
      "FINISHED --2021-09-17 20:35:12--\n",
      "Total wall clock time: 0.3s\n",
      "Downloaded: 1 files, 1.9M in 0.1s (14.3 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise\n",
    "# Bajamos el dataset. En windows pueden descargarlo entrando al link manualmente\n",
    "!wget https://files.grouplens.org/datasets/movielens/ml-100k/u.data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "upr4K6xdHTKs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LXsGZdW3HcO5"
   },
   "outputs": [],
   "source": [
    "mlens = pd.read_csv(\"u.data\",sep=\"\\t\",header=None)\n",
    "mlens.columns = [\"user_id\",\"item_id\",\"rating\",\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2s79cYd-HcRU"
   },
   "outputs": [],
   "source": [
    "mlens = mlens.drop(\"timestamp\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKH6q03DHf7-"
   },
   "source": [
    "El paquete surprise no recibe directamente un objeto DataFrame sino que tiene para parsear y leer un conjunto de datos debe hacerlo a través de dos nuevos objetos: Reader y Dataset. En Reader debemos especificar el valor mínimo y el valor máximo de los ratings y Dataset nos permite leer datos desde distintas fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PWFx0PlDHcTh"
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "reader = Reader(rating_scale=(mlens[\"rating\"].min(),mlens[\"rating\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ci2dBtFjHcV2"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_df(mlens,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqFjIQG1HhpD",
    "outputId": "2c15ad9c-4385-4ff2-b72d-49baa81ea032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fddcb3ea250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cRkUQM1HjpH"
   },
   "source": [
    "Ahora cargue SVD y GridSearchCV, ambos de surprise.  \n",
    "Nota: GridSearchCV no está en surprise.GridSearchCV, surprise.GridSearch está deprecado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y6vJ1l67HhrV"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9rASOiaHlx0"
   },
   "source": [
    "Genere una grilla de parámetros donde se prueben distintas combinaciones de:  \n",
    "  - epochs: es la cantidad de pasadas sobre el dataset que hará el algoritmo empleando descenso por el gradiente  \n",
    "  - biased: usar parámetros de sesgo o no  \n",
    "  - lr_all: learning rate para todos los parámetros  \n",
    "  - reg_all: término de regularización para todos los parámetros (lambda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l0b0EXhuHk2b"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005], 'reg_all': [0.4, 0.6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0ixlU6SHq1A"
   },
   "source": [
    "Emplee GridSearchCV, SVD y el diccionario con los parámetros para probar, y entrene un modelo. Note que a GridSearchCV necesita pasarle un modelo sin instanciar. Además, setee el parámetro refit a True y con measures = [\"rmse\",\"fcp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V7o5DBaRHnSl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ONYXq-WvHsFc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyuSp5wDHt4C"
   },
   "source": [
    "Imprima el rmse y el fcp, y la mejor combinación de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "79Xy07GyHsIS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ezZrFNPqHwS5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGIRLMlQHwZV"
   },
   "source": [
    "Guarde el modelo con mayor fcp y prediga el rating para el user id 196 e item id 242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JlbNlPM8HweY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lx3-D3WpH1Dx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qGwaio03H2P0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4BTjjQzH5ux"
   },
   "source": [
    "Pruebe empleando otros modelos como SVDpp, NMF, KNNWithZScore e intente superar el valor obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6OSuKTDXH7Cj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW6k_jrD-8gE"
   },
   "source": [
    "## Recomendación basada en el contenido\n",
    "\n",
    "En este ejemplo vamos a tomar un corpus de textos de autores latinoamericanos para sugerir uno similar a uno dado. Para esto construiremos una matriz TFIDF, de frecuencias normalizadas de términos por documento, y usaremos la similitud coseno para medir distancias entre los distintos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNwDm-RF-923",
    "outputId": "9601651c-fe7c-4983-c55c-24f9e6012ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'borges'...\n",
      "remote: Enumerating objects: 211, done.\u001b[K\n",
      "remote: Counting objects: 100% (211/211), done.\u001b[K\n",
      "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
      "remote: Total 211 (delta 89), reused 171 (delta 49), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (211/211), 2.21 MiB | 11.61 MiB/s, done.\n",
      "Resolving deltas: 100% (89/89), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karen-pal/borges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXF23-Eu--_3",
    "outputId": "4de5d765-9014-43cc-e42d-d6e4614393bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(719, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# usando el asterisco de \"wildcard\" traemos todos los archivos en formato pickle\n",
    "pkls = Path('.').glob('./borges/datasets/*texts.pkl')\n",
    "\n",
    "# leemos todos los pickles y concatenarlos en un DataFrame\n",
    "for pkl in pkls:\n",
    "    with open(pkl, 'rb') as inp:\n",
    "        df_ = pickle.load(inp)\n",
    "    df = pd.concat([df, df_])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "4YD4iOYeYtkr",
    "outputId": "83e8a970-268c-4a77-aa7d-2213715d41e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>text_metadata</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://ciudadseva.com/texto/la-primera-nevada/</td>\n",
       "      <td>{'title': 'La primera nevada', 'metadata': '[C...</td>\n",
       "      <td>Los objetos que me dejó Torroba se fueron inco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://ciudadseva.com/texto/el-prodigioso-mil...</td>\n",
       "      <td>{'title': 'El prodigioso miligramo', 'metadata...</td>\n",
       "      <td>Una hormiga censurada por la sutileza de sus c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 link  ...                                               text\n",
       "7     https://ciudadseva.com/texto/la-primera-nevada/  ...  Los objetos que me dejó Torroba se fueron inco...\n",
       "17  https://ciudadseva.com/texto/el-prodigioso-mil...  ...  Una hormiga censurada por la sutileza de sus c...\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2ex6sjJe_p3f"
   },
   "outputs": [],
   "source": [
    "# separamos de la metadata el título y autor en sus propias columnas\n",
    "df['title'] = df['text_metadata'].apply(lambda x: x['title'])\n",
    "df['author'] = df['text_metadata'].apply(lambda x: x['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-1IHgFs_q_8",
    "outputId": "b5b36750-d537-4923-b4b5-1055246f9661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jorge Luis Borges             60\n",
       "Julio Cortázar                55\n",
       "Baldomero Lillo               50\n",
       "Augusto Monterroso            45\n",
       "Juan José Arreola             45\n",
       "Alfonso Reyes                 37\n",
       "Enrique Anderson Imbert       36\n",
       "Mario Benedetti               33\n",
       "Julio Ramón Ribeyro           27\n",
       "Roberto Arlt                  25\n",
       "Clarice Lispector             25\n",
       "Julio Torri                   23\n",
       "Felisberto Hernández          15\n",
       "Luis Vidales                  14\n",
       "Adolfo Bioy Casares           13\n",
       "Rubén Darío                   13\n",
       "Álvaro Mutis                  11\n",
       "Edmundo Valadés               10\n",
       "Juan Rulfo                    10\n",
       "Juan Rodolfo Wilcock          10\n",
       "Elena Garro                    9\n",
       "Manuel A. Alonso               9\n",
       "Salarrué                       9\n",
       "Juan Bosch                     8\n",
       "Alejo Carpentier               8\n",
       "Eduardo Gudiño Kieffer         8\n",
       "Virgilio Díaz Grullón          7\n",
       "Andrés Rivera                  7\n",
       "Silvina Ocampo                 7\n",
       "Rodolfo Walsh                  6\n",
       "Ricardo Güiraldes              6\n",
       "Pablo Palacio                  5\n",
       "Manuel Romero de Terreros      5\n",
       "José Donoso                    5\n",
       "Julio Garmendia                4\n",
       "[Cuento - Texto completo.]     4\n",
       "Gregorio López y Fuentes       4\n",
       "Amparo Dávila                  3\n",
       "María Luisa Bombal             3\n",
       "José María Arguedas            3\n",
       "Sergio Pitol                   3\n",
       "Octavio Paz                    3\n",
       "José Edwards                   3\n",
       "Rómulo Gallegos                3\n",
       "Rubén Bareiro Saguier          3\n",
       "Inés Arredondo                 3\n",
       "Macedonio Fernández            3\n",
       "Virgilio Piñera                3\n",
       "Vicente Huidobro               3\n",
       "José Lezama Lima               3\n",
       "Leonora Carrington             2\n",
       "Santiago Dabove                2\n",
       "Teresa de la Parra             2\n",
       "Humberto Arenal                2\n",
       "Ricardo Jaimes Freyre          1\n",
       "Manuel González Zeledón        1\n",
       "Esteban Echeverría             1\n",
       "Carmen Lyra                    1\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vemos los autores disponibloes\n",
    "df['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9inGyjhv_rCF",
    "outputId": "59ee28a7-2e05-410f-a698-e564135307df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quitamos duplicados y reiniciamos el índice\n",
    "df = df.drop_duplicates(subset=[c for c in df.columns if c != 'text_metadata'])\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4wCrn4jU_rEc"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbLpYY_iY5d0"
   },
   "source": [
    "Vamos a calcular las matrices de ocurrencias de términos usando sklearn.\n",
    "\n",
    "Ámbas clases primero construyen el vocabulario total, y luego:  \n",
    "- **CountVectorizer** nos devuelve la frecuencia absoluta de cada término por cada documento.\n",
    "- [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf): calcula la frecuencia de cada término por documento, y normaliza por el total de documentos donde el término aparece.\n",
    "\n",
    "$${tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}$$\n",
    "\n",
    "$$\n",
    "idf( t, D ) = log \\frac{ \\text{| } D \\text{ |} }{ 1 + \\text{| } \\{ d \\in D : t \\in d \\} \\text{ |} }\n",
    "$$ \n",
    "\n",
    "\n",
    "$$ tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D ) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIr21dmN_wJS",
    "outputId": "ed27c855-4ad6-4526-b719-31ca7ae395e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1]]\n",
      "['aparecen', 'con', 'contenido', 'de', 'del', 'documento', 'en', 'frecuencias', 'información', 'la', 'las', 'matriz', 'otorga', 'palabras', 'por', 'que', 'relaciona', 'se', 'su', 'tema', 'un']\n"
     ]
    }
   ],
   "source": [
    "# Instanciamos el CV\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "doc1 = 'la matriz de frecuencias por palabras otorga información del contenido de un documento'\n",
    "doc2 = 'las palabras que aparecen en un documento se relaciona con su tema'\n",
    "# Definimos una lista con todos los strings\n",
    "data_corpus = [doc1, doc2]\n",
    "\n",
    "# Fiteamos el CV y transformamos los datos\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "\n",
    "# Pasamos de sparse matrix a array usando .toarray()\n",
    "\n",
    "print(X.toarray())\n",
    "# Usando el metodo .get_feature_names() del CV podemos acceder al indice de palabras\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOphjaAF_w3S",
    "outputId": "b9849ded-87be-43ce-e38a-b51e1ac9e048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = list(stopwords.words('spanish'))\n",
    "# eliminamos las \"stop words\", palabras comunes no informativas\n",
    "tf = TfidfVectorizer(stop_words=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CN_uqifv_w57"
   },
   "outputs": [],
   "source": [
    "# calculamos los features para cada ítem (texto)\n",
    "tfidf_matrix = tf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2IfVii1V_0gK"
   },
   "outputs": [],
   "source": [
    "# calculamos las similitudes entre todos los documentos\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "n = 6\n",
    "\n",
    "# diccionario creado para guardar el resultado en un formato (autor - titulo : puntaje, titulo, autor)\n",
    "results = {} \n",
    "for idx, row in df.iterrows():\n",
    "    # guardamos los indices similares basados en la similitud coseno. Los ordenamos en modo ascendente, siendo 0 nada de similitud y 1 total\n",
    "    similar_indices = cosine_similarities[idx].argsort()[:-n-2:-1] \n",
    "    # guardamos los N más cercanos\n",
    "    similar_items = [(f\"{df['author'][i]} - {df['title'][i]}\", round(cosine_similarities[idx][i], 3)) for i in similar_indices]\n",
    "    results[f\"{row['author']} - {row['title']}\"] = similar_items[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8rlNnYF_1PE",
    "outputId": "5dc1fe47-00c6-4112-e28c-b778314a3a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jorge Luis Borges - La escritura del dios', 0.144),\n",
      " ('Jorge Luis Borges - El inmortal', 0.135),\n",
      " ('Jorge Luis Borges - Utopía de un hombre que está cansado', 0.125),\n",
      " ('Felisberto Hernández - El acomodador', 0.122),\n",
      " ('Clarice Lispector - La búsqueda de la dignidad', 0.121),\n",
      " ('Jorge Luis Borges - Funes el memorioso', 0.11)]\n"
     ]
    }
   ],
   "source": [
    "pprint(results['Jorge Luis Borges - El Aleph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q7U9jPXJ_3j9"
   },
   "outputs": [],
   "source": [
    "def recomendar(autor, titulo):\n",
    "    pprint(results[f\"{autor} - {titulo}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NV66PpBs_4iY",
    "outputId": "e2fdca9b-3537-4399-b2c2-b7e3d53886cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Felisberto Hernández - El acomodador', 0.134),\n",
      " ('Felisberto Hernández - El cocodrilo', 0.101),\n",
      " ('Felisberto Hernández - Menos Julia', 0.089),\n",
      " ('Julio Cortázar - Después del almuerzo', 0.088),\n",
      " ('Julio Cortázar - La noche boca arriba', 0.086),\n",
      " ('Julio Cortázar - La señorita Cora', 0.086)]\n"
     ]
    }
   ],
   "source": [
    "recomendar('Julio Cortázar', 'Axolotl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RecSys.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
