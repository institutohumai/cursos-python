#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/MachineLearning/1_Introduccion/rls.ipynb"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>
# <div align="center"> Record치 abrir en una nueva pesta침a </div>

# # Regresi칩n Lineal Simple
# 
# La RLS, es la aproximaci칩n m치s simple al aprendizaje supervisado. En particular, la regresi칩n lineal es una herramienta 칰til para predecir una respuesta cuantitativa. 
# 
# Es un m칠todo que tiene muchos a침os y est치 presente en toda la bibliograf칤a.
# 
# Aunque parezca super simple comparado con las t칠cnicas modernas de machine learning, la regresi칩n lineal a칰n es un m칠todo 칰til y ampliamente usado. 
# 
# Principalmente, sirve como un buen punto de partida para aproximaciones m치s nuevas: muchas de las t칠cnicas **fancy** pueden interpretarse como generalizaciones o extensiones de la regresi칩n lineal. 
# 
# Por lo tanto es s칰per importante tener una buena compresi칩n de la regresi칩n lineal antes de estudiar los algoritmos m치s complejos de machine learning. 
# 
# 
# ## Dataset Advertising
# 
# Supongamos que que somos consultores estad칤sticos, y nos contratan con el objetivo de aumentar las ventas de un determinado producto. 
# El dataset Advertising consiste en las ventas del producto en 200 mercados, y el presupuesto dedicado en publicidad en 3 medios: TV, radio y diario.
# 
# Si logramos identificar una relaci칩n entre la inversi칩n en publicidad y las ventas, podremos recomendarle a nuestro cliente hacia d칩nde debe dirigir su inversi칩n en publicidad.
# 
# La variables predictoras ser치n los presupuestos para cada canal y la variable de respuesta ser치 las ventas.
# 
# <u>Exploremos un poco los datos:</u>

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
plt.rcParams["figure.figsize"] = (20,5)


# In[2]:


df = pd.read_csv('https://datasets-humai.s3.amazonaws.com/datasets/advertising.csv')


# In[3]:


df.head()


# Veamos la relaci칩n entre las ventas y la publicidad en cada uno de los medios

# In[4]:


fig, (ax1, ax2, ax3) = plt.subplots(1, 3)
df.plot.scatter(x='TV', y='Sales', ax=ax1)
df.plot.scatter(x='Radio', y='Sales', ax=ax2)
df.plot.scatter(x='Newspaper', y='Sales', ax=ax3);


# Pensemos en estos datos. Algunas preguntas que podr칤an surgir:
# <ul>
# <li>쮿ay alguna relaci칩n entre el presupuesto en publicidad y las ventas?</li>
# <li>쯈u칠 tan fuerte es esa relaci칩n?</li>
# <li>쮺u치les de los medios mencionados contribuyen a las ventas?</li>
# <li>쮺on cu치nta precisi칩n podemos predecir las ventas futuras?</li>
# <li>쮼s esta relaci칩n lineal?</li>
# </ul>
# 
# Resulta que la regresi칩n lineal puede ser usada para responder cada una de estas preguntas y algunas m치s. 
# Veamos algunos conceptos y luego intentaremos responderlas.
# 
# La regresi칩n lineal simple intenta predecir una respuesta cuantitativa Y en base a una 칰nica variable predictora X. 
# Asume que hay aproximadamente una relaci칩n lineal entre X e Y. 
# 
# Matem치ticamente:  $$ Sales  \approx \hat {\beta}_{0} +  \hat {\beta}_{1} TV $$
# 
# 洧띻0 y 洧띻1 son dos constantes que representan el intercepto y la pendiente en el modelo lineal. 
# 
# Juntos, 洧띻0 y 洧띻1 son conocidos como los **par치metros del modelo**. 
# 
# Una vez que hemos usado nuestro set de entrenamiento para producir los estimadores     y    para los coeficientes del modelo, podemos predecir futuras ventas en base a un valor particular de TV.
# 
# ## 쮺칩mo calculamos los par치metros del modelo?
# 
# Vamos a elegir el par 洧띻0 y 洧띻1 tales que minimizan la distancia entre la l칤nea recta y los verdaderos valores que observamos:
# 
# <img src="https://i.ibb.co/8c2zbDy/mco.png" alt="Girl in a jacket" width="80%">
# 
# Ahora con Python:
# 

# In[5]:


from sklearn.linear_model import LinearRegression


# In[6]:


#Mi modelo ser치 una instancia de la clase LinearRegression (춰Recuerden Programaci칩n Orientada a Objetos!)
model = LinearRegression(fit_intercept=True)


# In[7]:


# Definimos la "X" y la "y" con las que vamos a entrenar nuestro modelo 
X = df.loc[:,['TV']]
y = df['Sales']


# Noten que alrededor de tv hay dos corchetes, mientras que alrededor de Sales hay uno s칩lo. 
# 
# Miren lo siguiente:

# In[8]:


type(X)


# In[9]:


type(y)


# In[10]:


X.shape


# In[11]:


y.shape


# En scikit learn las variables explicativas se expresan en un DataFrame y la variable explicada es siempre una serie. 

# In[12]:


#Los coeficientes (Betas) del modelo todav칤a no est치n definidos
model.coef_


# In[13]:


# Usamos el m칠todo fit para entrenar el modelo


# In[14]:


model.fit(X,y)


# In[15]:


model.coef_


# In[16]:


model.intercept_


# <strong> 쮺칩mo interpretamos estos coeficientes? </strong>
# 
# $ \hat {\beta}_{0} = 6.9748214882298925 $
# 
# Este coeficiente indica que cuando la publicidad en TV es de 0, de todas maneras las ventas son de 6.97 unidades.
# 
# $ \hat {\beta}_{1} = 0.05546477 $
# 
# Este coeficiente indica que cuando agregamos 1 unidad de publicidad en TV, las ventas aumentan en 0.05 unidades.
# 

# ## Ejercicio
# 
# 쮺u치ntas ventas esperar칤amos con una inversi칩n en televisi칩n de 4 unidades?

# In[ ]:





# In[ ]:





# # Precisi칩n de los coeficientes estimados
# 
# La matem치tica que soporta la regresi칩n lineal simple, se basa en suponer que la variable explicativa (X) y la explicada (y) guardan una relaci칩n lineal perfecta perturbada por **ruido aleatoreo**: fen칩menos que no podemos o no queremos explicar dentro del modelo y que no dependen de X.
# 
# Los fen칩menos del mundo real nunca son exactamente as칤, pero vamos a encontrar que esta simplificaci칩n es 칰til en muchos casos para, por ejemplo, estudiar la relaci칩n entre X e y. 
# 
# Lo bueno de Python es que podemos simular datos que s칤 cumplen estrictamente este supuesto de linealidad + ruido aleatoreo y observar qu칠 pasa con las derviaciones estad칤sticas. 
# 
# Supongamos que el precio de los departamentos de una ciudad es de 10000usd de base + usd2000/m2 m치s una perturbaci칩n aleatoria. Nuestra ciudad est치 compuesta por 1000 departamentos. 
# 
# Vamos a simular esa poblaci칩n:

# In[19]:


# Las superficies de los departamentos se distribuyen normalmente y 
# tienen una media de 100 mts2 con un desv칤o est치ndar de 20mts2
superficies = np.random.normal(loc=100, scale=20, size=1000).astype(int)
print(superficies[0:30])


# In[20]:


# Los errores aleatorios tienen un promedio de $0 y un desv칤o est치ndar de usd3000
errores = np.random.normal(loc=0, scale=80000, size=1000).astype(int)
print(errores[0:30])


# In[21]:


# Generamos nuestra "poblaci칩n" de 1000 departamentos
precios_departamentos = (superficies * 2000 + 10000 + errores).astype(int)
print(precios_departamentos[0:30])


# Ahora supongamos que somos un grupo de relevadoras de precios y esa poblaci칩n es completamente desconocida para nosotras. Tenemos la posiblidad de tocar el timbre a algunos vecinos de la ciudad y preguntarles cu치nto pagaron por su casa, pero esto nos cuesta tiempo y esfuerzo. 
# 
# Nos preguntamos entonces:
# 
# 
# 
# * Dada una casa de 100mts2 쮺u치l es su precio? 쮺u치nta confianza puedo tener en ese valor? 쯏 dada una casa de 500mts2?
# * 쯇uedo afirmar con X% de confianza, que a mayor n칰mero de mts2 mayor precio?
# * 쮺u치ntas casas tenemos que conocer para poder estimar los precios con un X% de confianza?
# * 쮺u치ntas casas tenemos que conocer para entender cu치nto influyen los mts2 en el precio con un X% de confianza?
# 
# Todas estas preguntas se pueden responder si suponemos que en nuestra poblaci칩n se cumplen los supuestos de la regresi칩n lineal (vamos a entrar en detalle en la pr칩xima clase) y aplicamos t칠cnicas estad칤sticas. 

# ## 1. La confianza en las predicciones
# 
# 쯈u칠 pasa si tomamos una muestra de 30 departamentos? 쮺칩mo se ver칤a nuestra regresi칩n?

# In[22]:


df_poblacion = pd.DataFrame({'superficies':superficies,'precios':precios_departamentos})

df_muestra = df_poblacion.sample(30)


# In[23]:


model.fit(df_muestra[['superficies']],df_muestra['precios'])


# In[24]:


coeficiente = model.coef_
print(coeficiente)


# In[25]:


f'Seg칰n el modelo que podemos construir con esta muestra, por cada mts2 de superficie, el precio aumenta ${coeficiente[0]}'


# 쯈u칠 pasa si tomamos otra muestra?

# In[31]:


df_muestra = df_poblacion.sample(30)
model.fit(df_muestra[['superficies']],df_muestra['precios'])
print(model.coef_)


# Ahora tomemos 100 muestras y vamos a graficarlas. Tambi칠n veamos en rojo la verdadera funci칩n generadora de los datos: 
# 
# precio_venta = 10000 + 2000 * superficia

# In[32]:


for i in range(100):
    # Tomamos una muesta de 30 departamentos
    df_muestra = df_poblacion.sample(30)
    # Entrenamos el modelo sobre la muestra
    model.fit(df_muestra[['superficies']],df_muestra['precios'])
    # Utilizamos al modelo para predecir los valores de todos los departamentos
    predicciones = model.predict(df_poblacion[['superficies']])
    # Graficamos cada una de las 100 regresiones
    plt.plot(df_poblacion['superficies'],predicciones,color='blue',alpha=0.1)

proceso_generador_perfecto = 10000 + df_poblacion['superficies'] * 2000
plt.plot(df_poblacion['superficies'],proceso_generador_perfecto,color='red')
plt.show()


# Todas las regresiones son distintas, pero las predicciones se parecen mucho m치s alrededor de 100 que en los extremos Recuerdan cu치l era la superficie promedio de los departamentos en nuestra ciudad? 
# 
# 
# ### Conclusi칩n 1
# 
# Las predicciones son m치s precisas cerca del centroide de los datos que en los extremos. En otras palabras, nuestro modelo conoce bien lo que vio y m치s all치 de eso, s칩lo puede hacer extrapolaciones cada vez m치s imprecisas.
# 
# 쯈u칠 pasa si en lugar de 30, tomamos muestras m치s grandes? Es decir, aunque cueste m치s esfuerzo hacemos un relevamiento m치s exahustivo...

# In[33]:


for i in range(100):
    # Ahora tomamos una muesta de 150 departamentos
    df_muestra = df_poblacion.sample(150)
    # Entrenamos el modelo sobre la muestra
    model.fit(df_muestra[['superficies']],df_muestra['precios'])
    # Utilizamos al modelo para predecir los valores de todos los departamentos
    predicciones = model.predict(df_poblacion[['superficies']])
    # Graficamos cada una de las 100 regresiones
    plt.plot(df_poblacion['superficies'],predicciones,color='blue',alpha=0.1)

proceso_generador_perfecto = 10000 + df_poblacion['superficies'] * 2000
plt.plot(df_poblacion['superficies'],proceso_generador_perfecto,color='red')
plt.show()


# ### Conclusi칩n 2
# Si tomamos muestras m치s grandes, las regresiones son todas m치s parecidas entre s칤.
# 
# La interpretaci칩n estad칤stica de esta incerteza en las predicciones, est치 dada por los intervalos de confianza. 
# Algunas librer칤as de Python permiten calcular los intervalos de confianza de un modelo. No es el caso de scikit learn porque esta librer칤a est치 pensada para machine learning en general, no s칩lo para regresiones y busca crear una interfaz com칰n para todos los modelos. 
# 
# Para acceder a estimaciones estad칤sticas como los intervalos de confianza, en el pr칩ximo encuentro vamos a utilizar statsmodel.
