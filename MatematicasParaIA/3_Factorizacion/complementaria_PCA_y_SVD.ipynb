{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/MatematicasParaIA/3_Factorizacion/complementaria_PCA_y_SVD.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-YxbmC0oR-x"
      },
      "source": [
        "# Descomposición en Valores Singulares (_SVD_) y Análisis de Componentes Principales (_PCA_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI6tN-ESojRI"
      },
      "source": [
        "¿Qué es Principal Component Analysis (PCA)?\n",
        "\n",
        "Es un método que nos permite reescribir nuestros datos mediante una transformación lineal de las variables de origen. La nueva base conserva vectores ortogonales cuya covarianza entre diferentes variables es cero (son independientes en términos de la correlación)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdC0n2YJorsH"
      },
      "source": [
        "¿Para qué usamos PCA?\n",
        "\n",
        "* Reducción de dimensionalidad\n",
        "* Análisis exploratorio de datos\n",
        "* Modelos predictivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQtfqE5To5-w"
      },
      "source": [
        "### Datos representados por matrices\n",
        "\n",
        "Supongamos que tenemos una matriz $X$ de dimensiones $n×p$, donde $n$ es el número de muestras y $p$ es el número de variables que describen cada una de las muestras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmkoR5O6o_G4"
      },
      "source": [
        "La idea central es generar un **Cambio de base** de las variables originales a partir de una **transformación lineal** que cumpla ciertas características. Los vectores que forman la nueva base se denominan **componentes principales**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCE85Ix_pWZu"
      },
      "source": [
        "### Covarianza entre variables y varianza explicada por un componenete principal\n",
        "\n",
        "La matriz de covarianza $C$ de dimensiones $pxp$ viene dada por\n",
        "\n",
        "$$ C=X^{T}X/(n−1)$$ \n",
        "\n",
        "Es una matriz simétrica por lo que es diagonalizable:\n",
        "\n",
        "$$C=VLV^T$$\n",
        "\n",
        "donde $V$ es una matriz de autovectores (cada columna es un autovector) y $L$ es una matriz diagonal con los autovalores $\\lambda_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNjaRB5isZOW"
      },
      "source": [
        "Los autovectores se denominana componenetes principales del conjunto de datos e implican un cambio de base (transfomración lineal) respecto de las variables originales. \n",
        "\n",
        "Este cambio de base busca encontrar las direcciones donde la varianza de cada variable es máxima y la covarianza entre dos variables diferentes es cero.\n",
        "\n",
        "Cada autovector o componenete principal de $X$ se forma con los \"pesos\" que tiene de cada variable original de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XW9TAVEoc6eC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNDWo0mbdAD2"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "np.shape(iris.data)\n",
        "print(iris.feature_names)\n",
        "X=iris.data\n",
        "for i in range(3):\n",
        "  X[:,i]=X[:,i]-np.mean(X[:,i])\n",
        "np.mean(X[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxatvs47dxfe"
      },
      "outputs": [],
      "source": [
        "cov_data = np.dot(X.T, X) / (n-1)\n",
        "cov_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQPxBNBhd7bk"
      },
      "outputs": [],
      "source": [
        "img = plt.matshow(cov_data, cmap=plt.cm.rainbow)\n",
        "plt.colorbar(img, ticks = [-1, 0, 1], fraction=0.045)\n",
        "for x in range(cov_data.shape[0]):\n",
        "    for y in range(cov_data.shape[1]):\n",
        "        plt.text(x, y, \"%0.2f\" % cov_data[x,y], size=12, color='black', ha=\"center\", va=\"center\")\n",
        "        \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IVuYtwDdiaBT"
      },
      "outputs": [],
      "source": [
        "# Matriz de datos centrada en cero\n",
        "n, m = X.shape\n",
        "# Calculamos matriz de covarianza\n",
        "C = np.dot(X.T, X) / (n-1)\n",
        "# Cálculo de autovectores y autovalores\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(C)\n",
        "# Proyectamos los datos en la base de autovectores\n",
        "X_pca = np.dot(X, eigen_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnGLNtOEvivA"
      },
      "source": [
        "Autovectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNAkLYdrkNkh"
      },
      "outputs": [],
      "source": [
        "eigen_vecs[:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw3362CevlBI"
      },
      "source": [
        "Autovalores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDr7BerMkMEJ",
        "outputId": "a3158ff5-1579-46aa-9fab-e4bfa212b565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.48787243, 1.23308635, 0.05891336, 0.24113905])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eigen_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr9bcqzpvoi_"
      },
      "source": [
        "Componenetes principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDKOO6JsjSjD"
      },
      "outputs": [],
      "source": [
        "X_pca[0:9,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRgYHiGAvtCc"
      },
      "source": [
        "Recalculamos la matriz de covarianza para las nuevas variables (las proyecciones de cada muestra sobre cada autovector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1r7jidxoGgR"
      },
      "outputs": [],
      "source": [
        "# Calculamos la matriz de covarianza para los datos en la nueva base de autovectores\n",
        "cov_data = np.dot(X_svd.T, X_svd) / (n-1)\n",
        "cov_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeJBxa5XoKhx"
      },
      "outputs": [],
      "source": [
        "img = plt.matshow(cov_data, cmap=plt.cm.rainbow)\n",
        "plt.colorbar(img, ticks = [-1, 0, 1], fraction=0.045)\n",
        "for x in range(cov_data.shape[0]):\n",
        "    for y in range(cov_data.shape[1]):\n",
        "        plt.text(x, y, \"%0.2f\" % cov_data[x,y], size=12, color='black', ha=\"center\", va=\"center\")\n",
        "        \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-8xZdbDoP9R"
      },
      "source": [
        "## Implementando PCA mediante SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAIhVuVaoQ95"
      },
      "source": [
        "Si realizamos una descomposiciòn en valores singulares para los datos almacenados en la matriz $X$,  obtenemos la siguiente factorización:\n",
        "\n",
        "$$X=USV^⊤$$\n",
        "\n",
        "Donde $U$ es una matriz unitaria y $S$ es la matriz diagonal de valores singulares. Usando esta expresión para $X$ podemos calcular la matriz de covarianza $C$\n",
        "\n",
        "$$C=VSU^⊤USV^{⊤}/(n−1)=V\\frac{S}{n−1} V^{⊤} $$\n",
        "\n",
        "Dado que la descoposición de $C$ es única, podemos relacionar dicha factorización con la que teníamos para $C$ de lo cuál se concluye que los vectores singulares $V$ de la derecha de la expresión son las direcciones principales (o componenetes principales) y que los valores singulares están relacionados con los autovalores de la matriz de covarianza $C$ mediante la siguiente expresión:\n",
        "$$λ_i=\\frac{S^2_i}{(n−1)}$$\n",
        "\n",
        "Los componenetes principales están dados por\n",
        "\n",
        "$$XV=USV^⊤V=US$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8q-bF5YvUps"
      },
      "source": [
        "### Utilizando SVD para implementar PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmuAh7CHidfi"
      },
      "outputs": [],
      "source": [
        "# Matriz de datos X\n",
        "n, m = X.shape\n",
        "# Calculamos SVD\n",
        "U, Sigma, Vh = np.linalg.svd(X, \n",
        "full_matrices=False, \n",
        "compute_uv=True)\n",
        "# Transformamos X con los componenentes de SVD\n",
        "X_svd = np.dot(U, np.diag(Sigma))\n",
        "X_svd[0:9,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpC2yupEv6fc"
      },
      "source": [
        "### Seleccionando los componenetes principales según la varianza explicada requerida\n",
        "\n",
        "Si uno requiere implementar una selección de features, puede realizar un PCA y retener las variables (autovectores) que impliquen un porcentaje de la varianza total de los datos que sea acorde al grado de precisión de reconstrucción de los datos requerida.\n",
        "\n",
        "Si pretendo reconstruir los datos originales con un 95% de precisión, tengo que tomar tantos autovectores como sea necesario para que la suma de la varianza total del conjunto de autovectores seleccionados alcance un 95% de la varianza total de los datos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Práctica complementaria PCA y SVD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
