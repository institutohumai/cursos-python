{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/3_Embeddings/2_Entrenamiento_Word2Vec.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "Mb76_9llbCZ5"
      },
      "id": "Mb76_9llbCZ5"
    },
    {
      "cell_type": "markdown",
      "id": "7f8d1615",
      "metadata": {
        "origin_pos": 0,
        "id": "7f8d1615"
      },
      "source": [
        "# Implementación del Entrenamiento de Word2Vec\n",
        "\n",
        "Ahora que conocemos los detalles técnicos de los modelos word2vec y los métodos de entrenamiento aproximados, veamos sus implementaciones. Específicamente, tomaremos como ejemplo el modelo skip-gram con muestreo negativo. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb6c160",
      "metadata": {
        "origin_pos": 3,
        "id": "7cb6c160"
      },
      "source": [
        "# Cargando el Dataset\n",
        "\n",
        "Para extraer la información de la Semántica Distribucional necesaria para entrenar embeddings en Español, usaremos el libro \"El ingenioso hidalgo don Quijote de la Mancha\". Cabe aclarar que este libro será usado solamente para que se entiendan las salidas de cada uno de los pasos de este proceso. En la práctica necesitamos un Dataset MUCHÍSIMO MÁS GRANDE para generar embeddings coherentes como la Wikipedia entera, pero eso llevaría mucho tiempo y no es el objetivo de esta clase.\n",
        "\n",
        "Las siguientes celdas de código descargan el libro, leen el archivo en listas de oraciones y generan el vocabulario de torchtext como hemos hechos en clases anteriores.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/2000/2000-0.txt\n",
        "!mv \"2000-0.txt\" \"quijote.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XNEFFgt3Zvo",
        "outputId": "c504823c-0638-4aac-b650-d120b8c26781"
      },
      "id": "8XNEFFgt3Zvo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-22 18:24:17--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2226045 (2.1M) [text/plain]\n",
            "Saving to: ‘2000-0.txt’\n",
            "\n",
            "2000-0.txt          100%[===================>]   2.12M  4.43MB/s    in 0.5s    \n",
            "\n",
            "2022-08-22 18:24:18 (4.43 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt(filename, n_ignored):\n",
        "    \"\"\"\n",
        "    Carga un archivo txt en una lista de oraciones.\n",
        "    A su vez, cada oración es una lista de tokens separados por espacio.\n",
        "    Ignora las primeras n_ignored lineas.\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        raw_text = f.read()\n",
        "    return [line.split() for i,line in enumerate(raw_text.split('\\n')) if i>=n_ignored]\n",
        "\n",
        "oraciones_quijote = read_txt(\"quijote.txt\",28)\n",
        "\n",
        "for i, oracion in enumerate(oraciones_quijote):\n",
        "  if i > 10:break\n",
        "  print(oracion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdzVEk_S3zIs",
        "outputId": "ce3237bb-1d1a-40f1-9052-57ea7a36904d"
      },
      "id": "pdzVEk_S3zIs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'ingenioso', 'hidalgo', 'don', 'Quijote', 'de', 'la', 'Mancha']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['por', 'Miguel', 'de', 'Cervantes', 'Saavedra']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['El', 'ingenioso', 'hidalgo', 'don', 'Quijote', 'de', 'la', 'Mancha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from collections import Counter, OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "def make_vocab(oraciones,min_freq=1):\n",
        "  #Comprueba que oraciones es una lista de listas\n",
        "  if oraciones and isinstance(oraciones[0], list):\n",
        "    #Transforma una lista anidada en una lista simple \n",
        "    tokens = [token for line in oraciones for token in line]\n",
        "  counter_obj = collections.Counter()\n",
        "  counter_obj.update(tokens)\n",
        "  sorted_by_freq_tuples = sorted(counter_obj.items(), key=lambda x: x[1], reverse=True)\n",
        "  ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "  vocabulario = vocab(ordered_dict, min_freq=min_freq)\n",
        "  return vocabulario, ordered_dict\n",
        "\n",
        "vocab_completo, _ = make_vocab(oraciones_quijote)\n",
        "vocab_min_freq_10, ordered_dict = make_vocab(oraciones_quijote,10)\n",
        "\n",
        "print(\"El tamaño del vocabulario sin frecuencia mínima es\",len(vocab_completo))\n",
        "print(\"El tamaño del vocabulario con frecuencia mínima de 10 es\",len(vocab_min_freq_10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDpKTR-y9-X9",
        "outputId": "e20be4d0-f1c3-4453-8df8-8d5de3a9fd5e"
      },
      "id": "HDpKTR-y9-X9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tamaño del vocabulario sin frecuencia mínima es 39766\n",
            "El tamaño del vocabulario con frecuencia mínima de 10 es 3241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "937d6b9b",
      "metadata": {
        "origin_pos": 7,
        "id": "937d6b9b"
      },
      "source": [
        "## Subsampling\n",
        "\n",
        "Los datos de texto suelen tener palabras de alta frecuencia como \"el\", \"un\" y \"en\": que incluso pueden aparecer miles de millones de veces en corpus muy grandes. Sin embargo, estas palabras a menudo coexisten con muchas palabras diferentes en las ventanas de contexto, proporcionando señales poco útiles. Por ejemplo, considere la palabra \"chip\" en una ventana de contexto: intuitivamente, su coexistencia con una palabra de baja frecuencia como \"intel\" es más útil en el entrenamiento que la coexistencia con una palabra de alta frecuencia como \"el\". Además, el entrenamiento con grandes cantidades de palabras (de alta frecuencia) es lento. Por lo tanto, al entrenar modelos de embedding de palabras, las de alta frecuencia se pueden submuestrear. Específicamente, cada palabra indexada $w_i$ en el conjunto de datos se descartará con probabilidad\n",
        "\n",
        "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),$$\n",
        "\n",
        "donde $f(w_i)$ es la relación entre la cantidad de palabras $w_i$ y la cantidad total de palabras en el dataset, y la constante $t$ es un hiperparámetro ($10^{-4}$ en el experimento).\n",
        "Podemos ver que solo cuando la frecuencia relativa $f(w_i) > t$ se puede descartar la palabra (de alta frecuencia) $w_i$, y cuanto mayor sea la frecuencia relativa de la palabra, mayor será la probabilidad de que se descarte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda nos muestra las 10 palabras más frecuentes en el quijote. Como verán son palabras de relleno que pueden encajar en cualquier oración independientemente de sus significado y eso provee muy poco información a la semántia distribucional."
      ],
      "metadata": {
        "id": "ZMQsQXHFL8HP"
      },
      "id": "ZMQsQXHFL8HP"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_frequent_in_vocab(vocab, ordered_dic, n):\n",
        "  return [(item,ordered_dic[item]) \n",
        "      for i, item in enumerate(ordered_dic)\n",
        "      if vocab.__contains__(item) and i<n]\n",
        "\n",
        "get_most_frequent_in_vocab(vocab_min_freq_10,ordered_dict,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeejmHUmBUv4",
        "outputId": "961c0c7a-7457-4fd2-8196-7681971b547e"
      },
      "id": "oeejmHUmBUv4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('que', 19546),\n",
              " ('de', 18132),\n",
              " ('y', 15976),\n",
              " ('la', 10329),\n",
              " ('a', 9627),\n",
              " ('el', 8009),\n",
              " ('en', 7941),\n",
              " ('no', 5620),\n",
              " ('se', 4751),\n",
              " ('los', 4701)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente función elimina palabras frecuentes del dataset siguiendo la fórmula propuesta."
      ],
      "metadata": {
        "id": "p9ONLCyNMSHh"
      },
      "id": "p9ONLCyNMSHh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b507b1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T07:59:59.833831Z",
          "iopub.status.busy": "2022-07-13T07:59:59.833196Z",
          "iopub.status.idle": "2022-07-13T08:00:01.608416Z",
          "shell.execute_reply": "2022-07-13T08:00:01.602958Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "4b507b1d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "def subsample(oraciones):\n",
        "  \"\"\"Subsample high-frequency words.\"\"\"\n",
        "  #Comprueba que oraciones es una lista de listas\n",
        "  if oraciones and isinstance(oraciones[0], list):\n",
        "    #Transforma una lista anidada en una lista simple \n",
        "    tokens = [token for line in oraciones for token in line]\n",
        "  counter_obj = collections.Counter()\n",
        "  counter_obj.update(tokens)\n",
        "  num_tokens = sum(counter_obj.values())\n",
        "\n",
        "  # Devuelve true si hay que conservar el token\n",
        "  def keep(token):\n",
        "      return(random.uniform(0, 1) <\n",
        "              math.sqrt(1e-4 / counter_obj[token] * num_tokens))\n",
        "\n",
        "  return ([[token for token in line if keep(token)] for line in oraciones],\n",
        "          counter_obj)\n",
        "\n",
        "\n",
        "oraciones_quijote_subsampled, counter = subsample(oraciones_quijote)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6fe693c",
      "metadata": {
        "origin_pos": 9,
        "id": "d6fe693c"
      },
      "source": [
        "El siguiente fragmento de código traza el histograma del número de tokens por oración antes y después del submuestreo. Como era de esperar, el submuestreo acorta significativamente las oraciones al eliminar las palabras de alta frecuencia, lo que acelerará el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fbe3bb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:01.616171Z",
          "iopub.status.busy": "2022-07-13T08:00:01.614334Z",
          "iopub.status.idle": "2022-07-13T08:00:01.840211Z",
          "shell.execute_reply": "2022-07-13T08:00:01.839371Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "07fbe3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "a46f6847-397f-45a8-f823-47fd78dd2a65"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
        "    \"\"\"Plot the histogram for list length pairs.\"\"\"\n",
        "    plt.rcParams['figure.figsize'] = (3.5, 2.5)\n",
        "    _, _, patches = plt.hist(\n",
        "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    for patch in patches[1].patches:\n",
        "        patch.set_hatch('/')\n",
        "    plt.legend(legend)\n",
        "\n",
        "show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence',\n",
        "                            'count', oraciones_quijote, oraciones_quijote_subsampled);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e633efee",
      "metadata": {
        "origin_pos": 11,
        "id": "e633efee"
      },
      "source": [
        "Para fichas individuales, la frecuencia de muestreo de la palabra de alta frecuencia \"que\" es inferior a 1/20.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554d7b2e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:01.846193Z",
          "iopub.status.busy": "2022-07-13T08:00:01.845604Z",
          "iopub.status.idle": "2022-07-13T08:00:01.889935Z",
          "shell.execute_reply": "2022-07-13T08:00:01.888553Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "554d7b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "edfcdb10-de73-412f-8fc1-00756d213b41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# of \"que\": before=19546, after=845'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def compare_counts(token):\n",
        "    return (f'# of \"{token}\": '\n",
        "            f'before={sum([l.count(token) for l in oraciones_quijote])}, '\n",
        "            f'after={sum([l.count(token) for l in oraciones_quijote_subsampled])}')\n",
        "\n",
        "compare_counts('que')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10e5674",
      "metadata": {
        "origin_pos": 13,
        "id": "b10e5674"
      },
      "source": [
        "Por el contrario, las palabras de baja frecuencia como \"mesa\" se mantienen por completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb2e72f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:01.895006Z",
          "iopub.status.busy": "2022-07-13T08:00:01.894157Z",
          "iopub.status.idle": "2022-07-13T08:00:01.938427Z",
          "shell.execute_reply": "2022-07-13T08:00:01.937336Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "3bb2e72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa3dc2db-111c-4048-a989-8d2ff47b3f38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# of \"mesa\": before=16, after=16'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "compare_counts('mesa')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e448d77",
      "metadata": {
        "origin_pos": 15,
        "id": "5e448d77"
      },
      "source": [
        "Después del submuestreo, asignamos índices a los tokens y así generamos el corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f4554e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:01.943023Z",
          "iopub.status.busy": "2022-07-13T08:00:01.942155Z",
          "iopub.status.idle": "2022-07-13T08:00:02.415672Z",
          "shell.execute_reply": "2022-07-13T08:00:02.414417Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "20f4554e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434aeec2-1ffb-4651-c5c3-a45d8ba06e78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1388, 684, 735], [1655, 2950], [1388, 684, 735]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "corpus = [[vocab_min_freq_10[x] for x in line if vocab_min_freq_10.__contains__(x)] \n",
        "          for line in oraciones_quijote_subsampled if line != []]\n",
        "corpus[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbb8691",
      "metadata": {
        "origin_pos": 17,
        "id": "ffbb8691"
      },
      "source": [
        "## Extracción de palabras centrales y palabras de contexto\n",
        "\n",
        "La siguiente función get_centers_and_contexts extrae todas las palabras centrales y sus palabras de contexto del corpus. Muestrea uniformemente un número entero entre 1 y max_window_size al azar como el tamaño de la ventana de contexto. Para cualquier palabra central, aquellas palabras cuya distancia no exceda el tamaño de la ventana de contexto muestreada son sus palabras de contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7875cc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:02.421203Z",
          "iopub.status.busy": "2022-07-13T08:00:02.420423Z",
          "iopub.status.idle": "2022-07-13T08:00:02.429884Z",
          "shell.execute_reply": "2022-07-13T08:00:02.428921Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "2c7875cc"
      },
      "outputs": [],
      "source": [
        "def get_centers_and_contexts(corpus, max_window_size):\n",
        "  \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
        "  centers, contexts = [], []\n",
        "  for line in corpus:\n",
        "    # Para formar un par de \"palabra central--palabra de contexto\", \n",
        "    # cada oración debe tener al menos 2 palabras\n",
        "    if len(line) < 2:\n",
        "      continue\n",
        "    centers += line\n",
        "    for i in range(len(line)):  # Ventana de contexto centrada en `i`\n",
        "      window_size = random.randint(1, max_window_size)\n",
        "      indices = list(range(max(0, i - window_size),\n",
        "                            min(len(line), i + 1 + window_size)))\n",
        "      # Excluir la palabra central de las palabras de contexto\n",
        "      indices.remove(i)\n",
        "      contexts.append([line[idx] for idx in indices])\n",
        "  return centers, contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5014c1",
      "metadata": {
        "origin_pos": 19,
        "id": "0a5014c1"
      },
      "source": [
        "A continuación, para ilustrar su funcionamiento, creamos un conjunto de datos artificial que contiene dos oraciones de 7 y 3 palabras, respectivamente. Haremos que el tamaño máximo de la ventana de contexto sea 2 e imprimiremos todas las palabras centrales y sus palabras de contexto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30c4abbd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:02.434770Z",
          "iopub.status.busy": "2022-07-13T08:00:02.433809Z",
          "iopub.status.idle": "2022-07-13T08:00:02.441352Z",
          "shell.execute_reply": "2022-07-13T08:00:02.440303Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "30c4abbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b2f77d-76f8-47a8-8328-5b71b6f5f841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
            "center 0 has contexts [1, 2]\n",
            "center 1 has contexts [0, 2, 3]\n",
            "center 2 has contexts [1, 3]\n",
            "center 3 has contexts [2, 4]\n",
            "center 4 has contexts [3, 5]\n",
            "center 5 has contexts [3, 4, 6]\n",
            "center 6 has contexts [4, 5]\n",
            "center 7 has contexts [8, 9]\n",
            "center 8 has contexts [7, 9]\n",
            "center 9 has contexts [8]\n"
          ]
        }
      ],
      "source": [
        "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
        "print('dataset', tiny_dataset)\n",
        "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
        "    print('center', center, 'has contexts', context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e373284f",
      "metadata": {
        "origin_pos": 21,
        "id": "e373284f"
      },
      "source": [
        "En este ejercicios, estableceremos el tamaño máximo de la ventana de contexto en 5. La siguiente celda extrae del todas las palabras centrales y sus palabras de contexto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba96f80c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:02.444860Z",
          "iopub.status.busy": "2022-07-13T08:00:02.444585Z",
          "iopub.status.idle": "2022-07-13T08:00:04.361159Z",
          "shell.execute_reply": "2022-07-13T08:00:04.359915Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "ba96f80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37373ee3-1a4b-47b5-f690-c032ce585972"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# center-context pairs: 303511'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
        "f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6da5b6d",
      "metadata": {
        "origin_pos": 23,
        "id": "d6da5b6d"
      },
      "source": [
        "## Muestreo negativo\n",
        "\n",
        "Usamos muestreo negativo para entrenamiento aproximado. Para muestrear palabras negativas de acuerdo con una distribución predefinida, definimos la siguiente clase RandomGenerator, donde la distribución de muestreo (posiblemente no normalizada) se pasa a través del argumento sampling_weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3f03a3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:04.366525Z",
          "iopub.status.busy": "2022-07-13T08:00:04.365451Z",
          "iopub.status.idle": "2022-07-13T08:00:04.376698Z",
          "shell.execute_reply": "2022-07-13T08:00:04.375606Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "9f3f03a3"
      },
      "outputs": [],
      "source": [
        "class RandomGenerator:\n",
        "  \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
        "  def __init__(self, sampling_weights):\n",
        "    # Exclude\n",
        "    self.population = list(range(1, len(sampling_weights) + 1))\n",
        "    self.sampling_weights = sampling_weights\n",
        "    self.candidates = []\n",
        "    self.i = 0\n",
        "\n",
        "  def draw(self):\n",
        "    if self.i == len(self.candidates):\n",
        "      # Cache `k` random sampling results\n",
        "      self.candidates = random.choices(\n",
        "          self.population, self.sampling_weights, k=10000)\n",
        "      self.i = 0\n",
        "    self.i += 1\n",
        "    return self.candidates[self.i - 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3bc35a",
      "metadata": {
        "origin_pos": 27,
        "id": "8e3bc35a"
      },
      "source": [
        "\n",
        "Para un par de palabra central y palabra de contexto, muestreamos aleatoriamente `K` (5 en el experimento) palabras negativas. Según las sugerencias del paper de word2vec, la probabilidad de muestreo $P(w)$ de una palabra negativa $w$ se establece en su frecuencia relativa en el diccionario elevada a la potencia de 0,75.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3ff846",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:04.380913Z",
          "iopub.status.busy": "2022-07-13T08:00:04.380635Z",
          "iopub.status.idle": "2022-07-13T08:00:17.812323Z",
          "shell.execute_reply": "2022-07-13T08:00:17.811340Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "fb3ff846"
      },
      "outputs": [],
      "source": [
        "def get_negatives(all_contexts, vocab, counter, K):\n",
        "    \"\"\"Devuelve palabras ruidosas para muestreo negativo.\"\"\"\n",
        "    # Pesos de muestreo para palabras con índices 1, 2, ... \n",
        "    # (índice 0 es el token <unk> excluido) en el vocabulario\n",
        "    tokens = vocab.get_itos()\n",
        "    sampling_weights = [counter[tokens[i]]**0.75\n",
        "                        for i in range(1, len(tokens))]\n",
        "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            neg = generator.draw()\n",
        "            # Las palabras ruidosas no pueden ser de contexto\n",
        "            if neg not in contexts:\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "all_negatives = get_negatives(all_contexts, vocab_min_freq_10, counter, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers[500], all_contexts[500], all_negatives[500]"
      ],
      "metadata": {
        "id": "-EADG70Ckbxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1635e20-fe5f-465a-c7ee-109aeb2daedd"
      },
      "id": "-EADG70Ckbxy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260,\n",
              " [754, 926, 1248],\n",
              " [4, 1510, 240, 78, 290, 971, 5, 2563, 13, 103, 263, 53, 41, 2898, 126])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576cdd18",
      "metadata": {
        "origin_pos": 29,
        "id": "576cdd18"
      },
      "source": [
        "## Cargando ejemplos de entrenamiento en minilotes\n",
        "\n",
        "Una vez extraídas todas las palabras centrales junto con sus palabras de contexto y las palabras negativas muestreadas, transformaremos los datos en minilotes de ejemplos que se pueden cargar de forma iterativa durante el entrenamiento.\n",
        "\n",
        "En un minilote, el ejemplo $i^\\mathrm{th}$ incluye una palabra central, sus palabras de contexto $n_i$ y palabras negativas $m_i$. Debido a los diferentes tamaños de ventana de contexto, $n_i+m_i$ varía para diferentes $i$.\n",
        "Por lo tanto, para cada ejemplo, concatenamos sus palabras de contexto y palabras negativas en la variable `contexts_negatives` y rellenamos con ceros hasta que la longitud de la concatenación alcance $\\max_i n_i+m_i$ (`max_len`).\n",
        "Para excluir los rellenos en el cálculo de la pérdida, definimos una variable de máscara `mask`. Hay una correspondencia de uno a uno entre los elementos en `mask` y los elementos en `context_negative`, donde los ceros (de lo contrario unos) en `mask` corresponden a rellenos en `context_negative`.\n",
        "\n",
        "\n",
        "Para distinguir entre ejemplos positivos y negativos, separamos las palabras de contexto de las palabras negativas en contexts_negatives a través de una variable labels. De forma similar a las máscaras, también existe una correspondencia uno a uno entre los elementos de las etiquetas y los elementos de los contextos_negativos, donde los unos (de lo contrario, los ceros) de las etiquetas corresponden a las palabras de contexto en contexts_negatives.\n",
        "\n",
        "La idea anterior se implementa en la siguiente función `collate_batch`. Sus datos de entrada son una lista con una longitud igual al tamaño del lote, donde cada elemento es un ejemplo que consta de la palabra central `center`, sus palabras de contexto `context` y sus palabras negativas `negative`. Esta función devuelve un minilote que se puede cargar para realizar cálculos durante el entrenamiento, como incluir la variable de máscara.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def collate_batch(data):\n",
        "  max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "  centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "  for center, context, negative in data:\n",
        "    centers += [center]\n",
        "    cur_len = len(context) + len(negative)\n",
        "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "    masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "    labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "  return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
        "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
      ],
      "metadata": {
        "id": "7EioQottmNcp"
      },
      "id": "7EioQottmNcp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cdfe3c01",
      "metadata": {
        "origin_pos": 31,
        "id": "cdfe3c01"
      },
      "source": [
        "Probemos esta función usando un mini lote de dos ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b0576e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:17.826729Z",
          "iopub.status.busy": "2022-07-13T08:00:17.826207Z",
          "iopub.status.idle": "2022-07-13T08:00:17.833712Z",
          "shell.execute_reply": "2022-07-13T08:00:17.832890Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "90b0576e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369451dd-a61c-44af-b7ff-f9cd7deb40cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers = tensor([[1],\n",
            "        [1]])\n",
            "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
            "        [2, 2, 2, 3, 3, 0]])\n",
            "lengths = tensor([[1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 0]])\n",
            "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
        "x_2 = (1, [2, 2, 2], [3, 3])\n",
        "batch = collate_batch((x_1, x_2))\n",
        "\n",
        "names = ['centers', 'contexts_negatives', 'lengths', 'labels']\n",
        "for name, data in zip(names, batch):\n",
        "    print(name, '=', data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09fd6c83",
      "metadata": {
        "origin_pos": 33,
        "id": "09fd6c83"
      },
      "source": [
        "## Juntar todo\n",
        "\n",
        "Por último, definimos la función `load_data_quijote` que lee el quijote y devuelve el iterador de datos y el vocabulario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ebd8fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:17.839455Z",
          "iopub.status.busy": "2022-07-13T08:00:17.838975Z",
          "iopub.status.idle": "2022-07-13T08:00:17.850906Z",
          "shell.execute_reply": "2022-07-13T08:00:17.849834Z"
        },
        "origin_pos": 35,
        "tab": [
          "pytorch"
        ],
        "id": "23ebd8fc"
      },
      "outputs": [],
      "source": [
        "def load_data_quijote(batch_size, max_window_size, num_noise_words):\n",
        "    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n",
        "    oraciones_quijote = read_txt(\"quijote.txt\",28)\n",
        "    vocabulario, ordered_dict = make_vocab(oraciones_quijote,10)\n",
        "    oraciones_quijote_subsampled, counter = subsample(oraciones_quijote)\n",
        "    corpus = [[vocabulario[x] for x in line if vocabulario.__contains__(x)] \n",
        "          for line in oraciones_quijote_subsampled if line != []]\n",
        "    all_centers, all_contexts = get_centers_and_contexts(\n",
        "        corpus, max_window_size)\n",
        "    all_negatives = get_negatives(\n",
        "        all_contexts, vocabulario, counter, num_noise_words)\n",
        "\n",
        "    class QuijoteDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, centers, contexts, negatives):\n",
        "            assert len(centers) == len(contexts) == len(negatives)\n",
        "            self.centers = centers\n",
        "            self.contexts = contexts\n",
        "            self.negatives = negatives\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return (self.centers[index], self.contexts[index],\n",
        "                    self.negatives[index])\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.centers)\n",
        "\n",
        "    dataset = QuijoteDataset(all_centers, all_contexts, all_negatives)\n",
        "\n",
        "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n",
        "                                      collate_fn=collate_batch)\n",
        "    return data_iter, vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d5c061",
      "metadata": {
        "origin_pos": 36,
        "id": "75d5c061"
      },
      "source": [
        "Imprimamos el primer minilote del iterador de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5554f7fa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:17.855811Z",
          "iopub.status.busy": "2022-07-13T08:00:17.855181Z",
          "iopub.status.idle": "2022-07-13T08:00:35.434274Z",
          "shell.execute_reply": "2022-07-13T08:00:35.433242Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "5554f7fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c674605-8fd6-47a8-9050-8a221d0970cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers shape: torch.Size([512, 1])\n",
            "contexts_negatives shape: torch.Size([512, 42])\n",
            "lengths shape: torch.Size([512, 42])\n",
            "labels shape: torch.Size([512, 42])\n"
          ]
        }
      ],
      "source": [
        "data_iter, vocabulario = load_data_quijote(512, 5, 5)\n",
        "for batch in data_iter:\n",
        "    for name, data in zip(names, batch):\n",
        "        print(name, 'shape:', data.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 3,
        "id": "167f28a7"
      },
      "source": [
        "# Proceso de Entrenamiento\n",
        "\n",
        "Implementamos el modelo skip-gram mediante el uso de capas `Embedding` y multiplicaciones de matrices por lotes.\n",
        "Primero, revisemos cómo funcionan las capas Embedding.\n",
        "\n",
        "\n",
        "### Embedding Layer\n",
        "\n",
        "Una capa embedding asigna el índice de un token a su vector de características.\n",
        "Los pesos de estas capas conforman una matriz cuyo número de filas es igual al tamaño del diccionario (`input_dim`) y el número de columnas es igual a\n",
        "la dimensión del vector para cada token (`output_dim`).\n",
        "Después de entrenar un modelo de embedding de palabras, estos peso se convertiran en el embedding que utilizaremos para representar a la palabra\n"
      ],
      "id": "167f28a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.057372Z",
          "iopub.status.busy": "2022-07-13T08:46:59.056623Z",
          "iopub.status.idle": "2022-07-13T08:46:59.083445Z",
          "shell.execute_reply": "2022-07-13T08:46:59.082546Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4f39446",
        "outputId": "adf2d680-6f5e-4f63-97a0-bbd2b110c442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parametro embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
        "print(f'Parametro embedding_weight ({embed.weight.shape}, '\n",
        "      f'dtype={embed.weight.dtype})')"
      ],
      "id": "b4f39446"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "368c805c"
      },
      "source": [
        "La entrada de una capa Embedding es el índice de un token (palabra). Para cualquier índice de token i, su representación vectorial se puede obtener de la i-ésima fila de la matriz de pesos en la capa de Embedding. Dado que la dimensión del vector (output_dim) se estableció en 4, la capa de Embedding devuelve vectores con forma (2, 3, 4) para un minilote de índices de token con forma (2, 3).\n"
      ],
      "id": "368c805c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.087027Z",
          "iopub.status.busy": "2022-07-13T08:46:59.086572Z",
          "iopub.status.idle": "2022-07-13T08:46:59.095719Z",
          "shell.execute_reply": "2022-07-13T08:46:59.094847Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "fc4850bf",
        "outputId": "529bde9d-610f-49bb-868b-06f9097ff187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3282,  0.0739,  0.0997, -1.3252],\n",
              "         [-0.3501, -1.0170,  0.7502,  0.0447],\n",
              "         [ 1.7760, -0.7577, -0.9302, -0.2768]],\n",
              "\n",
              "        [[-2.6091, -1.2353, -1.2927, -0.6188],\n",
              "         [-0.1381,  0.2361,  0.6832, -0.5569],\n",
              "         [ 0.4046,  0.1417, -2.1642, -0.0186]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "embed(x)"
      ],
      "id": "fc4850bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 21,
        "id": "eec239d2"
      },
      "source": [
        "### Parámetros del Modelo\n",
        "\n",
        "Al comienzo de la fase de entrenamiento, creamos dos matrices: una matriz de embedding y una matriz de contexto. Estas dos matrices tienen un embedding de palabra central y de contexto respectivamente para cada palabra en nuestro vocabulario. \n",
        "\n",
        "![Imgur](https://i.imgur.com/YY2jU6Q.png)\n",
        "\n",
        "En nuestro modelo, estas matrices estarán definidas como capas Embedding y la dimensión de los vectores será 100.\n"
      ],
      "id": "eec239d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.144259Z",
          "iopub.status.busy": "2022-07-13T08:46:59.143981Z",
          "iopub.status.idle": "2022-07-13T08:46:59.161008Z",
          "shell.execute_reply": "2022-07-13T08:46:59.160138Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "d78a01ca"
      },
      "outputs": [],
      "source": [
        "embed_size = 100\n",
        "central_embedding = nn.Embedding(num_embeddings=len(vocabulario),\n",
        "                                 embedding_dim=embed_size)\n",
        "context_embedding = nn.Embedding(num_embeddings=len(vocabulario),\n",
        "                                 embedding_dim=embed_size)\n",
        "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocabulario),\n",
        "                                 embedding_dim=embed_size),\n",
        "                    nn.Embedding(num_embeddings=len(vocabulario),\n",
        "                                 embedding_dim=embed_size))"
      ],
      "id": "d78a01ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "7fb1689c"
      },
      "source": [
        "### Definición de la función forward\n",
        "\n",
        "En la función forward, la entrada del modelo skip-gram incluye a `center` (los índices de la palabra central con forma (tamaño del lote, 1)) y `contexts_and_negatives` (los indices concatenados de las palabras de contexto y negativas con forma (tamaño del lote, max_len) ). \n",
        "\n",
        "Estas dos variables se transforman primero de los índices de token en vectores a través de la capa de Embedding\n",
        "\n",
        "![Imgur](https://i.imgur.com/qjIjDZN.png)\n",
        "\n",
        "Luego se calcula el producto punto entre el vector de la palabra central por cada uno de los vectores de contexto y negativos usando una multiplicación de matrices por lotes. Cada elemento de la salida es el producto escalar de un vector de palabra central y un vector de palabra de contexto o negativa.\n",
        "\n",
        "![Imgur](https://i.imgur.com/vWco0hG.png)"
      ],
      "id": "7fb1689c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.099131Z",
          "iopub.status.busy": "2022-07-13T08:46:59.098668Z",
          "iopub.status.idle": "2022-07-13T08:46:59.103145Z",
          "shell.execute_reply": "2022-07-13T08:46:59.102312Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "a0e4a7f7"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, central_embedding, context_embedding):\n",
        "      super().__init__()\n",
        "      self.central_embedding = central_embedding\n",
        "      self.context_embedding = context_embedding\n",
        "      \n",
        "    def forward(self, center, contexts_and_negatives):\n",
        "      v = central_embedding(center)\n",
        "      u = context_embedding(contexts_and_negatives)\n",
        "      pred = torch.bmm(v, u.permute(0, 2, 1))\n",
        "      return pred"
      ],
      "id": "a0e4a7f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "b78eaf94"
      },
      "source": [
        "Imprimamos la forma de la salida de este SkipGram para algunas entradas de ejemplo"
      ],
      "id": "b78eaf94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.106555Z",
          "iopub.status.busy": "2022-07-13T08:46:59.106142Z",
          "iopub.status.idle": "2022-07-13T08:46:59.113746Z",
          "shell.execute_reply": "2022-07-13T08:46:59.112942Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "1bb0a36a",
        "outputId": "aacdeb48-aa9c-4524-bbf7-b54814887e61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model = SkipGram(central_embedding, context_embedding)\n",
        "model(torch.ones((2, 1), dtype=torch.long),\n",
        "          torch.ones((2, 4), dtype=torch.long)).shape"
      ],
      "id": "1bb0a36a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "09a8efdb"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "\n",
        "Una vez que tenemos la función forward, el proceso de entrenamiento es similar a los modelos vistos con anterioridad: se calcula la función de pérdida y eso se utiliza para modificar los parámetros. La particularidad del entrenamiento de embeddings es que los embeddings que queremos obtener como salida son los mismos parámetros que estamos entrenando. \n",
        "\n",
        "![Imgur](https://i.imgur.com/PpgE131.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "09a8efdb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función de pérdida\n",
        "\n",
        "De acuerdo a la definición de la función de pérdida para muestreo negativo que presentamos anteriormente\n",
        "usaremos la entropía cruzada binaria (también conocida como sigmoidea). "
      ],
      "metadata": {
        "id": "Pig1UeYaFsQN"
      },
      "id": "Pig1UeYaFsQN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.117446Z",
          "iopub.status.busy": "2022-07-13T08:46:59.116865Z",
          "iopub.status.idle": "2022-07-13T08:46:59.121964Z",
          "shell.execute_reply": "2022-07-13T08:46:59.121161Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "6d8868b1"
      },
      "outputs": [],
      "source": [
        "class SigmoidBCELoss(nn.Module):\n",
        "    # Binary cross-entropy loss with masking\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, target, mask=None):\n",
        "        out = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, target, weight=mask, reduction=\"none\")\n",
        "        return out.mean(dim=1)\n",
        "\n",
        "loss = SigmoidBCELoss()"
      ],
      "id": "6d8868b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 17,
        "id": "f9b32ac6"
      },
      "source": [
        "Si probamos con algunos datos inventados.\n"
      ],
      "id": "f9b32ac6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.125574Z",
          "iopub.status.busy": "2022-07-13T08:46:59.124987Z",
          "iopub.status.idle": "2022-07-13T08:46:59.132979Z",
          "shell.execute_reply": "2022-07-13T08:46:59.132182Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "18b12dd6",
        "outputId": "ac2d6103-b826-414d-db53-fbba47637cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9352, 1.8462])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\n",
        "label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
        "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
        "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
      ],
      "id": "18b12dd6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 19,
        "id": "8524cb3b"
      },
      "source": [
        "Vemos que los mismos resultados se pueden generar usando la función sigmoidea (aunque de manera menos eficiente.\n",
        "\n",
        "Podemos considerar a las dos salidas como dos pérdidas normalizadas que se promedian sobre las predicciones no enmascaradas.\n"
      ],
      "id": "8524cb3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.136520Z",
          "iopub.status.busy": "2022-07-13T08:46:59.135910Z",
          "iopub.status.idle": "2022-07-13T08:46:59.140998Z",
          "shell.execute_reply": "2022-07-13T08:46:59.140174Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "7a56ebd8",
        "outputId": "964cf71c-709d-4099-831f-fdf8cfe67031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9352\n",
            "1.8462\n"
          ]
        }
      ],
      "source": [
        "def sigmd(x):\n",
        "    return -math.log(1 / (1 + math.exp(-x)))\n",
        "\n",
        "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
        "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')"
      ],
      "id": "7a56ebd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 24,
        "id": "3c70a041"
      },
      "source": [
        "## Definición del ciclo de entrenamiento\n",
        "\n",
        "El ciclo de entrenamiento se define a continuación. Debido a la existencia de relleno, el cálculo de la función de pérdida es ligeramente diferente en comparación con las funciones de entrenamiento anteriores.\n"
      ],
      "id": "3c70a041"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.164617Z",
          "iopub.status.busy": "2022-07-13T08:46:59.164054Z",
          "iopub.status.idle": "2022-07-13T08:46:59.173023Z",
          "shell.execute_reply": "2022-07-13T08:46:59.172202Z"
        },
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "id": "819e88a8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train(net, data_iter, lr, num_epochs, device):\n",
        "    def init_weights(module):\n",
        "        if type(module) == nn.Embedding:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "    net.apply(init_weights)\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    L = 0\n",
        "    N = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        start, num_batches = time.time(), len(data_iter)\n",
        "        for i, batch in enumerate(data_iter):\n",
        "            optimizer.zero_grad()\n",
        "            center, context_negative, mask, label = [\n",
        "                data.to(device) for data in batch]\n",
        "\n",
        "            pred = net(center, context_negative)\n",
        "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
        "                     / mask.sum(axis=1) * mask.shape[1])\n",
        "            l.sum().backward()\n",
        "            optimizer.step()\n",
        "            L += l.sum()\n",
        "            N += l.numel()\n",
        "        print(f'loss {L / N:.3f}, '\n",
        "          f'{N / (time.time() - start):.1f} tokens/sec on {str(device)}')"
      ],
      "id": "819e88a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 27,
        "id": "fef09682"
      },
      "source": [
        "Ahora podemos entrenar un modelo skip-gram usando muestreo negativo.\n"
      ],
      "id": "fef09682"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:46:59.176604Z",
          "iopub.status.busy": "2022-07-13T08:46:59.175979Z",
          "iopub.status.idle": "2022-07-13T08:47:29.485918Z",
          "shell.execute_reply": "2022-07-13T08:47:29.484775Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "10bfabca",
        "outputId": "de1e1c15-74ff-467a-9a71-f3d70b2fca84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.546, 26076.6 tokens/sec on cpu\n",
            "loss 0.492, 65591.9 tokens/sec on cpu\n",
            "loss 0.471, 100710.6 tokens/sec on cpu\n",
            "loss 0.458, 129073.4 tokens/sec on cpu\n",
            "loss 0.446, 165474.2 tokens/sec on cpu\n",
            "loss 0.435, 201679.3 tokens/sec on cpu\n",
            "loss 0.424, 220450.8 tokens/sec on cpu\n",
            "loss 0.413, 257219.6 tokens/sec on cpu\n",
            "loss 0.402, 294453.1 tokens/sec on cpu\n",
            "loss 0.392, 318767.5 tokens/sec on cpu\n",
            "loss 0.383, 362168.0 tokens/sec on cpu\n",
            "loss 0.374, 398474.0 tokens/sec on cpu\n",
            "loss 0.365, 424050.1 tokens/sec on cpu\n",
            "loss 0.358, 451857.6 tokens/sec on cpu\n",
            "loss 0.350, 436042.1 tokens/sec on cpu\n",
            "loss 0.344, 475244.5 tokens/sec on cpu\n",
            "loss 0.337, 557238.9 tokens/sec on cpu\n",
            "loss 0.331, 586896.2 tokens/sec on cpu\n",
            "loss 0.326, 613785.0 tokens/sec on cpu\n",
            "loss 0.321, 672680.1 tokens/sec on cpu\n"
          ]
        }
      ],
      "source": [
        "def try_gpu(i=0): \n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "lr, num_epochs = 0.002, 20\n",
        "train(model, data_iter, lr, num_epochs, try_gpu())"
      ],
      "id": "10bfabca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 29,
        "id": "1b0f7619"
      },
      "source": [
        "## Aplicación de Embeddings de Palabras\n",
        "\n",
        "Después de entrenar el modelo word2vec, podemos usar la similitud de coseno de los vectores de palabras del modelo entrenado para encontrar palabras del diccionario que sean más similares semánticamente a una palabra de entrada.\n",
        "\n",
        "Recordemos que estamos entrenado sobre un dataset muy pequeño y los resultados no deberían ser muy buenos.\n"
      ],
      "id": "1b0f7619"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:47:29.490799Z",
          "iopub.status.busy": "2022-07-13T08:47:29.490183Z",
          "iopub.status.idle": "2022-07-13T08:47:29.498043Z",
          "shell.execute_reply": "2022-07-13T08:47:29.497212Z"
        },
        "origin_pos": 31,
        "tab": [
          "pytorch"
        ],
        "id": "954fb8fd",
        "outputId": "69d6119f-475d-4576-8502-fe1721c409c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.353: salida\n",
            "cosine sim=0.331: vino,\n",
            "cosine sim=0.301: do\n"
          ]
        }
      ],
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    W = embed.weight.data\n",
        "    assert vocabulario.__contains__(query_token), \"El token no está en el vocabulario\"\n",
        "    x = W[vocabulario[query_token]]\n",
        "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
        "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
        "                                      torch.sum(x * x) + 1e-9)\n",
        "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
        "    tokens = vocabulario.get_itos()\n",
        "    for i in topk[1:]:  # Remove the input words\n",
        "        print(f'cosine sim={float(cos[i]):.3f}: {tokens[i]}')\n",
        "\n",
        "get_similar_tokens('día', 3, net[0])"
      ],
      "id": "954fb8fd"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "2_Entrenamiento_Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}