{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Embeddings_Pre-entrenados.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/3_Embeddings/3_Embeddings_Pre-entrenados.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "H6SQZXFUbOQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar Embeddings Pre-entrenados\n",
        "\n",
        "Hasta ahora aprendimos como se hace para entrenar desde 0 nuestros embeddings con un dataset propio. Sin embargo, muchas veces esto es innecesario ya que seguramente mucha gente ya se ha enfrentado al mismo problema y ha entrenado embeddings que pueden ser reutilizados.\n",
        "\n",
        "En esta clase aprenderemos cómo importar Embeddings pre-entrenados en nuestros modelos."
      ],
      "metadata": {
        "id": "4zAN4LJXO2Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargando los datos\n",
        "\n",
        "En esta sección descargaremos un archivo txt que contiene 1.000.653 embeddings de palabras de dimensión 300 entrenadas con el [Spanish Billion Words Corpus](https://crscardellino.ar/SBWCE/). Estas incrustaciones fueron entrenadas usando word2vec.\n",
        "Los hiperparámtero usados para el entrenamiento son:\n",
        "\n",
        "Las incrustaciones de palabras se entrenaron utilizando los siguientes parámetros:\n",
        "\n",
        "* El algoritmo seleccionado fue el modelo skip-gram con muestreo negativo.\n",
        "* La frecuencia mínima de palabras fue de 5.\n",
        "* La cantidad de “palabras ruidosas” para el muestreo negativo fue de 20.\n",
        "* Las 273 palabras más comunes se submuestrearon.\n",
        "* La dimensión de la incrustación de la palabra final fue 300.\n",
        "\n",
        "El corpus original tenía la siguiente cantidad de datos:\n",
        "\n",
        "* Un total de 1420665810 palabras sin procesar.\n",
        "* Un total de 46925295 oraciones.\n",
        "* Un total de 3817833 tokens únicos.\n",
        "\n",
        "Luego de aplicar el modelo skip-gram, filtrado de palabras con menos de 5 ocurrencias así como el downsampling de las 273 palabras más comunes, se obtuvieron los siguientes valores:\n",
        "\n",
        "* Un total de 771508817 palabras sin procesar.\n",
        "* Un total de 1000653 tokens únicos."
      ],
      "metadata": {
        "id": "byJIlpkPPgWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda descarga y descomprime el archivo txt con los embeddings. El algoritmo de compresión bzip2 es algo lento así que tenga paciencia, puede tardar unos minutos en descomprimirse el archivo."
      ],
      "metadata": {
        "id": "zvJwx7mSR_Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2\n",
        "!bzip2 -d SBW-vectors-300-min5.txt.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "behcSDCnQvEZ",
        "outputId": "9b0ead30-09a0-45c4-84d2-42324e83c90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-24 00:10:08--  https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2\n",
            "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 818175453 (780M) [application/x-bzip2]\n",
            "Saving to: ‘SBW-vectors-300-min5.txt.bz2’\n",
            "\n",
            "SBW-vectors-300-min 100%[===================>] 780.27M  11.8MB/s    in 68s     \n",
            "\n",
            "2022-08-24 00:11:18 (11.4 MB/s) - ‘SBW-vectors-300-min5.txt.bz2’ saved [818175453/818175453]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El archivo txt indica en su primera linea la cantidad de embeddings y la dimensión de cada uno de ellos. Luego, cada linea contendrá el token vectorizado y luego el vector propiamente dicho."
      ],
      "metadata": {
        "id": "7vM5MS-1VGnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"SBW-vectors-300-min5.txt\", 'r') as f:\n",
        "  n_lin = 0\n",
        "  for line in f:\n",
        "    print(line)\n",
        "    n_lin += 1\n",
        "    if n_lin>3: break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHTNAAr0B85j",
        "outputId": "122c600c-77d0-4039-e532-c79df22bc286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000653 300\n",
            "\n",
            "de -0.029648 0.011336 0.019949 -0.088832 -0.025225 0.056844 0.025473 0.014068 0.163694 -0.067154 0.014738 0.027134 0.066443 -0.044846 -0.044987 -0.040898 0.030311 0.034196 -0.049240 0.008537 -0.068091 -0.087938 0.035300 0.149385 -0.012350 0.012613 0.029350 0.069596 0.039111 0.057652 0.069954 -0.066217 -0.041784 0.028623 0.026772 -0.066392 0.002953 -0.012188 -0.030363 0.040222 0.034858 0.027469 -0.029034 -0.048748 -0.038582 -0.051553 -0.033501 -0.019008 0.003043 0.110712 -0.025096 0.111082 0.035244 0.114207 0.010195 0.051511 -0.040649 -0.113944 0.044873 0.052011 0.067360 0.049054 -0.127085 -0.031846 0.032848 0.040825 -0.084873 0.059801 -0.067424 0.016531 -0.084565 0.057024 0.083288 -0.010136 -0.048508 0.051757 0.046664 0.018102 -0.052320 -0.000765 0.053662 -0.009967 0.082858 0.009068 0.054575 -0.003466 -0.023376 0.023069 0.088513 0.018504 -0.039503 -0.032980 -0.002139 0.000010 -0.107627 0.007699 0.046351 -0.003062 0.030500 0.113650 0.032536 -0.097301 -0.013734 0.098345 0.080898 -0.064173 -0.008874 -0.144751 0.037585 0.013290 0.059674 0.006163 0.007318 0.000053 -0.060292 -0.059135 0.049497 -0.011438 -0.095108 -0.043465 0.048567 -0.043990 -0.030774 0.005092 -0.032265 0.009392 0.018503 0.084857 0.109709 -0.020662 0.017696 0.026699 -0.076638 -0.014106 -0.035155 0.046999 -0.003727 -0.047805 0.044270 0.011314 0.036524 -0.069505 -0.014850 -0.003538 -0.047049 0.029349 0.034521 -0.032199 0.116497 -0.077610 0.068234 -0.016126 -0.066454 -0.079914 -0.020723 -0.064905 0.069560 0.021368 -0.049497 -0.046599 0.067663 -0.069035 0.118015 0.027463 -0.006176 -0.034514 -0.026515 0.040308 0.091113 -0.080539 0.132408 -0.070958 0.019730 0.033399 0.003489 -0.159659 -0.004230 0.004888 -0.056615 0.061021 0.025117 0.099613 0.063876 -0.006202 -0.049316 -0.020530 0.008522 -0.094168 -0.009451 -0.034682 -0.026801 0.065383 0.042528 -0.013688 0.035068 0.119803 -0.020278 0.111882 -0.068336 0.050245 -0.123240 0.002248 0.010229 -0.062540 -0.017837 -0.115210 0.051036 0.026920 0.083457 0.062902 0.003083 -0.037112 -0.015425 0.001716 0.018002 0.101100 0.019446 0.074839 0.059951 0.072243 -0.025459 -0.039853 0.077950 0.060199 -0.070603 0.060652 0.023855 -0.035858 -0.058043 -0.075130 0.000670 0.082828 -0.013141 0.144692 -0.105775 0.087624 0.047030 -0.015700 -0.042360 0.026337 0.144966 0.021922 0.012097 -0.011443 -0.110208 0.009333 -0.081423 -0.027137 0.000827 0.085389 0.034464 0.032338 -0.015040 0.009954 -0.011759 -0.042395 -0.000467 0.055047 -0.049710 -0.104978 0.031742 0.037020 -0.007016 -0.049712 0.041684 0.018348 -0.001201 0.019282 0.005763 0.074952 -0.018588 0.016055 0.119526 -0.014613 0.020598 0.027312 0.024252 -0.024044 -0.026332 -0.063152 -0.095363 -0.034335 -0.062552 -0.035730 0.091165 0.009686 0.020341 -0.012004 0.011826 -0.084301 0.011144 0.074969 -0.004862 -0.014076 0.025692 -0.077322 -0.022998 -0.128057 -0.004917 0.062628\n",
            "\n",
            "DIGITO -0.013002 -0.000781 0.032629 -0.088482 0.021298 0.039986 0.068380 0.006264 0.091135 0.000434 0.009331 0.028423 0.020692 -0.013583 -0.032420 0.024689 0.033696 -0.023145 -0.035633 -0.010274 -0.081628 -0.053617 0.051587 0.091048 -0.016919 0.033980 0.073040 0.100553 0.027454 0.053528 -0.013338 -0.055065 -0.045165 0.053637 0.017824 -0.010603 0.013005 -0.002873 -0.069372 0.008059 -0.001416 0.012431 -0.080941 -0.065539 0.007878 -0.021076 -0.035602 -0.049555 0.048422 0.104744 -0.054366 0.018188 0.031350 0.098584 0.094406 0.019084 -0.033111 -0.092422 0.058111 0.042169 0.009418 0.052326 -0.076443 -0.065708 0.039609 0.070779 -0.104470 0.016466 -0.100397 -0.012615 -0.062208 0.044405 0.040897 0.047476 -0.010739 0.005267 0.081850 0.034962 -0.094599 0.015806 0.040549 -0.003756 0.036048 0.022974 0.035902 0.023096 0.039259 -0.016199 0.043539 0.014725 -0.006119 -0.018587 -0.014097 -0.002395 -0.134557 0.041796 0.001652 0.058954 0.078891 0.114630 0.105141 -0.128402 -0.066549 0.079760 0.024396 -0.113124 -0.001198 -0.135299 0.037189 -0.035586 0.094150 0.018785 -0.005085 -0.018881 -0.115887 -0.082005 0.024251 -0.033302 -0.078087 -0.022660 0.007569 -0.096935 -0.023436 -0.023365 -0.026818 -0.023903 0.046716 0.047488 0.067228 -0.033471 0.038515 0.023762 -0.125687 0.042650 -0.000855 0.001957 -0.052234 -0.066039 0.082063 0.001344 -0.009984 -0.117670 -0.004480 -0.006035 -0.006549 0.032035 -0.021916 -0.028900 0.087054 -0.103109 0.033930 -0.036428 0.002393 -0.009741 -0.069586 -0.020493 0.001128 -0.067282 -0.043965 -0.075068 0.099662 -0.017608 0.066231 -0.015208 0.031142 -0.027223 -0.033040 0.053951 0.046496 -0.131409 0.131169 -0.007959 -0.011152 0.000514 -0.028036 -0.168291 -0.022996 -0.032068 -0.039740 0.016381 0.005333 0.099232 0.051381 -0.000236 -0.073249 0.012495 -0.015802 -0.091128 -0.003723 -0.022181 -0.046669 0.059063 0.064704 0.028548 0.011399 0.117871 -0.028101 0.068917 0.008404 0.030469 -0.104959 0.002380 -0.040827 0.041906 -0.024125 -0.046123 0.006722 0.074472 0.107256 0.066871 -0.017771 -0.007295 -0.007886 -0.031781 0.050508 0.043651 -0.043853 0.119111 0.048094 0.027306 -0.012738 -0.055844 0.000362 0.073290 -0.094030 0.088980 0.033241 -0.082521 -0.114527 -0.077395 -0.057670 0.046407 0.032855 0.090644 -0.084556 0.130570 0.046755 -0.000518 -0.062754 0.003713 0.142896 -0.000069 0.012594 -0.018961 -0.071201 -0.050190 -0.107127 -0.011871 0.030770 0.038137 0.023721 0.028529 0.065621 0.018548 0.044810 -0.023708 0.028006 0.028420 0.000359 -0.051305 0.035992 0.077839 -0.046144 -0.010108 0.056583 0.040588 0.106301 0.046546 -0.028884 0.031776 -0.023081 0.011159 0.058000 -0.002292 -0.021265 0.030547 -0.046024 0.021797 0.090825 -0.097326 -0.077190 0.060535 -0.078626 -0.108777 0.048595 -0.039361 0.022155 -0.015696 0.007409 -0.130169 0.048188 0.095554 -0.007935 -0.010960 0.009057 -0.055717 -0.016302 -0.132496 0.029352 0.063434\n",
            "\n",
            "la -0.022313 0.022251 0.036704 -0.096540 -0.052861 0.024058 0.011043 0.002622 0.156215 -0.042965 0.039763 -0.003066 0.038801 -0.053368 -0.041667 -0.030635 -0.007328 0.003714 -0.114164 -0.025042 -0.051972 -0.055420 0.042268 0.105376 -0.060363 -0.019926 -0.001696 0.102281 0.040454 0.088647 0.088293 -0.019142 0.003859 -0.027580 0.018365 -0.088503 -0.011589 -0.052447 -0.008289 -0.020715 0.009673 0.012709 -0.082492 -0.040010 -0.057307 -0.087804 -0.020654 0.018062 0.034962 0.131913 -0.033211 0.130602 0.014101 0.057181 0.005193 0.058847 -0.062297 -0.131932 0.017364 0.088293 0.053714 0.019530 -0.121452 0.018164 0.060706 0.088240 -0.059637 0.037769 -0.052032 0.079613 -0.133862 -0.006583 0.068153 0.041149 -0.059254 0.057758 0.010501 0.058813 -0.026589 -0.078938 0.032387 -0.010713 0.054048 0.016489 0.052541 0.019058 -0.035579 0.035734 0.079074 0.027151 -0.034716 -0.041740 0.027742 0.038502 -0.131790 0.000054 0.023742 -0.015529 -0.016263 0.074132 -0.034666 -0.074852 0.005035 0.113395 0.032837 0.004166 0.004269 -0.135889 0.062360 0.051681 0.044063 0.054768 0.067986 0.032836 -0.095447 -0.072281 0.028622 0.029793 -0.057763 -0.034192 0.001806 -0.043905 -0.044698 0.007713 -0.047600 0.014539 -0.004542 0.047801 0.049999 -0.044603 0.038886 0.022660 -0.053500 -0.062190 -0.085413 0.072058 0.029248 -0.031418 -0.055516 0.025744 0.060207 -0.014266 -0.022034 0.018754 -0.015960 -0.001577 0.020190 -0.069271 0.138597 -0.101960 0.043598 0.005266 -0.075013 -0.063113 0.049575 -0.053393 0.043651 0.012303 -0.034139 0.024200 0.055642 -0.030245 0.100529 -0.017504 -0.060918 -0.100398 -0.043245 0.047509 0.040789 -0.048056 0.107280 -0.079895 0.049304 0.028526 0.040193 -0.097320 -0.000557 0.004501 -0.000329 0.006951 0.037162 0.053592 0.071350 0.025196 -0.031765 -0.000354 -0.013615 -0.080432 0.026182 -0.030302 -0.028832 -0.002504 0.071907 -0.027518 0.008791 0.142758 0.008447 0.099430 -0.051668 0.107455 -0.098141 -0.060972 -0.013618 -0.041751 0.039856 -0.068698 0.027978 -0.026681 0.034578 0.036537 -0.019795 0.007946 0.014677 -0.028922 0.026407 0.100824 0.024597 0.057108 0.026002 0.004507 0.008445 -0.070869 0.119097 0.036694 0.017460 0.024830 0.049018 -0.042634 -0.075510 -0.044751 0.038456 0.074046 -0.046579 0.143879 -0.062126 0.027630 0.072420 -0.044438 -0.066515 0.036794 0.196606 0.001752 0.048581 0.016231 -0.133789 0.010679 -0.072775 -0.007094 -0.010939 0.084204 0.018636 0.107546 -0.044642 0.017310 -0.013498 0.008038 0.006861 0.067795 -0.064582 -0.110481 0.034113 0.054558 -0.020856 -0.100647 0.049993 0.001914 -0.059515 0.010465 0.000765 0.135641 -0.019326 0.037263 0.065312 0.008730 0.007405 0.044418 0.038109 -0.022185 -0.045651 -0.064789 -0.119470 0.002855 -0.037815 -0.011621 0.034758 0.026737 0.077853 -0.000177 -0.009866 -0.037531 0.018481 0.067658 0.005637 -0.034123 0.003338 -0.062329 -0.016266 -0.087151 -0.020682 0.033452\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando el contenido en memoria"
      ],
      "metadata": {
        "id": "PV5pi1PqVlV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se crea una clase que nos permitirá almacenar los embeddings en memoria y acceder a ellos de manera más estructurada.\n",
        "\n",
        "Esta clase contendrá varios atributos útiles:\n",
        "* idx_to_token: es una lista que contendrá los tokens\n",
        "* idx_to_vec: es una lista que contendrá los embeddings\n",
        "* dim: es la dimensión de los embeddings\n",
        "* token_to_idx: devuelve el id correspondiente al token pasado como parámetro\n"
      ],
      "metadata": {
        "id": "-LP9mqXuVplC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3BGMxp82Jxe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class TokenEmbedding:\n",
        "  \"\"\"Token Embedding.\"\"\"\n",
        "  def __init__(self, file_name, n):\n",
        "    self.idx_to_token, self.idx_to_vec, self.dim = self._load_embedding(\n",
        "        file_name, n)\n",
        "    self.unknown_idx = 0\n",
        "    self.token_to_idx = {token: idx for idx, token in\n",
        "                          enumerate(self.idx_to_token)}\n",
        "  \n",
        "\n",
        "  def _load_embedding(self, file_name, n):\n",
        "    idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "    with open( file_name, 'r') as f:\n",
        "      first_read = True\n",
        "      i=0\n",
        "      for line in f:\n",
        "        if n<i: break\n",
        "        else: i+=1\n",
        "        if first_read:\n",
        "          first_read = False\n",
        "          continue\n",
        "        elems = line.rstrip().split(' ')\n",
        "        token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
        "        # Skip header information, such as the top row in fastText\n",
        "        if len(elems) > 1:\n",
        "            idx_to_token.append(token)\n",
        "            idx_to_vec.append(elems)\n",
        "    idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
        "    return idx_to_token, torch.tensor(idx_to_vec), len(idx_to_vec[0])\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
        "                for token in tokens]\n",
        "    vecs = self.idx_to_vec[torch.tensor(indices)]\n",
        "    return vecs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación cargaremos los embeddings en el objeto `spanish_w2v`. Cabe aclarar que sólo cargaremos 500k tokens debido a que no entran todos en memoria.\n",
        "\n"
      ],
      "metadata": {
        "id": "s17JzxqBXzh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_w2v = TokenEmbedding(\"SBW-vectors-300-min5.txt\",500000)"
      ],
      "metadata": {
        "id": "8UbnXec6_JiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera es fácil acceder al embedding de cualquier palabra que queramos."
      ],
      "metadata": {
        "id": "fqW2w8lUYvJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_mesa = spanish_w2v.token_to_idx[\"mesa\"]\n",
        "spanish_w2v.idx_to_vec[id_mesa]"
      ],
      "metadata": {
        "id": "H8DejW1VAwzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93219d8-7c21-468e-b9a0-fb62ca2fb3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-7.2965e-02,  1.8812e-02,  1.0573e-01,  3.3087e-02, -8.5658e-02,\n",
              "         9.9318e-02, -6.4557e-02,  4.0981e-02,  2.6975e-02, -6.2916e-02,\n",
              "         5.1500e-04, -2.4984e-02,  1.7487e-02, -3.5730e-03, -2.8035e-02,\n",
              "         1.3317e-02,  5.2800e-02, -3.8670e-03,  2.7829e-02, -6.9262e-02,\n",
              "        -3.3747e-02, -4.3120e-02, -4.5179e-02,  7.9108e-02,  9.0945e-02,\n",
              "        -2.9899e-02, -1.9439e-02,  1.1969e-01,  5.3333e-02,  4.1652e-02,\n",
              "        -7.9298e-02, -1.1909e-01,  4.7590e-03, -8.5445e-02, -5.1491e-02,\n",
              "         2.8829e-02,  4.3646e-02,  2.3469e-02, -3.9472e-02,  6.6565e-02,\n",
              "         3.4349e-02, -1.1352e-01,  2.9633e-02, -6.8393e-02, -7.8980e-02,\n",
              "        -5.1030e-02,  1.5873e-02,  6.8210e-03, -1.3847e-02, -1.0377e-01,\n",
              "         1.5152e-02,  6.3327e-02,  4.6139e-02,  4.1723e-02,  7.0962e-02,\n",
              "        -5.1490e-02, -3.6193e-02, -6.5074e-02,  5.2661e-02,  4.8595e-02,\n",
              "        -3.9467e-02,  1.7295e-02,  2.7180e-02, -5.6163e-02, -1.5222e-01,\n",
              "        -1.8895e-02, -3.7351e-02, -9.4126e-02,  1.0864e-02, -5.4802e-02,\n",
              "         6.0664e-02, -1.2041e-01, -1.5933e-02, -1.4114e-02, -2.0040e-02,\n",
              "         7.8145e-02, -1.9210e-03,  4.7873e-02, -5.2690e-03,  6.3960e-03,\n",
              "         1.6572e-02, -6.5264e-02,  8.8573e-02, -2.7076e-02,  2.5835e-02,\n",
              "         4.5680e-02,  3.0289e-02, -4.9748e-02,  1.3978e-01, -2.8198e-02,\n",
              "         5.8938e-02, -8.2410e-03,  7.2396e-02, -6.4488e-02, -2.2840e-02,\n",
              "        -8.0084e-02, -2.0614e-02, -4.3852e-02,  2.5669e-02, -3.5534e-02,\n",
              "        -1.3377e-01, -3.0028e-02, -1.7812e-02,  4.0712e-02, -6.7375e-02,\n",
              "        -6.1264e-02, -3.9620e-02,  2.5470e-02,  6.8341e-02,  1.1997e-01,\n",
              "        -5.6913e-02, -1.4330e-03, -2.2420e-02,  6.7990e-03,  4.7216e-02,\n",
              "        -7.1813e-02, -2.0927e-02, -3.7481e-02,  5.5001e-02, -1.0239e-01,\n",
              "         5.5282e-02,  1.2663e-01, -1.6031e-02, -2.3537e-02,  1.1030e-03,\n",
              "         8.8190e-02, -9.6810e-03,  5.2523e-02,  8.2700e-04,  7.9630e-03,\n",
              "         4.1320e-03,  8.3381e-02, -1.2161e-01, -4.8663e-02, -1.1925e-01,\n",
              "         2.9802e-02, -3.3852e-02, -3.0930e-02,  2.3303e-02,  4.6677e-02,\n",
              "        -2.0751e-02,  7.1737e-02, -8.8082e-02,  1.9230e-03, -8.5971e-02,\n",
              "        -6.3725e-02,  8.3655e-02, -2.6281e-02,  8.4011e-02, -4.3660e-03,\n",
              "        -5.6815e-02, -8.6599e-02,  9.3640e-03,  1.3836e-02,  7.8423e-02,\n",
              "        -9.5800e-04, -6.9860e-03,  9.5688e-02, -7.7569e-02,  4.9799e-02,\n",
              "         2.5180e-03, -1.5180e-02, -1.9751e-02, -7.1570e-02,  1.0840e-02,\n",
              "        -3.6250e-02, -1.4431e-02,  7.8668e-02, -3.5343e-02,  7.8440e-03,\n",
              "        -3.4349e-02,  4.4518e-02, -7.7251e-02, -2.1117e-02,  9.7420e-02,\n",
              "        -3.1298e-02, -4.2933e-02, -5.4311e-02, -5.4446e-02, -1.4852e-02,\n",
              "         6.1176e-02,  1.5390e-03,  3.4195e-02, -3.8996e-02, -5.7026e-02,\n",
              "        -4.3349e-02,  3.4385e-02,  2.1430e-03,  1.9686e-02, -3.0484e-02,\n",
              "         1.8180e-03, -1.9238e-02,  4.9492e-02, -7.3456e-02,  3.0170e-03,\n",
              "         2.5878e-02, -1.5066e-02,  1.2485e-02,  1.4489e-02, -3.6717e-02,\n",
              "        -7.1559e-02,  7.6850e-03,  3.1903e-02,  2.3532e-02,  2.4778e-02,\n",
              "        -1.7716e-02, -6.2673e-02,  3.9564e-02, -4.9403e-02, -1.0600e-04,\n",
              "        -1.3193e-01, -9.0682e-02,  3.6070e-03, -5.3675e-02, -1.9700e-04,\n",
              "        -6.8400e-03,  7.0420e-03,  9.1030e-03, -2.3142e-02, -1.1744e-01,\n",
              "         5.2303e-02, -6.9070e-02, -5.3226e-02, -5.6094e-02, -2.5892e-02,\n",
              "         5.9660e-03, -6.6575e-02,  7.2716e-02, -6.8447e-02,  3.3805e-02,\n",
              "         4.6888e-02, -4.6435e-02, -1.6769e-02,  4.8038e-02,  1.3744e-02,\n",
              "         1.3751e-01, -1.0784e-01,  3.5564e-02, -2.1066e-02,  2.0527e-02,\n",
              "         1.3358e-01, -5.1240e-02, -5.3971e-02,  2.1455e-02, -1.1308e-01,\n",
              "        -5.9979e-02, -4.6500e-04, -4.3485e-02, -6.7705e-02,  5.6650e-03,\n",
              "         9.2280e-02,  7.1585e-02,  4.6540e-02,  7.3460e-02, -1.4592e-01,\n",
              "        -2.9299e-02,  1.7931e-02, -1.6600e-02, -1.8888e-02,  1.4140e-03,\n",
              "         1.6493e-02,  1.6932e-01, -4.6650e-02,  3.8184e-02, -9.8826e-02,\n",
              "        -1.0126e-01,  3.6143e-02,  7.0410e-03, -2.1670e-02,  8.8469e-02,\n",
              "        -5.4250e-03,  6.5298e-02,  5.0909e-02, -8.9400e-03, -1.8412e-02,\n",
              "         3.6405e-02,  9.9635e-02,  3.2827e-02, -3.0760e-03, -6.2409e-02,\n",
              "        -1.1388e-01,  6.3650e-02, -7.0078e-02,  6.2359e-02,  4.1926e-02,\n",
              "        -9.6686e-02,  5.6316e-02, -3.5630e-03,  1.6566e-02,  1.2681e-02,\n",
              "        -5.6794e-02,  1.8002e-02, -6.6206e-02,  3.2592e-02, -4.6641e-02,\n",
              "         3.7960e-03, -1.8899e-02, -2.4955e-02, -4.4995e-02,  1.0469e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluando los embeddings pre-entrenados"
      ],
      "metadata": {
        "id": "noEPVfyBY2QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando los vectores word2vec cargados, demostraremos su semántica aplicándolos en las siguientes tareas de analogía y similitud de palabras."
      ],
      "metadata": {
        "id": "LK-B1gXCZCXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similitud de Palabras"
      ],
      "metadata": {
        "id": "r5iz_9jfZO16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para encontrar palabras semánticamente similares para una palabra de entrada, basados en las similitudes de coseno entre vectores de palabras, implementamos la siguiente función knn (vecinos más cercanos)."
      ],
      "metadata": {
        "id": "0BSOcV0yZRfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(W, x, k):\n",
        "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
        "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
        "        torch.sqrt((x * x).sum()))\n",
        "    _, topk = torch.topk(cos, k=k)\n",
        "    return topk, [cos[int(i)] for i in topk]\n"
      ],
      "metadata": {
        "id": "b3_tg-vn4YJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, buscamos palabras similares utilizando los embeddings preentrenados de la instancia de `TokenEmbedding`"
      ],
      "metadata": {
        "id": "O-OXcrHtZqB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
        "    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word\n",
        "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')\n",
        "\n",
        "get_similar_tokens('muchacho', 3, spanish_w2v)"
      ],
      "metadata": {
        "id": "safMc1M672qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57176c47-bcf3-4d41-c638-23eb0d9f0a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.784: joven\n",
            "cosine sim=0.765: niño\n",
            "cosine sim=0.759: chico\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analogías\n"
      ],
      "metadata": {
        "id": "G4PaSXKTaBkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además de encontrar palabras similares, también podemos aplicar vectores de palabras a tareas de analogía de palabras. Por ejemplo, “hombre”:“mujer”::“hijo”:“hija” es la forma de una  analogía: “hombre” es a “mujer” como “hijo” es a “hija”. Específicamente, la tarea de completar la analogía de palabras se puede definir como: para una analogía de palabras \n",
        "$a:b :: c:d$, dadas las tres primeras palabras $a$, $b$ y $c$, encuentre $d$. Denote el vector de la palabra $w$ como $vec(w)$. Para completar la analogía, buscaremos la palabra cuyo vector se parezca más al resultado de $vec(c) + vec(b) - vec(a)$ ."
      ],
      "metadata": {
        "id": "dzMuub26aL6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analogy(token_a, token_b, token_c, embed):\n",
        "    vecs = embed[[token_a, token_b, token_c]]\n",
        "    x = vecs[1] - vecs[0] + vecs[2]\n",
        "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
        "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
      ],
      "metadata": {
        "id": "7W5EHxTS8aru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiquemos la analogía \"hombre-mujer\" usando los vectores de palabras cargados."
      ],
      "metadata": {
        "id": "VZAymNnMa10j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_analogy('hombre', 'mujer', 'hijo', spanish_w2v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rX6lKSzHC1r5",
        "outputId": "3d42c905-405c-4833-a6b8-db64ee3de4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hija'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos una analogía \"país-gentilicio\"."
      ],
      "metadata": {
        "id": "2P-Azmgma2t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_analogy('Argentina', 'argentino', 'España', spanish_w2v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S-rUQRCcDDhn",
        "outputId": "f8e72420-c484-4e3e-e7d5-d50f4572fcc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'español'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebe sus propias analogías y vea cuáles fueron capturadas por word2vec y cuáles no."
      ],
      "metadata": {
        "id": "j_GnykbibABX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generar una capa Embedding con vectores preentrenados."
      ],
      "metadata": {
        "id": "Rqo2tV85bKhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que debemos hacer en este punto es crear una capa Embedding, es decir, un diccionario que mapee índices enteros (que representan palabras) a vectores densos. Toma como entrada enteros, busca estos enteros en un diccionario interno y devuelve los vectores asociados.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jpxhmt9QbVwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, no vamos a usar todos los embeddings que descargamos, sino que solo usaremos aquellos que contenga el vocabulario que hemos construido para nuestra tarea. En nuestro caso recuperaremos el vocabulario del quijote que usamos la clase pasada."
      ],
      "metadata": {
        "id": "ZDf4y5WVb2yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from collections import Counter, OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "!wget https://www.gutenberg.org/files/2000/2000-0.txt\n",
        "!mv \"2000-0.txt\" \"quijote.txt\"\n",
        "\n",
        "def read_txt(filename, n_ignored):\n",
        "    \"\"\"\n",
        "    Carga un archivo txt en una lista de oraciones.\n",
        "    A su vez, cada oración es una lista de tokens separados por espacio.\n",
        "    Ignora las primeras n_ignored lineas.\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        raw_text = f.read()\n",
        "    return [line.split() for i,line in enumerate(raw_text.split('\\n')) if i>=n_ignored]\n",
        "\n",
        "def make_vocab(oraciones,min_freq=1):\n",
        "  #Comprueba que oraciones es una lista de listas\n",
        "  if oraciones and isinstance(oraciones[0], list):\n",
        "    #Transforma una lista anidada en una lista simple \n",
        "    tokens = [token for line in oraciones for token in line]\n",
        "  counter_obj = collections.Counter()\n",
        "  counter_obj.update(tokens)\n",
        "  sorted_by_freq_tuples = sorted(counter_obj.items(), key=lambda x: x[1], reverse=True)\n",
        "  ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "  vocabulario = vocab(ordered_dict, min_freq=min_freq)\n",
        "  return vocabulario, ordered_dict\n",
        "\n",
        "oraciones_quijote = read_txt(\"quijote.txt\",28)\n",
        "vocab_quijote, ordered_dict = make_vocab(oraciones_quijote,10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTV6RVuaIATV",
        "outputId": "118f287d-3154-4ec8-e9fd-8624dd10d060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-24 00:16:25--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2226045 (2.1M) [text/plain]\n",
            "Saving to: ‘2000-0.txt’\n",
            "\n",
            "2000-0.txt          100%[===================>]   2.12M  2.99MB/s    in 0.7s    \n",
            "\n",
            "2022-08-24 00:16:26 (2.99 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"Este vocabulario tiene un tamaño de {len(vocab_quijote.get_itos())}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HRauSXpLdQ7N",
        "outputId": "17eaecb6-75b9-4d1f-918f-989f14f1199f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Este vocabulario tiene un tamaño de 3241'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debemos construir una matriz de pesos que se cargará en la capa de incrustación de PyTorch. Su forma será igual a: `(longitud del vocabulario del conjunto de datos, dimensión de los vectores de palabras).` \n",
        "\n",
        "Para cada palabra en el vocabulario del conjunto de datos, verificamos si está en el vocabulario de word2vec. Si lo está, cargamos su vector de palabra pre-entrenado. De lo contrario, inicializamos un vector aleatorio."
      ],
      "metadata": {
        "id": "ZKYY8uJldxYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_len = len(vocab_quijote.get_itos())\n",
        "weights_matrix = np.zeros((len(vocab_quijote.get_itos()), spanish_w2v.dim))\n",
        "words_found = 0\n",
        "words_not_found = []\n",
        "\n",
        "for i, word in enumerate(vocab_quijote.get_itos()):\n",
        "    try: \n",
        "        weights_matrix[i] = spanish_w2v[[word]][0]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(spanish_w2v.dim, ))\n",
        "        words_not_found.append(word)\n",
        "\n",
        "weights_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1afGnX2sGgzp",
        "outputId": "603f046b-4c25-4f38-8e0f-81015df85eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3241, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al parecer, todas las palabras de nuestro vocabulario tenían su vector word2vec preentrenado."
      ],
      "metadata": {
        "id": "96B_MsqReCPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_found, matrix_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek8XSVpNMsz4",
        "outputId": "8e3fca1e-0fe6-453e-ec7d-35fc441e79bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3241, 3241)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos ver que coinciden la fila de la matriz con el vector que cargamos del archivo (excepto por diferencias de punto flotante)."
      ],
      "metadata": {
        "id": "BNaJ3WKVe4w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"mesa\"\n",
        "i = vocab_quijote[word]\n",
        "[(weights_matrix[i][j],spanish_w2v[[word]][0][j]) for j in range(300)]\n"
      ],
      "metadata": {
        "id": "I6DFVDvANkQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e5f32f-6486-4eb4-e339-b1f91e4edeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-0.07296500355005264, tensor(-0.0730)),\n",
              " (0.01881200075149536, tensor(0.0188)),\n",
              " (0.10572600364685059, tensor(0.1057)),\n",
              " (0.033087000250816345, tensor(0.0331)),\n",
              " (-0.085657998919487, tensor(-0.0857)),\n",
              " (0.0993179976940155, tensor(0.0993)),\n",
              " (-0.06455700099468231, tensor(-0.0646)),\n",
              " (0.040980998426675797, tensor(0.0410)),\n",
              " (0.026975000277161598, tensor(0.0270)),\n",
              " (-0.06291600316762924, tensor(-0.0629)),\n",
              " (0.0005150000215508044, tensor(0.0005)),\n",
              " (-0.024984000250697136, tensor(-0.0250)),\n",
              " (0.017487000674009323, tensor(0.0175)),\n",
              " (-0.0035729999653995037, tensor(-0.0036)),\n",
              " (-0.0280349999666214, tensor(-0.0280)),\n",
              " (0.01331700012087822, tensor(0.0133)),\n",
              " (0.052799999713897705, tensor(0.0528)),\n",
              " (-0.0038670001085847616, tensor(-0.0039)),\n",
              " (0.0278290007263422, tensor(0.0278)),\n",
              " (-0.06926199793815613, tensor(-0.0693)),\n",
              " (-0.03374699875712395, tensor(-0.0337)),\n",
              " (-0.04312000051140785, tensor(-0.0431)),\n",
              " (-0.04517899826169014, tensor(-0.0452)),\n",
              " (0.07910799980163574, tensor(0.0791)),\n",
              " (0.09094499796628952, tensor(0.0909)),\n",
              " (-0.029898999258875847, tensor(-0.0299)),\n",
              " (-0.019439000636339188, tensor(-0.0194)),\n",
              " (0.11968699842691422, tensor(0.1197)),\n",
              " (0.05333299934864044, tensor(0.0533)),\n",
              " (0.041652001440525055, tensor(0.0417)),\n",
              " (-0.0792979970574379, tensor(-0.0793)),\n",
              " (-0.11908700317144394, tensor(-0.1191)),\n",
              " (0.004759000148624182, tensor(0.0048)),\n",
              " (-0.08544500172138214, tensor(-0.0854)),\n",
              " (-0.05149099975824356, tensor(-0.0515)),\n",
              " (0.028829000890254974, tensor(0.0288)),\n",
              " (0.04364600032567978, tensor(0.0436)),\n",
              " (0.023468999192118645, tensor(0.0235)),\n",
              " (-0.03947199881076813, tensor(-0.0395)),\n",
              " (0.06656499952077866, tensor(0.0666)),\n",
              " (0.034348998218774796, tensor(0.0343)),\n",
              " (-0.11351700127124786, tensor(-0.1135)),\n",
              " (0.02963300049304962, tensor(0.0296)),\n",
              " (-0.06839299947023392, tensor(-0.0684)),\n",
              " (-0.07897999882698059, tensor(-0.0790)),\n",
              " (-0.0510299988090992, tensor(-0.0510)),\n",
              " (0.015873000025749207, tensor(0.0159)),\n",
              " (0.006821000017225742, tensor(0.0068)),\n",
              " (-0.01384699996560812, tensor(-0.0138)),\n",
              " (-0.10376500338315964, tensor(-0.1038)),\n",
              " (0.01515199989080429, tensor(0.0152)),\n",
              " (0.06332699954509735, tensor(0.0633)),\n",
              " (0.04613900184631348, tensor(0.0461)),\n",
              " (0.04172300174832344, tensor(0.0417)),\n",
              " (0.07096199691295624, tensor(0.0710)),\n",
              " (-0.05149000138044357, tensor(-0.0515)),\n",
              " (-0.03619299829006195, tensor(-0.0362)),\n",
              " (-0.06507399678230286, tensor(-0.0651)),\n",
              " (0.052661001682281494, tensor(0.0527)),\n",
              " (0.04859500005841255, tensor(0.0486)),\n",
              " (-0.03946699947118759, tensor(-0.0395)),\n",
              " (0.017294999212026596, tensor(0.0173)),\n",
              " (0.02717999927699566, tensor(0.0272)),\n",
              " (-0.0561630018055439, tensor(-0.0562)),\n",
              " (-0.15221500396728516, tensor(-0.1522)),\n",
              " (-0.018895000219345093, tensor(-0.0189)),\n",
              " (-0.03735100105404854, tensor(-0.0374)),\n",
              " (-0.09412600100040436, tensor(-0.0941)),\n",
              " (0.010863999836146832, tensor(0.0109)),\n",
              " (-0.054802000522613525, tensor(-0.0548)),\n",
              " (0.06066400185227394, tensor(0.0607)),\n",
              " (-0.12040700018405914, tensor(-0.1204)),\n",
              " (-0.015932999551296234, tensor(-0.0159)),\n",
              " (-0.014113999903202057, tensor(-0.0141)),\n",
              " (-0.0200399998575449, tensor(-0.0200)),\n",
              " (0.07814499735832214, tensor(0.0781)),\n",
              " (-0.0019209999591112137, tensor(-0.0019)),\n",
              " (0.04787300154566765, tensor(0.0479)),\n",
              " (-0.005268999841064215, tensor(-0.0053)),\n",
              " (0.006395999807864428, tensor(0.0064)),\n",
              " (0.016572000458836555, tensor(0.0166)),\n",
              " (-0.06526400148868561, tensor(-0.0653)),\n",
              " (0.08857300132513046, tensor(0.0886)),\n",
              " (-0.027076000347733498, tensor(-0.0271)),\n",
              " (0.025834999978542328, tensor(0.0258)),\n",
              " (0.04568000137805939, tensor(0.0457)),\n",
              " (0.030288999900221825, tensor(0.0303)),\n",
              " (-0.049747999757528305, tensor(-0.0497)),\n",
              " (0.1397790014743805, tensor(0.1398)),\n",
              " (-0.0281980000436306, tensor(-0.0282)),\n",
              " (0.05893800035119057, tensor(0.0589)),\n",
              " (-0.008240999653935432, tensor(-0.0082)),\n",
              " (0.07239600270986557, tensor(0.0724)),\n",
              " (-0.0644880011677742, tensor(-0.0645)),\n",
              " (-0.022840000689029694, tensor(-0.0228)),\n",
              " (-0.08008400350809097, tensor(-0.0801)),\n",
              " (-0.020614000037312508, tensor(-0.0206)),\n",
              " (-0.043852001428604126, tensor(-0.0439)),\n",
              " (0.025668999180197716, tensor(0.0257)),\n",
              " (-0.03553399816155434, tensor(-0.0355)),\n",
              " (-0.13377399742603302, tensor(-0.1338)),\n",
              " (-0.030028000473976135, tensor(-0.0300)),\n",
              " (-0.017812000587582588, tensor(-0.0178)),\n",
              " (0.04071199893951416, tensor(0.0407)),\n",
              " (-0.06737499684095383, tensor(-0.0674)),\n",
              " (-0.061264000833034515, tensor(-0.0613)),\n",
              " (-0.03962000086903572, tensor(-0.0396)),\n",
              " (0.025469999760389328, tensor(0.0255)),\n",
              " (0.06834100186824799, tensor(0.0683)),\n",
              " (0.11997400224208832, tensor(0.1200)),\n",
              " (-0.05691299960017204, tensor(-0.0569)),\n",
              " (-0.0014329999685287476, tensor(-0.0014)),\n",
              " (-0.022420000284910202, tensor(-0.0224)),\n",
              " (0.006798999849706888, tensor(0.0068)),\n",
              " (0.04721599817276001, tensor(0.0472)),\n",
              " (-0.07181300222873688, tensor(-0.0718)),\n",
              " (-0.020927000790834427, tensor(-0.0209)),\n",
              " (-0.037480998784303665, tensor(-0.0375)),\n",
              " (0.05500100180506706, tensor(0.0550)),\n",
              " (-0.1023859977722168, tensor(-0.1024)),\n",
              " (0.055282000452280045, tensor(0.0553)),\n",
              " (0.12662899494171143, tensor(0.1266)),\n",
              " (-0.01603100076317787, tensor(-0.0160)),\n",
              " (-0.02353700064122677, tensor(-0.0235)),\n",
              " (0.0011030000168830156, tensor(0.0011)),\n",
              " (0.088189996778965, tensor(0.0882)),\n",
              " (-0.009681000374257565, tensor(-0.0097)),\n",
              " (0.05252299830317497, tensor(0.0525)),\n",
              " (0.0008270000107586384, tensor(0.0008)),\n",
              " (0.007962999865412712, tensor(0.0080)),\n",
              " (0.004131999798119068, tensor(0.0041)),\n",
              " (0.08338099718093872, tensor(0.0834)),\n",
              " (-0.12160799652338028, tensor(-0.1216)),\n",
              " (-0.048663001507520676, tensor(-0.0487)),\n",
              " (-0.11925400048494339, tensor(-0.1193)),\n",
              " (0.029802000150084496, tensor(0.0298)),\n",
              " (-0.033851999789476395, tensor(-0.0339)),\n",
              " (-0.03092999942600727, tensor(-0.0309)),\n",
              " (0.023303000256419182, tensor(0.0233)),\n",
              " (0.04667700082063675, tensor(0.0467)),\n",
              " (-0.020750999450683594, tensor(-0.0208)),\n",
              " (0.07173699885606766, tensor(0.0717)),\n",
              " (-0.08808200061321259, tensor(-0.0881)),\n",
              " (0.0019229999743402004, tensor(0.0019)),\n",
              " (-0.08597099781036377, tensor(-0.0860)),\n",
              " (-0.06372500211000443, tensor(-0.0637)),\n",
              " (0.08365499973297119, tensor(0.0837)),\n",
              " (-0.026280999183654785, tensor(-0.0263)),\n",
              " (0.0840110033750534, tensor(0.0840)),\n",
              " (-0.004366000182926655, tensor(-0.0044)),\n",
              " (-0.056814998388290405, tensor(-0.0568)),\n",
              " (-0.08659899979829788, tensor(-0.0866)),\n",
              " (0.009363999590277672, tensor(0.0094)),\n",
              " (0.013836000114679337, tensor(0.0138)),\n",
              " (0.07842300087213516, tensor(0.0784)),\n",
              " (-0.0009580000187270343, tensor(-0.0010)),\n",
              " (-0.00698600010946393, tensor(-0.0070)),\n",
              " (0.09568800032138824, tensor(0.0957)),\n",
              " (-0.07756900042295456, tensor(-0.0776)),\n",
              " (0.04979899898171425, tensor(0.0498)),\n",
              " (0.002518000081181526, tensor(0.0025)),\n",
              " (-0.015180000104010105, tensor(-0.0152)),\n",
              " (-0.01975099928677082, tensor(-0.0198)),\n",
              " (-0.0715700015425682, tensor(-0.0716)),\n",
              " (0.01083999965339899, tensor(0.0108)),\n",
              " (-0.036249998956918716, tensor(-0.0362)),\n",
              " (-0.014430999755859375, tensor(-0.0144)),\n",
              " (0.07866799831390381, tensor(0.0787)),\n",
              " (-0.035342998802661896, tensor(-0.0353)),\n",
              " (0.00784400012344122, tensor(0.0078)),\n",
              " (-0.034348998218774796, tensor(-0.0343)),\n",
              " (0.04451800137758255, tensor(0.0445)),\n",
              " (-0.07725100219249725, tensor(-0.0773)),\n",
              " (-0.02111699990928173, tensor(-0.0211)),\n",
              " (0.09741999953985214, tensor(0.0974)),\n",
              " (-0.03129800036549568, tensor(-0.0313)),\n",
              " (-0.04293299838900566, tensor(-0.0429)),\n",
              " (-0.05431099981069565, tensor(-0.0543)),\n",
              " (-0.05444600060582161, tensor(-0.0544)),\n",
              " (-0.014852000400424004, tensor(-0.0149)),\n",
              " (0.06117599830031395, tensor(0.0612)),\n",
              " (0.001538999960757792, tensor(0.0015)),\n",
              " (0.03419499844312668, tensor(0.0342)),\n",
              " (-0.038995999842882156, tensor(-0.0390)),\n",
              " (-0.05702599883079529, tensor(-0.0570)),\n",
              " (-0.0433490015566349, tensor(-0.0433)),\n",
              " (0.03438499942421913, tensor(0.0344)),\n",
              " (0.0021430000197142363, tensor(0.0021)),\n",
              " (0.019686000421643257, tensor(0.0197)),\n",
              " (-0.030484000220894814, tensor(-0.0305)),\n",
              " (0.0018179999897256494, tensor(0.0018)),\n",
              " (-0.019238000735640526, tensor(-0.0192)),\n",
              " (0.0494920015335083, tensor(0.0495)),\n",
              " (-0.07345599681138992, tensor(-0.0735)),\n",
              " (0.0030169999226927757, tensor(0.0030)),\n",
              " (0.025877999141812325, tensor(0.0259)),\n",
              " (-0.015065999701619148, tensor(-0.0151)),\n",
              " (0.012485000304877758, tensor(0.0125)),\n",
              " (0.014488999731838703, tensor(0.0145)),\n",
              " (-0.0367170013487339, tensor(-0.0367)),\n",
              " (-0.07155899703502655, tensor(-0.0716)),\n",
              " (0.007685000076889992, tensor(0.0077)),\n",
              " (0.03190299868583679, tensor(0.0319)),\n",
              " (0.023531999439001083, tensor(0.0235)),\n",
              " (0.02477799914777279, tensor(0.0248)),\n",
              " (-0.017715999856591225, tensor(-0.0177)),\n",
              " (-0.06267300248146057, tensor(-0.0627)),\n",
              " (0.03956399857997894, tensor(0.0396)),\n",
              " (-0.04940300062298775, tensor(-0.0494)),\n",
              " (-0.00010599999950500205, tensor(-0.0001)),\n",
              " (-0.13193100690841675, tensor(-0.1319)),\n",
              " (-0.0906819999217987, tensor(-0.0907)),\n",
              " (0.0036069999914616346, tensor(0.0036)),\n",
              " (-0.05367499962449074, tensor(-0.0537)),\n",
              " (-0.00019700000120792538, tensor(-0.0002)),\n",
              " (-0.006839999929070473, tensor(-0.0068)),\n",
              " (0.0070420000702142715, tensor(0.0070)),\n",
              " (0.009103000164031982, tensor(0.0091)),\n",
              " (-0.023142000660300255, tensor(-0.0231)),\n",
              " (-0.11744300276041031, tensor(-0.1174)),\n",
              " (0.052303001284599304, tensor(0.0523)),\n",
              " (-0.0690699964761734, tensor(-0.0691)),\n",
              " (-0.05322600156068802, tensor(-0.0532)),\n",
              " (-0.05609399825334549, tensor(-0.0561)),\n",
              " (-0.025892000645399094, tensor(-0.0259)),\n",
              " (0.00596599979326129, tensor(0.0060)),\n",
              " (-0.06657499819993973, tensor(-0.0666)),\n",
              " (0.07271599769592285, tensor(0.0727)),\n",
              " (-0.06844700127840042, tensor(-0.0684)),\n",
              " (0.033805001527071, tensor(0.0338)),\n",
              " (0.04688800126314163, tensor(0.0469)),\n",
              " (-0.046434998512268066, tensor(-0.0464)),\n",
              " (-0.01676899939775467, tensor(-0.0168)),\n",
              " (0.048037998378276825, tensor(0.0480)),\n",
              " (0.013744000345468521, tensor(0.0137)),\n",
              " (0.13751299679279327, tensor(0.1375)),\n",
              " (-0.1078369989991188, tensor(-0.1078)),\n",
              " (0.03556400164961815, tensor(0.0356)),\n",
              " (-0.021066000685095787, tensor(-0.0211)),\n",
              " (0.020526999607682228, tensor(0.0205)),\n",
              " (0.1335819959640503, tensor(0.1336)),\n",
              " (-0.05124000087380409, tensor(-0.0512)),\n",
              " (-0.053971000015735626, tensor(-0.0540)),\n",
              " (0.02145499922335148, tensor(0.0215)),\n",
              " (-0.11307699978351593, tensor(-0.1131)),\n",
              " (-0.05997899919748306, tensor(-0.0600)),\n",
              " (-0.00046499999007210135, tensor(-0.0005)),\n",
              " (-0.04348500072956085, tensor(-0.0435)),\n",
              " (-0.06770499795675278, tensor(-0.0677)),\n",
              " (0.005665000062435865, tensor(0.0057)),\n",
              " (0.09228000044822693, tensor(0.0923)),\n",
              " (0.07158499956130981, tensor(0.0716)),\n",
              " (0.046539999544620514, tensor(0.0465)),\n",
              " (0.07345999777317047, tensor(0.0735)),\n",
              " (-0.14592300355434418, tensor(-0.1459)),\n",
              " (-0.029299000278115273, tensor(-0.0293)),\n",
              " (0.017930999398231506, tensor(0.0179)),\n",
              " (-0.016599999740719795, tensor(-0.0166)),\n",
              " (-0.018888000398874283, tensor(-0.0189)),\n",
              " (0.0014140000566840172, tensor(0.0014)),\n",
              " (0.016493000090122223, tensor(0.0165)),\n",
              " (0.1693190038204193, tensor(0.1693)),\n",
              " (-0.0466499999165535, tensor(-0.0466)),\n",
              " (0.03818399831652641, tensor(0.0382)),\n",
              " (-0.09882599860429764, tensor(-0.0988)),\n",
              " (-0.1012599989771843, tensor(-0.1013)),\n",
              " (0.03614300116896629, tensor(0.0361)),\n",
              " (0.0070409998297691345, tensor(0.0070)),\n",
              " (-0.02167000062763691, tensor(-0.0217)),\n",
              " (0.088468998670578, tensor(0.0885)),\n",
              " (-0.005425000097602606, tensor(-0.0054)),\n",
              " (0.06529799848794937, tensor(0.0653)),\n",
              " (0.050909001380205154, tensor(0.0509)),\n",
              " (-0.008940000087022781, tensor(-0.0089)),\n",
              " (-0.018411999568343163, tensor(-0.0184)),\n",
              " (0.03640500083565712, tensor(0.0364)),\n",
              " (0.09963499754667282, tensor(0.0996)),\n",
              " (0.03282700106501579, tensor(0.0328)),\n",
              " (-0.0030759999062865973, tensor(-0.0031)),\n",
              " (-0.06240899860858917, tensor(-0.0624)),\n",
              " (-0.11388099938631058, tensor(-0.1139)),\n",
              " (0.06364999711513519, tensor(0.0636)),\n",
              " (-0.07007800042629242, tensor(-0.0701)),\n",
              " (0.062359001487493515, tensor(0.0624)),\n",
              " (0.04192600026726723, tensor(0.0419)),\n",
              " (-0.0966859981417656, tensor(-0.0967)),\n",
              " (0.05631599947810173, tensor(0.0563)),\n",
              " (-0.00356299988925457, tensor(-0.0036)),\n",
              " (0.016566000878810883, tensor(0.0166)),\n",
              " (0.01268099993467331, tensor(0.0127)),\n",
              " (-0.056793998926877975, tensor(-0.0568)),\n",
              " (0.018001999706029892, tensor(0.0180)),\n",
              " (-0.06620600074529648, tensor(-0.0662)),\n",
              " (0.03259199857711792, tensor(0.0326)),\n",
              " (-0.04664099961519241, tensor(-0.0466)),\n",
              " (0.0037960000336170197, tensor(0.0038)),\n",
              " (-0.01889899931848049, tensor(-0.0189)),\n",
              " (-0.02495500072836876, tensor(-0.0250)),\n",
              " (-0.04499499872326851, tensor(-0.0450)),\n",
              " (0.10469499975442886, tensor(0.1047))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, creamos una red neuronal con una capa de Embedding como primera capa (cargamos en ella la matriz de pesos) y una capa GRU. Al hacer forward debemos llamar primero a la capa de embedding."
      ],
      "metadata": {
        "id": "Ommnh8-ff_kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn \n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "7uvgVeMLOG79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyNN(nn.Module):\n",
        "    def __init__(self, weights_matrix, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        \n",
        "    def forward(self, inp, hidden):\n",
        "        return self.gru(self.embedding(inp), hidden)"
      ],
      "metadata": {
        "id": "0zDjJ12ROeBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ToyNN(weights_matrix,256,2)"
      ],
      "metadata": {
        "id": "kRrUFLkgNXjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}