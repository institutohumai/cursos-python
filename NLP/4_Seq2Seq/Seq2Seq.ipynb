{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq",
      "provenance": [],
      "collapsed_sections": [
        "8Ke8ufJfxrUz",
        "vAGmz5MuAvRA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/4_Seq2Seq/Seq2Seq.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "rUYbjU7bHYIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de Secuencia a Secuencia"
      ],
      "metadata": {
        "id": "50DBDG03IO25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos de secuencia a secuencia son modelos de aprendizaje profundo que han logrado mucho éxito en tareas como traducción automática, resumen de texto y subtítulos de imágenes. Google Translate comenzó a usar un modelo de este tipo en producción a finales de 2016. \n",
        "\n",
        "Un modelo de secuencia a secuencia es un modelo que toma una secuencia de elementos (palabras, letras, características de una imagen, etc.) y genera otra secuencia de elementos. \n",
        "\n",
        "![Imgur](https://i.imgur.com/nNOUbuN.gif)"
      ],
      "metadata": {
        "id": "QxqQucfPISag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura Encoder-Decoder\n",
        "\n",
        "Para manejar este tipo de entradas y salidas, podemos diseñar una arquitectura con dos componentes principales.\n",
        "\n",
        "1. El primer componente es un `Encoder` (codificador): toma una secuencia de longitud variable como entrada y la transforma en un estado (también llamado contexto) con una forma fija. \n",
        "2. El segundo componente es un `Decoder` (encoder): mapea el estado codificado de una forma fija a una secuencia de longitud variable.\n",
        "\n",
        "\n",
        "![Imgur](https://i.imgur.com/dd2Qril.gif)"
      ],
      "metadata": {
        "id": "PUeteW93RLYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En las siguientes celdas vamos a definir una interfaz para el decoder y otra para el decoder. Las interfaces son clases que establecen las responsabilidades básicas de un objeto, pero que dejan que cada objeto decida como implementarlas.\n",
        "\n",
        "Como vemos en la siguiente celda un Encoder es un modelo que recibe una entrada secuencial X (y algún otro argumento opcional) y genera una salida."
      ],
      "metadata": {
        "id": "44AX7xJqSGR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def forward(self, X, *args):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "IMiAMZKaS5ke"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otro lado, el decoder debe tener una función adicional `init_state` para convertir la salida del encoder en el vector estado codificado. Y el forward debe recibir el estado codificado y la entrada X."
      ],
      "metadata": {
        "id": "qx51AvMZS_HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "AjWpa1OkR9N6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al final, la arquitectura Encoder-Decoder contiene tanto un encoder como un decoder, con argumentos adicionales opcionales. En la función forward, la salida del encoder se usa para producir el estado codificado, y el decoder usará este estado como una de sus entradas."
      ],
      "metadata": {
        "id": "0Drxqbw0W1X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
        "        # Sólo devuelve la salida del decoder\n",
        "        return self.decoder(dec_X, dec_state)[0]"
      ],
      "metadata": {
        "id": "PzedBSz3XVjW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Traducción Automática (Neuronal)"
      ],
      "metadata": {
        "id": "CYGAfF0TYCcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La traducción automática se refiere a la transformación automática de una secuencia de un idioma a otro. Durante décadas, los enfoques estadísticos habían dominado este campo antes del surgimiento del aprendizaje de extremo a extremo utilizando redes neuronales. Esta última a menudo se denomina traducción automática neuronal para distinguirse de la traducción automática estadística que implica el análisis estadístico en componentes como un modelo de traducción y un modelo de lenguaje. \n",
        "\n"
      ],
      "metadata": {
        "id": "z4r2-IytYD4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La traducción automática neuronal se implementa como un modelo de secuencia a secuencia donde el encoder y el decoder son RNNs y el contexto se obtiene a partir de las variables ocultas.\n",
        "\n",
        "En la siguiente visualización, cada pulso para el encoder o decoder es esta RNN procesando sus entradas y generando una salida para ese paso de tiempo. Dado que tanto el encoder como el decoder son RNN, cada vez que el paso uno de los RNN realiza algún procesamiento, actualiza su estado oculto en función de sus entradas y las entradas anteriores que ha visto.\n",
        "\n",
        "![Imgur](https://i.imgur.com/6AUERAx.gif)"
      ],
      "metadata": {
        "id": "pfWl9fRJYYle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos los estados ocultos del encoder. Observe cómo el último estado oculto es en realidad el contexto que pasamos al decoder. El decoder también mantiene un estado oculto que pasa de un paso de tiempo al siguiente. Simplemente no lo visualizamos en este gráfico porque estamos interesados en las partes principales del modelo por ahora."
      ],
      "metadata": {
        "id": "ozxSnbRNZipg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n"
      ],
      "metadata": {
        "id": "b0mAILZscktG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Técnicamente hablando, el encoder transforma una secuencia de entrada de longitud variable en una variable de contexto de forma fija $\\mathbf{c}$ y codifica la información de la secuencia de entrada en esta variable de contexto. Como se muestra en las figuras anteriores, podemos usar un RNN para diseñar dicho encoder.\n",
        "\n",
        "Consideremos un ejemplo de secuencia (tamaño de lote: 1). Suponga que la secuencia de entrada es $x_1, \\ldots, x_T$\n",
        ", tal que $x_t$ es el $t^{\\mathrm{ésimo}}$ token en la secuencia de texto de entrada. En el paso de tiempo $t$, la RNN transforma el vector de características de entrada $\\mathbf{x}_t$ para $x_t$ y el estado oculto $\\mathbf{h} _{t-1}$ del paso de tiempo anterior en el estado oculto actual. Podemos usar una función para expresar la transformación de la capa recurrente de la RNN:\n",
        "\n",
        "$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}). $$\n",
        "\n",
        "En general, el encoder transforma los estados ocultos en todos los pasos de tiempo en la variable de contexto a través de una función personalizada $q$\n",
        "\n",
        "$$\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n",
        "\n",
        "Por ejemplo, al elegir $q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$ como en la figura de la sección anterior, la variable de contexto es solo el estado oculto $\\mathbf{h}_T$\n",
        "de la secuencia de entrada en el paso de tiempo final.\n",
        "\n",
        "Hasta ahora, hemos utilizado un RNN unidireccional para diseñar el encoder, donde un estado oculto solo depende de la subsecuencia de entrada hasta el paso de tiempo actual del estado oculto. También podemos construir encoders usando RNN bidireccionales. En este caso, un estado oculto depende de la subsecuencia anterior y posterior al paso de tiempo (incluida la entrada en el paso de tiempo actual), que codifica la información de toda la secuencia.\n",
        "\n",
        "Ahora implementemos el encoder RNN. Tenga en cuenta que usamos una capa de embedding para obtener el vector de características para cada token en la secuencia de entrada. El peso de una capa de embedding es una matriz cuyo número de filas es igual al tamaño del vocabulario de entrada (vocab_size) y el número de columnas es igual a la dimensión del vector de características (embed_size). Para cualquier índice de token de entrada $i$, la capa de embedding obtiene la fila (a partir de 0) de la matriz de peso para devolver su vector de características. Además, aquí elegimos una GRU multicapa para implementar el encoder."
      ],
      "metadata": {
        "id": "MrsbXDsDcnN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_RNN(module):\n",
        "    if type(module) == nn.Linear:\n",
        "         nn.init.xavier_uniform_(module.weight)\n",
        "    if type(module) == nn.GRU:\n",
        "        for param in module._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(module._parameters[param])\n",
        "\n",
        "class RNNEncoder(Encoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
        "        self.hid_dim = num_hiddens\n",
        "        self.n_layers = num_layers\n",
        "        self.apply(init_RNN)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # X shape: (num_steps, batch_size)\n",
        "        embs = self.embedding(X.type(torch.int64))\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        output, state = self.rnn(embs)\n",
        "        # recuerde que output devolverá siempre el valor de las variables ocultas (de todas las direcciones) de la última capa\n",
        "        # por cada token en la secuencia de cada elemento del lote (num_steps, batch size, hid dim * n directions) .\n",
        "        # rnn_outputs shape: (num_steps, batch size, hid dim) \n",
        "        # recuerde que state devolverá siempre el valor de las variables ocultas de todas las capas \n",
        "        # (y en cada dirección si hay mas de una) de cada elemento del lote. (n layers * n directions, batch_size, num_hiddens) \n",
        "        # state shape: (num_layers, batch_size, num_hiddens)   \n",
        "        return output, state"
      ],
      "metadata": {
        "id": "cq26dXXihAJw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usemos un ejemplo concreto para ilustrar la implementación del encoder anterior. A continuación instanciamos un encoder GRU de dos capas cuyo número de unidades ocultas es 16. Dado un minilote de entradas de secuencia X (tamaño de lote: 4, número de pasos de tiempo: 9), los estados ocultos de la última capa en todos los pasos de tiempo (`outputs` devueltas por las capas recurrentes del encoder) son un tensor de forma (número de pasos de tiempo, tamaño del lote, número de unidades ocultas)."
      ],
      "metadata": {
        "id": "2_guzgYrhb7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
        "batch_size, num_steps = 4, 9\n",
        "\n",
        "encoder = RNNEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "X = torch.zeros((batch_size, num_steps))\n",
        "outputs, state = encoder(X)\n",
        "outputs.shape, (num_steps,batch_size,num_hiddens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHGX_MC5jXC0",
        "outputId": "a517e007-2e33-458f-9edd-b0c3d8f7707b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 9, 16]), (9, 4, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder \n",
        "\n",
        "Como acabamos de mencionar, la variable de contexto $\\mathbf{c}$ de la salida del encoder codifica toda la secuencia de entrada $x_1, \\ldots, x_T$. Dada la secuencia de salida $y_1, y_2, \\ldots, y_{T'}$ del conjunto de datos de entrenamiento, para cada paso de tiempo $t'$ (el símbolo difiere del paso de tiempo $t$ de las secuencias de entrada del encoder), la probabilidad de que la salida del decoder $y_{t'}$ está condicionada a la subsecuencia de salida anterior $y_1, \\ldots, y_{t'-1}$ y la variable de contexto $\\mathbf{c}$, es decir, $P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$.\n",
        "\n",
        "Para modelar esta probabilidad condicional en secuencias, podemos usar otro RNN como decoder. En cualquier paso de tiempo $t^\\prime$ en la secuencia de salida, la RNN toma la salida $y_{t^\\prime-1}$ del paso de tiempo anterior y la variable de contexto $\\mathbf{c}$ como su entrada, luego los transforma junto con el estado oculto anterior $\\mathbf{s}_{t^\\prime-1}$ en el estado oculto $\\mathbf{s}_{t^\\prime}$ en el paso de tiempo actual. Como resultado, podemos usar una función $g$ para expresar la transformación de la capa oculta del decoder:\n",
        "\n",
        "$$\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1}).$$\n",
        "\n",
        "![Imgur](https://i.imgur.com/TwY4S1M.png)\n",
        "\n",
        "Después de obtener el estado oculto del decoder, podemos usar una capa de salida y la operación softmax para calcular la distribución de probabilidad condicional $P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$ para la salida en el paso de tiempo $t^\\prime$.\n",
        "\n",
        "Siguiendo la figura, al implementar el decoder de la siguiente manera, usamos directamente el estado oculto en el paso de tiempo final del encoder para inicializar el estado oculto del decoder. Esto requiere que el encoder RNN y el decoder RNN **tengan el mismo número de capas y unidades ocultas**. Para incorporar aún más la información de la secuencia de entrada codificada, la variable de contexto se concatena con la entrada del decoder en todos los pasos de tiempo. Para predecir la distribución de probabilidad del token de salida, se usa una capa densa para transformar el estado oculto en la capa final del decoder RNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "anXkDvKXZpHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNDecoder(Decoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens,\n",
        "                           num_layers, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_hiddens, vocab_size)\n",
        "        self.hid_dim = num_hiddens\n",
        "        self.n_layers = num_layers\n",
        "        self.output_dim = vocab_size\n",
        "        self.apply(init_RNN)\n",
        "\n",
        "    def init_state(self, enc_output, *args):\n",
        "        #se queda con el output del último paso de tiempo\n",
        "        return enc_output[-1]\n",
        "\n",
        "    def forward(self, X, context, hidden):\n",
        "        # X shape: (batch_size)\n",
        "        # context shape: (batch_size, num_hiddens)\n",
        "        # hidden shape: (n layers, batch_size, num_hiddens)\n",
        "\n",
        "        input = X.unsqueeze(0)\n",
        "        context = context.unsqueeze(0)\n",
        "        #input shape: (1,batch_size)\n",
        "        #context shape: (1, batch_size, num_hiddens)\n",
        "\n",
        "        embs = self.embedding(input)\n",
        "        # embs shape: (1, batch_size, embed_size)\n",
        "\n",
        "        # Concatena el token de entrada con el contexto\n",
        "        embs_and_context = torch.cat((embs, context), -1)\n",
        "        #embs_and_context shape: (1, batch size, embed_size + num_hiddens)\n",
        "        \n",
        "        #Genera las salidas de la RNN\n",
        "        rnn_outputs, state = self.rnn(embs_and_context, hidden)\n",
        "        # recuerde que output devolverá siempre el valor de las variables ocultas (de todas las direcciones) de la última capa\n",
        "        # por cada token en la secuencia de cada elemento del lote (seq_len, batch size, hid dim * n directions) .\n",
        "        # rnn_outputs shape: (1, batch size, hid dim) \n",
        "        # recuerde que state devolverá siempre el valor de las variables ocultas de todas las capas \n",
        "        # (y en cada dirección si hay mas de una) de cada elemento del lote. (n layers * n directions, batch_size, num_hiddens) \n",
        "        # state shape: (num_layers, batch_size, num_hiddens)       \n",
        "\n",
        "        #Genera las probabilidades de cada token en el vocabulario\n",
        "        outputs = self.linear(rnn_outputs.squeeze(0))\n",
        "        #outputs = torch.cat((embs.squeeze(0), rnn_outputs.squeeze(0), context),dim = 1)\n",
        "\n",
        "        # outputs shape: (batch_size, vocab_size)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "7MiiT6JPfU2o"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teacher Forcing\n",
        "\n",
        "Si bien la entrada del encoder son solo los tokens de la secuencia de origen, la entrada y la salida del decoder no son tan sencillas en el entrenamiento de un encoder-decoder. Una opción es alimentar al decoder con la salida del último paso de tiempo $y_{t-1}$ como entrada para el modelo en el paso de tiempo actual $X_t$. Esto causa que cada token predicho sea esencial para la predicción de los siguientes, aunque quizás puede resultar en una oración de salida completamente distinta si hay errores en los primeros tokens.\n",
        "\n",
        "Otro enfoque es el Teacher Forcing (forzado del maestro), que es una estrategia para entrenar redes neuronales recurrentes que utiliza la etiqueta de la frase de destino como entrada, en lugar de la salida del modelo de un paso de tiempo anterior como entrada. El forzado del maestro funciona utilizando la salida real o esperada del conjunto de datos de entrenamiento en el paso de tiempo actual $y_t$ como entrada en el siguiente paso de tiempo $X_{t+1}$, en lugar de la salida generada por la red. Este enfoque tiene la ventaja de que errores al principio de la oración no condicionan la frase entera al entrenar, pero puede ocasionar que la red no sepa como sobreponerse a errores previos debido a que está acostumbrada a recibir siempre el token anterior correcto.\n",
        "\n",
        "Quizás un enfoque intermedio sea alternar aleatoriamente entre ambas opciones como haremos en este notebook. Agregamos un hiperparámetro que gestiona la probabilidad de que se aplique teacher forcing."
      ],
      "metadata": {
        "id": "21iZLFsHnuHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Seq2Seq_TF(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"¡Las dimensiones ocultas del encoder y del decoder deben ser iguales!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"¡El número de capas del encoder y del decoder deben ser iguales!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        enc_outputs, enc_hidden = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        context = self.decoder.init_state(enc_outputs)\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and context\n",
        "            dec_output, dec_hidden = self.decoder(input, context, enc_hidden)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "MVoz70K8rY9W"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función de Pérdida\n",
        "\n"
      ],
      "metadata": {
        "id": "8Ke8ufJfxrUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cada paso de tiempo, el decoder predice una distribución de probabilidad para los tokens de salida. Podemos aplicar softmax para obtener la distribución y calcular la pérdida de entropía cruzada para la optimización. \n",
        "\n",
        "![Imgur](https://i.imgur.com/Kd0z00x.png)\n",
        "\n",
        "Recuerde de la clase de embeddings que tokens de relleno se agregan al final de las secuencias para que las secuencias de diferentes longitudes se puedan cargar de manera eficiente en minilotes de la misma forma. Sin embargo, la predicción de los tokens de relleno debe excluirse de los cálculos de pérdidas. En la clase de embeddings usamos máscaras para eliminar los rellenos, acá podemos usar el parámetro `ignore_index` de las pérdidas de torch para que ignore los tokens `<pad>`"
      ],
      "metadata": {
        "id": "wS4ZjPQun4NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La siguiente celda no funcionará hasta que se cargue el vocabulario\n",
        "# y se establezca el índice de padding\n",
        "#loss = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "6_0Mabon-u2g"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando los Datos\n",
        "\n"
      ],
      "metadata": {
        "id": "vAGmz5MuAvRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a entrenar un modelo que traduzca al español frases escritas en inglés. Para eso necesitamos un dataset que tenga frases escritas en ambos idiomas para usar como datasets de origen y destino.\n",
        "\n",
        "Usaremos el dataset publicado por la organización [Tatoeba](https://tatoeba.org/es) que vamos a descargar en la siguiente celda."
      ],
      "metadata": {
        "id": "GpXu5ViyUQWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import re\n",
        "from shutil import unpack_archive\n",
        "\n",
        "data = None\n",
        "!wget -O spa-eng.zip http://www.manythings.org/anki/spa-eng.zip\n",
        "if not os.path.isfile(\"spa.txt\"):\n",
        "    unpack_archive('./spa-eng.zip', extract_dir='./', format='zip')\n",
        "with open('./spa.txt', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "    data = re.sub(\"\\tCC-BY 2\\.0.*\",\"\",data) # acá elimino información adicional\n",
        "    data = re.sub(r\"[\\u202f]|[\\xa0]\",\" \",data) # aca saco caracteres raros\n",
        "    data = re.sub(\"([,\\.:;!?])\",\" \\\\1\",data) # aca  y abajo tokenizo puntuación\n",
        "    data = re.sub(\"([¡¿])\",\"\\\\1 \",data).lower()\n"
      ],
      "metadata": {
        "id": "S_WUXllMA-3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725308a5-fa45-4dbd-adb6-5db7c11a46e3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-27 16:19:41--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5320329 (5.1M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.07M  7.12MB/s    in 0.7s    \n",
            "\n",
            "2022-08-27 16:19:42 (7.12 MB/s) - ‘spa-eng.zip’ saved [5320329/5320329]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la salida de la siguiente celda podemos ver la estructura de los datos: los tokens de cada frase están separados por un espacio y las frases están separadas por un tab."
      ],
      "metadata": {
        "id": "F2phbNyoVfYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IopirAQHVvQt",
        "outputId": "0828127f-f7ec-4c26-9eb9-51df1b5de061"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go .\tve .\n",
            "go .\tvete .\n",
            "go .\tvaya .\n",
            "go .\tváyase .\n",
            "hi .\thola .\n",
            "run !\t¡ corre !\n",
            "run !\t¡ corran !\n",
            "run !\t¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora tenemos que crear dos vocabularios para almacenar las frases en inglés y español respectivamente. En la siguiente celda separamos las frases usando el caracter `\\t` que representa un tab. Luego agregamos los tokens especiales `<bos>` y `<eos>` que van a simbolizar el inicio y el final de oración respectivamente. El inicio de oración solo lo usamos en la frase de destino para ser usado como entrada en el paso de tiempo 1 del decoder (para saber que tiene que generar la primera palabra usando solo la información provista por el vector de contexto).  "
      ],
      "metadata": {
        "id": "fPL8taPYU273"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "SRC_IDX, TGT_IDX = 0, 1\n",
        "\n",
        "SEED = 12312\n",
        "\n",
        "data2 = data.split('\\n')\n",
        "random.seed(SEED)\n",
        "random.shuffle(data2)\n",
        "\n",
        "data_list = []\n",
        "for i, line in enumerate(data2):\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "        # Skip empty tokens\n",
        "        new_src = [t for t in f'{parts[SRC_IDX]} <eos>'.split(' ') if t]\n",
        "        new_tgt = [t for t in f'<bos> {parts[TGT_IDX]} <eos>'.split(' ') if t]\n",
        "        length_src = len(new_src)\n",
        "        data_list.append((new_src, length_src, new_tgt))\n",
        "\n",
        "print(data_list[0][0], data_list[0][-1])\n",
        "print(len(data_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VeFH-9-BuSX",
        "outputId": "a5462aad-df6d-4f13-de1d-c659771d26bb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom', 'seldom', 'puts', 'sugar', 'in', 'his', 'coffee', '.', '<eos>'] ['<bos>', 'tom', 'casi', 'nunca', 'le', 'pone', 'azúcar', 'al', 'café', '.', '<eos>']\n",
            "138440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos crear los vocabularios de torchtext indicando todos los caracteres especiales (incluyendo '<unk>' para reemplazar las palabras que no aparecen en el vocabulario)."
      ],
      "metadata": {
        "id": "I7c9NpbXYFLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n = len(data_list)\n",
        "split1, split2 = int(0.7*n), int(0.9*n)\n",
        "train_list = data_list[:split1]\n",
        "val_list = data_list[split1:split2]\n",
        "test_list = data_list[split2:]\n",
        "\n",
        "counter_src, counter_tgt = Counter(), Counter()\n",
        "for i in range(len(train_list)):\n",
        "  counter_src.update(train_list[i][0])\n",
        "  counter_tgt.update(train_list[i][-1])\n",
        "\n",
        "vocab_src = vocab(counter_src, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "\n",
        "vocab_tgt = vocab(counter_tgt, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n"
      ],
      "metadata": {
        "id": "MlU1LfacCAhT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes celdas llevan adelante el preprocesamiento necesario para generar los dataloaders al igual que lo hicimos en el ejercicio de la clase 1. En la siguiente celda `BucketSampler` es una clase que arma los minilotes con oraciones de longitud parecida de manera que el relleno necesario sea mínimo.\n"
      ],
      "metadata": {
        "id": "tIfgHjHXZnCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class BucketSampler(Sampler):\n",
        "\n",
        "    def __init__(self, batch_size, train_list):\n",
        "        self.length = len(train_list)\n",
        "        self.train_list = train_list\n",
        "        self.batch_size = batch_size\n",
        "        indices = [(i, s[1]) for i, s in enumerate(self.train_list)]\n",
        "        random.seed(SEED)\n",
        "        random.shuffle(indices)\n",
        "        pooled_indices = []\n",
        "        # creamos minilotes de tamaños similares\n",
        "        for i in range(0, len(indices), batch_size * 100):\n",
        "            pooled_indices.extend(sorted(indices[i:i + batch_size * 100],\n",
        "                                         key=lambda x: x[1], reverse=True))\n",
        "\n",
        "        self.pooled_indices = pooled_indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.pooled_indices), self.batch_size):\n",
        "            yield [idx for idx, _ in self.pooled_indices[i:i + self.batch_size]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.length + self.batch_size - 1) // self.batch_size"
      ],
      "metadata": {
        "id": "YjpbkiK2CMV6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`collate_batch` es una función que le indica al DataLoader que datos devolver por cada ejemplo del Dataset. En este caso vamos a generar 3 lotes distintos en una tupla:\n",
        "* text_src: son las frases en el idioma origen que le pasaremos como entrada al encoder\n",
        "* text_tgt_in: son las frases en el idioma destino que le pasaremos como entrada al decoder (con `<bos>` como primer token y sin `<eos>`)\n",
        "* text_tgt_out: son las frases en el idioma destino que usaremos para calcular la pérdida (con `<eos>` como finalizador de oración y sin `<bos>`)"
      ],
      "metadata": {
        "id": "aluwHRsBe-xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "SRC_PAD_IDX = vocab_src['<pad>']\n",
        "TGT_PAD_IDX = vocab_tgt['<pad>']\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_src, length_list, text_tgt_in, text_tgt_out = [], [], [], []\n",
        "    for (src, length, tgt) in batch:\n",
        "        # convertimos el texto en tokens\n",
        "        processed_src = torch.tensor([vocab_src[token] for token in src])\n",
        "        processed_tgt = torch.tensor([vocab_tgt[token] for token in tgt])\n",
        "        text_src.append(processed_src)\n",
        "        text_tgt_in.append(processed_tgt[:-1])\n",
        "        text_tgt_out.append(processed_tgt[1:])\n",
        "        # guardamos la longitud de cada token\n",
        "        length_list.append(length)\n",
        "    # armamos la tupla que conformara un ejemplo de minilote.\n",
        "    result = (pad_sequence(text_src, padding_value=SRC_PAD_IDX),\n",
        "              pad_sequence(text_tgt_in, padding_value=TGT_PAD_IDX),\n",
        "              pad_sequence(text_tgt_out, padding_value=TGT_PAD_IDX),)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ipx7mj_YCOrw"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos crear los DataLoaders e imprimir su contenido."
      ],
      "metadata": {
        "id": "8EJaMXZVhMSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # A batch size of 64\n",
        "\n",
        "train_bucket = BucketSampler(batch_size, train_list)\n",
        "train_iter = DataLoader(train_list,\n",
        "                          batch_sampler=train_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "val_bucket = BucketSampler(batch_size, val_list)\n",
        "val_iter = DataLoader(val_list,\n",
        "                          batch_sampler=val_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "test_bucket = BucketSampler(batch_size, test_list)\n",
        "test_iter = DataLoader(test_list,\n",
        "                          batch_sampler=test_bucket,\n",
        "                          collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "2T5ppiECCSCv"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, input, out = next(iter(test_iter))\n",
        "\n",
        "a = [vocab_src.get_itos()[token] for example in src.T[:1] for token in example]\n",
        "b = [vocab_tgt.get_itos()[token] for example in out.T[:1] for token in example]\n",
        "c = [vocab_tgt.get_itos()[token] for example in input.T[:1] for token in example]\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "kpKpN6OJCUEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af05b0e3-3b4d-4aa7-fd6d-2e1f4ffd619f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', '1969', ',', 'roger', 'miller', '<unk>', 'a', 'song', 'called', '\"you', \"don't\", 'want', 'my', 'love', '.\"', 'today', ',', 'this', 'song', 'is', 'better', 'known', 'as', '<unk>', 'the', 'summer', 'time', '.\"', \"it's\", 'the', 'first', 'song', 'he', 'wrote', 'and', 'sang', 'that', 'became', 'popular', '.', '<eos>']\n",
            "['en', '1969', ',', 'roger', 'miller', 'grabó', 'una', 'canción', 'llamada', '<unk>', 'no', 'quieres', 'mi', '<unk>', '.', 'hoy', ',', 'esta', 'canción', 'es', 'más', 'conocida', 'como', '\"en', 'el', '<unk>', '.', 'es', 'la', 'primera', 'canción', 'que', 'escribió', 'y', 'cantó', 'que', 'se', 'convirtió', 'popular', '.', '<eos>']\n",
            "['<bos>', 'en', '1969', ',', 'roger', 'miller', 'grabó', 'una', 'canción', 'llamada', '<unk>', 'no', 'quieres', 'mi', '<unk>', '.', 'hoy', ',', 'esta', 'canción', 'es', 'más', 'conocida', 'como', '\"en', 'el', '<unk>', '.', 'es', 'la', 'primera', 'canción', 'que', 'escribió', 'y', 'cantó', 'que', 'se', 'convirtió', 'popular', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bucle de Entrenamiento\n",
        "\n",
        "Ahora que ya tenemos los datos cargados, entrenaremos nuestro modelo."
      ],
      "metadata": {
        "id": "TX3dksqNhj6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, tgt_input, tgt_out = batch\n",
        "        src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)        \n",
        "        #src: son las frases en el idioma origen que le pasaremos como entrada al encoder\n",
        "        #src.shape : [src len, batch size]\n",
        "        #tgt_input: son las frases en el idioma destino que le pasaremos como entrada al decoder (con `<bos>` como primer token y sin `<eos>`)\n",
        "        #tgt_out: son las frases en el idioma destino que usaremos para calcular la pérdida (con `<eos>` como finalizador de oración y sin `<bos>`)\n",
        "        #tgt.shape : [trg len, batch size]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        #como la función de pérdida solo funciona en entradas 2d con objetivos 1d,\n",
        "        # necesitamos aplanar cada una de ellas con .view\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = tgt_out[1:].view(-1)\n",
        "        #trg = [trg len * batch size]\n",
        "        #output = [trg len * batch size, output dim]\n",
        "        \n",
        "        #calculamos los gradientes\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        #recortamos los gradientes para evitar que exploten\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "I3lCQgDphwHn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El loop de evaluación es similar al de entranmiento (sin las actualizaciones de los parámetros), sin embargo, debemos asegurarnos de desactivar el forzamiento del maestro para la evaluación. Esto hará que el modelo solo use sus propias predicciones para hacer más predicciones dentro de una oración, lo que refleja cómo se usaría en producción.\n",
        "\n",
        "Otras diferencias incluyen:\n",
        "* configurar el modelo en modo de evaluación con `model.eval()`. Esto desactivará el dropout (y la normalización de lotes, si se usa).\n",
        "\n",
        "* Usar el bloque `with torch.no_grad()` para asegurarnos de que no se calculen gradientes dentro del bloque. Esto reduce el consumo de memoria y acelera las cosas."
      ],
      "metadata": {
        "id": "GtpJMm6oq2f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt_input, tgt_out = batch\n",
        "            src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)    \n",
        "            output = model(src, tgt_input, 0) #turn off teacher forcing\n",
        "            #output = [trg len, batch size, output dim]\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.view(-1, output_dim)\n",
        "            trg = tgt_out.view(-1)\n",
        "            #trg = [trg len * batch size]\n",
        "            #output = [trg len * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "1T33a6A6orp0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, crearemos una función que usaremos para decirnos cuánto tarda una época."
      ],
      "metadata": {
        "id": "dGYje7psrUsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "IpHOKK_mrT-Q"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último, definimos el modelo, la pérdida y el optimizador."
      ],
      "metadata": {
        "id": "mz1LHA2zrmfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "INPUT_DIM = len(vocab_src.get_itos())\n",
        "OUTPUT_DIM = len(vocab_tgt.get_itos())\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq_TF(enc, dec, device).to(device)\n",
        "\n",
        "model = Seq2Seq_TF(enc, dec, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
      ],
      "metadata": {
        "id": "-OcpXZ9OrzOf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, entrenamos nuestro modelo, guardando los parámetros que nos den la mejor pérdida de validación."
      ],
      "metadata": {
        "id": "KCSWyK8uh7hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pzWdWzNhobv",
        "outputId": "e2879104-395b-4ebe-d7fe-3128fe33ed6c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 48s\n",
            "\tTrain Loss: 4.695 | Train PPL: 109.435\n",
            "\t Val. Loss: 4.624 |  Val. PPL: 101.886\n",
            "Epoch: 02 | Time: 1m 47s\n",
            "\tTrain Loss: 3.298 | Train PPL:  27.047\n",
            "\t Val. Loss: 4.222 |  Val. PPL:  68.165\n",
            "Epoch: 03 | Time: 1m 47s\n",
            "\tTrain Loss: 2.801 | Train PPL:  16.463\n",
            "\t Val. Loss: 4.066 |  Val. PPL:  58.294\n",
            "Epoch: 04 | Time: 1m 47s\n",
            "\tTrain Loss: 2.548 | Train PPL:  12.776\n",
            "\t Val. Loss: 4.056 |  Val. PPL:  57.760\n",
            "Epoch: 05 | Time: 1m 47s\n",
            "\tTrain Loss: 2.415 | Train PPL:  11.186\n",
            "\t Val. Loss: 4.053 |  Val. PPL:  57.595\n",
            "Epoch: 06 | Time: 1m 47s\n",
            "\tTrain Loss: 2.335 | Train PPL:  10.331\n",
            "\t Val. Loss: 4.092 |  Val. PPL:  59.883\n",
            "Epoch: 07 | Time: 1m 47s\n",
            "\tTrain Loss: 2.282 | Train PPL:   9.801\n",
            "\t Val. Loss: 4.085 |  Val. PPL:  59.419\n",
            "Epoch: 08 | Time: 1m 47s\n",
            "\tTrain Loss: 2.252 | Train PPL:   9.508\n",
            "\t Val. Loss: 4.088 |  Val. PPL:  59.610\n",
            "Epoch: 09 | Time: 1m 47s\n",
            "\tTrain Loss: 2.233 | Train PPL:   9.328\n",
            "\t Val. Loss: 4.121 |  Val. PPL:  61.636\n",
            "Epoch: 10 | Time: 1m 47s\n",
            "\tTrain Loss: 2.212 | Train PPL:   9.138\n",
            "\t Val. Loss: 4.121 |  Val. PPL:  61.647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuUf-GYFBUwi",
        "outputId": "251ecb3b-d41a-43a0-86df-669c5eb6336d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 4.067 | Test PPL:  58.395 |\n"
          ]
        }
      ]
    }
  ]
}