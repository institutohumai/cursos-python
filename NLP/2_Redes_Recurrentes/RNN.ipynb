{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/2_Redes_Recurrentes/RNN.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "b2dRsWLvAUfY"
      },
      "id": "b2dRsWLvAUfY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de secuencias\n",
        "\n",
        "Hasta ahora hemos trabajado con series de datos donde a cada entrada le corresponde una salida. Por ejemplo, a una imagen le corresponde una categoría. A una serie de indicadores biométricos le corresponde un diagnóstico médico.\n",
        "\n",
        "En procesamiento de lenguajes naturales, nuestras salidas y nuestras entradas tienen una característica distinta. Veamos un ejemplo:\n",
        "\n",
        "> *Usted tiene 16 años. Está prohibido vender alcohol a menores de 18 años. No puedo venderle esa botella.*\n",
        "\n",
        "En el ejemplo anterior, tenemos tres afirmaciones, donde la última es un conclusión de las dos anteriores. En este sentido, cuando trabajamos con lenguajes tenemos el problema de lo próximo que se dice, depende de lo que se dijo antes. Es decir, estamos trabajando con secuencias temporales.\n",
        "\n",
        "Peor aún, muchas veces la última salida, debe ser tratada como una nueva entrada. Piense en el ejemplo anterior, si usted vive en un país latinoamericano o europeo, al llegar a **menores de** intuía que la edad límite sería **18 años**. Eso es porque como ciudadano de su país, sabe que esa es la ley. Es decir. **18 años** podría haber sido una predicción, una salida de su red. Ademas, al haber predicho **18 años** ahora podemos concluir que **No puedo venderle esa botella**. Si la ley dijera que los menores de **14 años** pueden comprar alcohol, la segunda frase carecería de sentido. Es decir **18 años** es una predicción, una salida en un momento, pero luego se convierte en una entrada o un *feature* en otro.\n",
        "\n",
        "Es por lo anterior que se dice que estos modelos son modelos autoregresivos: las salidas luego se convierten en entradas, como en un problema recursivo.\n",
        "\n",
        "La naturaleza autogresiva de nuestro modelo hace que debamos considerar la calidad de nuestras predicciones. Volviendo al ejemplo anterior, si nuestra predicción hubiera sido **14 años** en lugar de **18 años**, la conclusión final de nuestra frase sería distinta a la que hemos obtenido. Pequeños errores en un nuestras predicciones pueden acumularse a lo largo del tiempo y generar resultados absurdos.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Iz0SY76t3eM"
      },
      "id": "6Iz0SY76t3eM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos markovianos y variables ocultas.\n",
        "\n",
        "Volvamos de nuevo a nuestro ejemplo de oraciones \n",
        "\n",
        "> *Usted tiene 16 años. Está prohibido vender alcohol a menores de 18 años. No puedo venderle esa botella.*\n",
        "\n",
        "Supongamos de nuevo que queremos predecir **18 años**. La cantidad de escritas hasta ese momento es 11. Luego de predecir **18 años**, la cantidad de palabras aumentó a 13. Es decir, conforme predecimos y agregamos información, nuestro modelo debe responder a la cantidad creciente de palabras o ejemplos.\n",
        "\n",
        "Recordemos que todas estas herramientas nacieron de la estadística, por lo que nuestras predicciones se basaran en considerar la probabilidad de que diferentes palabras ocurran en simultaneo. Esto es verdaderamente un problema: mientas más palabras tenemos, menos probable es que vuelvan a ocurrir. Si ocurren infrecuentemente, necesitamos aumentar cada vez más la cantidad de ejemplos de nuestros datos. Esto puede ser un problema incluso para oraciones cortas. Una alternativa para paliar este problema es limitar la cantidad de palabra que miraremos hacia atras. \n",
        "\n",
        "Otra alternativa a esto es trabajar con variables ocultas. Las variables ocultas son cantidades que de alguna manera agrupan la información de todos los casos anteriores. Por ejemplo:\n",
        "\n",
        "> *Usted es menor de edad. No puedo venderle esa botella.*\n",
        "\n",
        "Hemos resumido toda la información de dos oraciones en una sola mucho más corta. \n",
        "\n",
        "De la misma manera que buscamos representaciones abstractas para palabras por medio de *tokens*, usaremos esos tokens para generar nuevas variables que resuman la información anterior. Es decir, generaremos una variable que de alguna manera tiene toda la información de **Usted es menor de edad**\n",
        "\n",
        "Al trabajar con variables ocultas, esperamos reemplazar las todas las palabras anteriroes con el último valor de la variable oculta. Así, nuestro problema que antes veía 13 variables o tokens ahora ve uno solo. Esto nos permite simplificar nuestro modelo para trabajar con **modelos makovianos de primer orden**.\n",
        "\n",
        "Sin entrar en mucho detalle, los modelos markovianos son el tipo de modelos autoregresivos como los que hemos descripto hasta ahora.\n",
        "\n",
        "Esbozo de la noción de modelo de Markov\n",
        "\n",
        "* Tenemos un estado (las últimas palabras escritas) a la cual llegamos a partir de un estado inicial bien definido\n",
        "* Tenemos una historia de estados pasados que afecta  a estados futuros (cada palabra predicha o dentro de nuestro dataset)\n",
        "* Hay una probabilidad asosciada a cada cambio de estado\n",
        "* Queremos predecir cual será el próximo estado de un grupo finito de estados (la próxima palabra). \n",
        "\n",
        "Al trabajar con un modelo markoviano sobre variables ocultas, esperamos que la variable oculta resuma con tanta fidelidad los tokens pasados que solo necesitemos la variable oculta más reciente. Al necesitar solo la más reciente, se dice que es un modelo markoviano de primer orden (requiera solo una variable anterior). La razón por la que buscamos trabajar con modelos de primer orden es que son menos costosos computacionalmente.\n",
        "\n",
        "En resumen nuestra propuesta para generar modelos de lenguaje consistira en lo siguiente:\n",
        "\n",
        "1. Tomaremos texto de para crear nuestro *datset*\n",
        "1. Transformaremos nuestro texto en algun tipo de representación simbólica (*tokens*)\n",
        "2. De esta manera, nuestro modelo de lenguaje se convertirá en un problema de clasificiación: Dadas las palabras anteriores, ¿Cuál es la siguiente palabra? \n",
        "  * Decimos que es un problema de clasificación, porque cada una de nuestra palabras es una categoría.\n",
        "3. Para crear nuestro modelo de lenguaje, usaremos variables ocultas en el contexto de un modelo markoviano.\n",
        "  * La justificación para esto es que el lenguaje tiene caracterísitcas de un modelo markoviano. \n",
        "\n"
      ],
      "metadata": {
        "id": "dBzU3HegcAx3"
      },
      "id": "dBzU3HegcAx3"
    },
    {
      "cell_type": "markdown",
      "id": "55d1c6f5",
      "metadata": {
        "origin_pos": 0,
        "id": "55d1c6f5"
      },
      "source": [
        "# Redes neuronales recurrentes\n",
        "\n",
        "En la sección anterior intentamos argumentar que el lenguaje puede modelarse con un modelo markoviano. Además, propusimos trabajar con modelos markovinos con variables ocultas. La idea de trabajar con variables ocultas era poder trabajar con un modelo markoviano de primer orden. Queremos usar estos modelos de primer orden porque sabemos que nos permitiran ahorrar uso de memoria, así como disminuir el uso de recursos computacionales.\n",
        "\n",
        "Nuestra propuesta para trabajar con variables ocultas, será trabajar con las unidades ocultas de un perceptrón multicapa \n",
        "\n",
        "![](http://d2l.ai/_images/mlp.svg)\n",
        "\n",
        "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W} + \\mathbf{b}$$\n",
        "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W} + \\mathbf{b})$$\n",
        "\n",
        "Sin embargo, como trabajaremos con secuencias temporales, nuestra entrada al tiempo $t$, debe depender del tiempo $t-1$. Es decir, la próxima palabra debe depender de las palabras anteriores. En un perceptrón multicapa, esa dependecia temporal no está presente. Es por esto que debemos reestructurar nuestra capa para que permita generar modelos autoregresivos de secuencias.Dado queremos usar las variables ocultas como cantidades que resumen toda la información anterior, son estas cantidades las que tendran una dependencia temporal\n",
        "\n",
        "$$\\mathbf{O}_{t} = \\mathbf{H}_{t} \\mathbf{W}_{O} + \\mathbf{b}$$\n",
        "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{X} + \\mathbf{H}_{t-1} \\mathbf{W}_{H}  + \\mathbf{b}_h).$$\n",
        "\n",
        "Notemos que $\\mathbf{H}_t$ depende del valor anterior, $\\mathbf{H}_{t-1}$ y del nuevo valor $\\mathbf{X}_t$. Esta dependencia temporal es la que hace que nuestra nueva red neuronal sea una *red neuronal recurrente*. Recordemos si nuestro modelo está correctamente entrenado la salida $\\mathbf{O}_t$ debe coincidir con el resultado correcto o *grounding truth* de $\\mathbf{X}_{t+1}$. Esta era la naturaleza autoregresiva de nuestros modelos.\n",
        "\n",
        "En la siguiente figura mostramos el proceso de cálculo nuestra capa recurrente\n",
        "\n",
        "![An RNN with a hidden state.](http://d2l.ai/_images/rnn.svg)\n",
        "\n",
        "En la figura, vemos que ocurre a cada instante $t$:\n",
        "\n",
        "1. Tenemos una capa densa con función de activación $\\phi$ que toma nuestra matriz de diseño $\\mathbf{X}_t$ y nuestra variable oculta $\\mathbf{H}_{t-1}$.\n",
        "2. A la salida generamos nuestra nueva variable oculta $\\mathbf{H}_t$.\n",
        "3. Con $\\mathbf{H}_t$ y otra capa densa generamos nuestra salida $\\mathbf{O}_t$\n",
        "\n",
        "Muchas veces, en el paso 1 lo que se hace es concatenar $\\mathbf{X}_t$ y $\\mathbf{H}_{t-1}$ para de esa manera definir una unica matriz de pesos. A continuación mostramos como esta concatenación genera el mismo resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b7637a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:55.475202Z",
          "iopub.status.busy": "2022-07-13T08:00:55.474847Z",
          "iopub.status.idle": "2022-07-13T08:00:57.339238Z",
          "shell.execute_reply": "2022-07-13T08:00:57.338221Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "26b7637a"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d87206",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:57.343418Z",
          "iopub.status.busy": "2022-07-13T08:00:57.342726Z",
          "iopub.status.idle": "2022-07-13T08:00:57.375242Z",
          "shell.execute_reply": "2022-07-13T08:00:57.374374Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "id": "e3d87206",
        "outputId": "56509466-2480-454e-ce7c-6e6902ed72dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6495, -0.0479, -1.5496,  3.3693],\n",
              "        [-2.7558, -0.9881, -0.2930,  4.0410],\n",
              "        [ 0.0106,  0.6291,  0.8908,  0.0216]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
        "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
        "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0631962",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:00:57.378759Z",
          "iopub.status.busy": "2022-07-13T08:00:57.378219Z",
          "iopub.status.idle": "2022-07-13T08:00:57.384752Z",
          "shell.execute_reply": "2022-07-13T08:00:57.383963Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "f0631962",
        "outputId": "3df371b6-778a-433b-8c77-4c97e47e73cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6495, -0.0479, -1.5496,  3.3693],\n",
              "        [-2.7558, -0.9881, -0.2930,  4.0410],\n",
              "        [ 0.0106,  0.6291,  0.8908,  0.0216]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problemas con nuestras predicciones "
      ],
      "metadata": {
        "id": "WwIXwbeI_aLs"
      },
      "id": "WwIXwbeI_aLs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un último punto que hemos evitado hasta ahora es el problema de la predicción en secuencias temporales. Dijimos más arriba que la salida $\\mathbf{O}_t$ debe coincidir con el resultado correcto o *grounding truth* de $\\mathbf{X}_{t+1}$. Sin embargo, para entrenar correctamente nuestro modelo debemos cada tanto usar la salida $\\mathbf{O}_t$ en lugar de $\\mathbf{X}_{t+1}$. Si usamos siempre nuestra *grounding truth* en lugar de nuestra predicción, corremos el riesgo de que nuestra red no aprenda a adaptarse a malas predicciones que genera. Al mismo tiempo, nuestra red aprende a modelar las secuencia de entrenamiento, pero puede no saber que hacer con oraciones nuevas.\n",
        "\n",
        "El ejemplo sería como el siguiente:\n",
        "\n",
        "Entrenamos una red con \"El ingenioso hidalgo Don Quijote de la Mancha\". Luego usamos la red para completar:\n",
        "\n",
        "> *En un lugar de la Mancha de cuyo nombre prefiero no ...*\n",
        "\n",
        "La red predice \"recordar\", en lugar de \"acordarme\". En el siguiente paso, si usamos \"acordarme\" la nueva predicción podría ser \"vivía\", pero si usaramos \"recordar\" la predicción podría ser \"Residía\". Es decir, al usar solo la *grounding truth* en lugar de nuestra predicción, el modelo no aprende a corregir sus errores, y se queda \"estancado\" en los datos de entrenamiento. El mayor problema es que cuando tengamos un modelo funcionando, NO HAY *GROUNDING TRUTH*. Esto hace que nuestro modelo pueda fallar estrepitosamente si no usamos nuestra predicción como entrada al modelo. Pero al mismo tiempo, si usamos siempre nuestra predicción, los errores se acumular paulatinamente.\n",
        "\n",
        "Más adelante hablaremos de la técnica llamada **teacher forcing** y como elegir cuando usar la predicción y el *grounding truth* para generar redes que puedan adaptarse a la variabilidad de nuestra predicción."
      ],
      "metadata": {
        "id": "YU3ts24yJIEt"
      },
      "id": "YU3ts24yJIEt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminares a la implementación de RNN\n",
        "\n",
        "Antes de discutir pasar a implementar una RNN desde cero, queremos discutir algunos temás más que serán importantes conocer. \n"
      ],
      "metadata": {
        "id": "c1ek0XFJNua2"
      },
      "id": "c1ek0XFJNua2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Muestro de secuencias.\n",
        "\n",
        "Cuando tenemos que elegir que ejemplos debemos usar de un dataset que no contiene secuencias, simplemente mezclabamos aleatoriamente los ejemplos y luego los usabamos para entrenar nuestras redes. \n",
        "\n",
        "Sin embargo, en secuencias temporales no podemos hacer esto. Si mezclamos aleatriamente podemos terminar generando secuencias sin sentido\n",
        "\n",
        "> *En un lugar de la Mancha de cuyo nombre prefiero no acordarme*\n",
        "\n",
        "luego de mezclarlo\n",
        "\n",
        "> *nombre En de de no lugar prefiero la Mancha cuyo acordarme un*\n",
        "\n",
        "Por esta razón debemos generar paritiones y mezclarlas. Por ejemplo podemos generar particiones de 4 elementos \n",
        "\n",
        "> [*En un lugar de*] [*la Mancha de cuyo*] [*nombre prefiero no acordarme*]\n",
        "\n",
        "> [*la Mancha de cuyo*][*nombre prefiero no acordarme*] [*En un lugar de*]\n",
        "\n",
        "Además de, podemos elegir un offset o deplazamiento. En el ejemplo anterior, un offset de 1 generaría: \n",
        "\n",
        "> *En* [*un lugar de la*] [*Mancha de cuyo nombre*] [*prefiero no acordarme, no*]\n",
        "\n"
      ],
      "metadata": {
        "id": "fg--yzxEYu2T"
      },
      "id": "fg--yzxEYu2T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplejidad\n",
        "\n",
        "La perplejidad es una métrica que es usada en procesamiento de lenguajes naturales para tener una idea de que tan \"convencido\" está  nuestro modelo de la siguiente palabra que adivinará. Como métrica está relacionada a la entropía y la entropía cruzada, por lo que volveremos a usar los ejemplos del juego que discutimos anteriormente."
      ],
      "metadata": {
        "id": "NWqMFr9Oc_8G"
      },
      "id": "NWqMFr9Oc_8G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos nuestro juego:\n",
        "\n",
        "* Materiales:\n",
        "  * una bolsa o recipiente opaco\n",
        "  * 4 pelotas con los números 1, 2, 3, 4\n",
        "* Preparativos:\n",
        "  * Se colocan las pelotas en la bolsa \n",
        "  * El primer jugador saca una de las pelotas de la bolsa\n",
        "* Objetivo general: Adivinar con el menor número de preguntas posibles cuál el número de la pelota que tiene el primer jugador .\n",
        "  * Solo pueden hacerse preguntas que tengan como respuestas sí o no.\n",
        "\n",
        "Recordemos la estrategia optima:\n",
        "\n",
        "```\n",
        "1. Preguntar: \"¿El número es par?\"\n",
        "  A. Si la respuesta es sí, preguntar: \"¿Es el número 4?\"\n",
        "    a. Si la respuesta es sí, sabemos que es el número 4, hemos ganado.\n",
        "    b. Si la respuesta es no, sabemos que es el número 2, hemos ganado.\n",
        "  B. Si la respuesta es no, preguntar: \"¿Es el número 3?\"\n",
        "    a. Si la respuesta es sí, sabemos que es el número 3, hemos ganado.\n",
        "    b. Si la respuesta es no, sabemos que es el número 1, hemos ganado.\n",
        "```\n",
        "\n",
        "Recordemos la entropía de nuestra estrategia:\n",
        "\n",
        "||$1$|$2$|$3$|$4$|total|\n",
        "|---|---|---|---|---|:-:|\n",
        "|probabilidad de ocurrir|$\\dfrac{1}{4}$|$\\dfrac{1}{4}$|$\\dfrac{1}{4}$|$\\dfrac{1}{4}$|-|\n",
        "|número de preguntas|$2$|$2$|$2$|$2$|-|\n",
        "|producto|$\\dfrac{1}{2}$|$\\dfrac{1}{2}$|$\\dfrac{1}{2}$|$\\dfrac{1}{2}$|$2$|\n",
        "\n",
        "Ahora preguntamos, ¿cuantas opciones posibles tenemos en nuestro juego? \n",
        "> Como todas las pelotas son equiprobables, tenemos 4 opciones distintas.\n",
        "\n",
        "La pregunta ahora es que pasará en nuestros segundo juego cuando preguntemos ¿cuantas opciones posibles hay? \n",
        "\n",
        "Segundo juego:\n",
        "\n",
        "* Materiales:\n",
        "  * una bolsa o recipiente opaco\n",
        "  * 8 pelotas con los números 1, 1, 1, 1, 2, 2, 3, 4\n",
        "* Preparativos:\n",
        "  * Se colocan las pelotas en la bolsa \n",
        "  * El primer jugador saca una de las pelotas de la bolsa\n",
        "* Objetivo general: Adivinar con el menor número de preguntas posibles cuál el número de la pelota que tiene el primer jugador .\n",
        "  * Solo pueden hacerse preguntas que tengan como respuestas sí o no.\n",
        "\n",
        "Estrategia optima\n",
        "\n",
        "```\n",
        "1. Preguntar: \"¿Es el número 1?\"\n",
        "  A. Si la respuesta es sí, hemos ganado.\"\n",
        "  B. Si la respuesta es no, preguntar: \"¿Es el número 2?\"\n",
        "    a. Si la respuesta es sí, hemos ganado.\n",
        "    b. Si la respuesta es no, preguntar: \"¿Es el número 3?.\n",
        "      I. Si la respuesta es sí, sabemos que es el número 3, hemos ganado.\n",
        "      I. Si la respuesta es no, sabemos que es el número 4, hemos ganado.\n",
        "```\n",
        "\n",
        "||$1$|$2$|$3$|$4$|total|\n",
        "|---|---|---|---|---|:-:|\n",
        "|probabilidad de ocurrir|$\\dfrac{4}{8}$|$\\dfrac{2}{8}$|$\\dfrac{1}{8}$|$\\dfrac{1}{8}$|-|\n",
        "|número de preguntas|$1$|$2$|$3$|$3$|-|\n",
        "|producto|$\\dfrac{1}{2}$|$\\dfrac{1}{2}$|$\\dfrac{3}{8}$|$\\dfrac{3}{8}$|$1.75$|\n",
        "\n",
        "Dado que la probabilidad de la pelota 1 es mucho mayor que las demás, no tiene sentido decir que tenemos 4 opciones equiprobables. Por el contrario, debemos tener menos de 2 opciones. Esto es porque el 75% de las veces caeremos en la pelota con el número 1 o la pelota con el número 2. La pregunta es, como encontramos esta cantidad de opciones en promedio. \n",
        "\n"
      ],
      "metadata": {
        "id": "MULs23SnG49d"
      },
      "id": "MULs23SnG49d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el primer juego, la cantidad de opciones promedio era 4 y la entropia era 2. Si modificamos el primer juego usando 8 pelotas distintas que puedan ocurrir de manera equiprobable, veíamos que la entropía es 3, y el número de opciones 8. Facilmente podemos ver como sera el resultado final.\n",
        "\n",
        "Si tenemos $n$ eventos equiprobables, la entropía sera:\n",
        "\n",
        "$$P(x) = \\dfrac{1}{n},∀x\\in\\mathbb{Z}~\\land 1\\leqslant x \\leqslant n $$\n",
        "\n",
        "$$H(P(x)) = \\log_2(n)$$\n",
        "\n",
        "Del resultado anterior podemos ver que el número de opciones equiprobables $n$ es igual a $2^H$. Esta cantidad, la llamaremos perplejidad y la usaremos para definir el número de opciones promedio para una distribución probabilistica arbitraria. \n",
        "\n",
        "$$\\text{PPL}(P(x)) = 2^{H(P(X))}$$\n",
        "\n",
        "Para nuestro segun juego tenemos que su perplejidad es de $2^{1.75} ≈3.364$\n",
        "\n",
        "Rápidamente podemos ver que la perplejidad nos permite una interpretación del número. Una perplejidad igual a 1, indica que nuestro modelo está convencido de cual será la siguiente palabra. Mientras que si nuestro modelo está entrenado con 10000 palabras y tiene una perplejidad de 10000, nuestro modelo nunca sabe cual de las palabras poner a continuación.\n",
        "\n",
        "De la misma manera, definimos una perplejidad para una entroía cruzada. Sin embargo, para la entropía cruzada la perplejidad puede dar infinito. Esto sería equivalente a decir que nuestro modelo aprendió algo que  nada tiene que ver con el problema que queríamos resolver. Está tan perdido que no sabe que hacer. Algo parecido a lo que le puede pasar a un hablante de español cuando llega a una país con una lengua que desconoce como puede ser el neerlandés.\n",
        "\n"
      ],
      "metadata": {
        "id": "SB2ebb-Ig1-6"
      },
      "id": "SB2ebb-Ig1-6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementando una RNN desde 0\n",
        "\n",
        "Ahora sí, implmentaremos una RNN desde 0\n",
        "\n"
      ],
      "metadata": {
        "id": "kyPC0nnhVC0e"
      },
      "id": "kyPC0nnhVC0e"
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNScratch(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_inputs = num_inputs\n",
        "        self.W_xh = torch.nn.Parameter(\n",
        "            torch.randn(num_inputs, num_hiddens) * 0.01)\n",
        "        self.W_hh = torch.nn.Parameter(\n",
        "            torch.randn(num_hiddens, num_hiddens) * 0.01)\n",
        "        self.b_h = torch.nn.Parameter(torch.zeros(num_hiddens))\n",
        "\n",
        "    def forward(self, inputs, state=None):\n",
        "        if state is not None:\n",
        "            state, = state\n",
        "        outputs = []\n",
        "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
        "            state = torch.tanh(torch.matmul(X, self.W_xh) + (\n",
        "                torch.matmul(state, self.W_hh) if state is not None else 0)\n",
        "                             + self.b_h)\n",
        "            outputs.append(state)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "guOpu48cXE2W"
      },
      "id": "guOpu48cXE2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos que ocurre al generar una RNN y luego alimentarla con un tensor arbitrario"
      ],
      "metadata": {
        "id": "i4lgiUB8hGmo"
      },
      "id": "i4lgiUB8hGmo"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 secuencias de longitud 3 de vectores de 4 componentes\n",
        "A = torch.randn(5, 3, 4)\n",
        "print(A[0]) # una secuencia de 3 vectores de 4 componentes\n",
        "\n",
        "# red recurrente que toma vectores de 4 componentes y entrega vectores de 2\n",
        "rnn =  RNNScratch(4, 2)\n",
        "\n",
        "O, H = rnn(A)\n",
        "\n",
        "print(len(O), O[0].shape) # 5 secuencias de longitud 3 de vectores de 2 componentes\n",
        "print(H) # 3 vectores de 2 componentes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xawJJ6Jbebhp",
        "outputId": "75ffec6e-66d7-42f1-f477-877818fecb22"
      },
      "id": "xawJJ6Jbebhp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2819, -0.2242,  1.5291, -1.4235],\n",
            "        [ 1.3052,  0.0299,  1.3173,  0.6242],\n",
            "        [ 0.8394,  0.3529, -0.3716,  1.9146]])\n",
            "5 torch.Size([3, 2])\n",
            "tensor([[-0.0339,  0.0065],\n",
            "        [ 0.0002, -0.0324],\n",
            "        [-0.0014, -0.0100]], grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay que considerar que hasta ahora solo hemos creado la variable oculta $\\mathbf{H}$, no hemos aplicado la capa densa final que debemos usar para predecir la próxima palabra."
      ],
      "metadata": {
        "id": "QYjlwPcEjU3s"
      },
      "id": "QYjlwPcEjU3s"
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLMScratch(torch.nn.Module):\n",
        "    \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "    def __init__(self, rnn, vocab_size):\n",
        "        super().__init__()\n",
        "        self.rnn = rnn\n",
        "        self.vocab_size = vocab_size\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        self.W_hq = torch.nn.Parameter(\n",
        "            torch.randn(\n",
        "                self.rnn.num_hiddens, self.vocab_size) * 0.01)\n",
        "        self.b_q = torch.nn.Parameter(torch.zeros(self.vocab_size))\n",
        "\n",
        "    def one_hot(self, X):\n",
        "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "        # Output shape: (num_steps, batch_size, vocab_size)\n",
        "        return  torch.nn.functional.one_hot(X.T,\n",
        "                                            self.vocab_size).type(torch.float32)\n",
        "\n",
        "    def output_layer(self, rnn_outputs):\n",
        "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
        "        return torch.stack(outputs, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, X, state=None):\n",
        "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "        embs = self.one_hot(X)\n",
        "        rnn_outputs, _ = self.rnn(embs, state)\n",
        "        return self.output_layer(rnn_outputs)\n",
        "\n",
        "    def predict(self, prefix, num_preds, vocab, device=None):\n",
        "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "        state, outputs = None, [vocab[prefix[0]]]\n",
        "        for i in range(len(prefix) + num_preds - 1):\n",
        "            X = torch.tensor([[outputs[-1]]], device=device)\n",
        "            embs = self.one_hot(X)\n",
        "            rnn_outputs, state = self.rnn(embs, state)\n",
        "            #if type(state) is tuple: state = state[0]\n",
        "            if i < len(prefix) - 1:  # Warm-up period\n",
        "                outputs.append(vocab[prefix[i + 1]])\n",
        "            else:  # Predict `num_preds` steps\n",
        "                Y = self.output_layer(rnn_outputs)\n",
        "                outputs.append(int(torch.reshape(torch.argmax(Y, axis=2),\n",
        "                                                 (1,))))\n",
        "        return ''.join([vocab.get_itos()[i] for i in outputs])"
      ],
      "metadata": {
        "id": "ui8QRHNluev3"
      },
      "id": "ui8QRHNluev3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También aquí podemos revisar con que entramos a nuestra red y con que salimos."
      ],
      "metadata": {
        "id": "8pjM0nkldFqA"
      },
      "id": "8pjM0nkldFqA"
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLMScratch(rnn, 4)\n",
        "outputs = model(torch.ones((3, 5), dtype=torch.int64))\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQzJS3008MQl",
        "outputId": "8fc540c2-38a1-44d4-ed87-873aca55eb0a"
      },
      "id": "CQzJS3008MQl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation en el tiempo\n",
        "\n",
        "Ahora bien, esta implmentación nos permite señalar el mayor problema que tienen la RNN. Para esto, consideremos el gradiente de la función de pérdida como si solo tuvieramos la variable oculta a la salida.\n",
        "\n",
        "Como analizamos varias salidas a diferentes instantes, debemos pesar sobre cada uno de nuestros instantes. Esto define una función de pérdida promediada en el tiempo:\n",
        "\n",
        "$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n",
        "\n",
        "Apliquemos a continuación los correspondientes gradientes:\n",
        "\n",
        "$$\\begin{aligned}\\frac{\\partial L}{\\partial w_h}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_h}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_h}.\\end{aligned}$$\n",
        "\n",
        "En donde hemos usado la definición de las cariables $h_t$ y $o_t$\n",
        "\n",
        "$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\\\o_t &= g(h_t, w_o),\\end{aligned}$$\n",
        "\n",
        "El problema de las redes recurrentes justamente lo tenemos en el tercer factor. Si calculamos la derivada parcial encontramos que $h_t$ depende de $h_{t-1}$, que a su vez depende de $h_{t-2}$...\n",
        "\n",
        "$$\\frac{\\partial h_t}{\\partial w_h}= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
        "\n",
        "Estamos en un problema de recurrencia. Si ha trabajado con este tipo de problemas, sabe que estos pueden dar lugar a recurrencias poco costosas (el cálculo del factorial de $t!$ depende de unas $t$ operaciones) o ridículamente costosas (el cálculo del $t$-ésimo número de Fibonacci depende de alrededor $2^t$ operaciones).\n",
        "\n",
        "Puede demostrarse que la expresión anterior es análoga al algoritmo de cálculo de un polinomio como los implementados en programas como Mathematica o Matlab o bibliotecas como `numpy`. Se sabe que este cálculo sólo depende de $t$ operaciones... a condicion de que hayamos cuardado todos los gradientes anteriores. Esto último es en realidad nuestro mayor problema: nuestro gradiente a cada instante $t$ es una matriz en el mejor de los casos, con lo cual necesitaremos guardar $t$ matrices para el tiempo $t$, $t+1$ para el tiempo $t+1$... O en su defecto volver a calcular cada gradiente desde cero.\n",
        "\n",
        "Este problema en donde necesitamos memoria de trabajo infinita o tener que volver a calcular multiples matrices es un problema de alguna manera insalvable para el cálculo del gradiente. Por este motivo, las soluciones consisten es hacer una aproximación. Las aproximaciones son dos:\n",
        "\n",
        "* Detener el cálculo del gradiente más alla de un número fijo de pasos. Es decir, decidir que le cálculo del gradiente recursivo se detendra luego de 10 pasos hacia atras. 10 es un número arbitrario que elegimos segun conveniencia\n",
        "* Detener el cálculo de manera aleatoria. Antes de calcular cada paso hacia atras, una distribución probabilistica no dice si debemos tenernos o no. Además, debemos hacer algunas correciones para que el valor esperado de nuestro gradiente calculado de manera aleatoria, coincida con el valor real. \n",
        "\n",
        "Se ha visto que estas dos soluciones producen resultados similares y que ninguno consituye una gran mejora respecto al otro."
      ],
      "metadata": {
        "id": "JHskds_Rj0gH"
      },
      "id": "JHskds_Rj0gH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Gradient clipping*\n",
        "\n",
        "Dijimos anteriormente que la relación de recurrencia anterior equivale al cálculo de una polinomio de grado $t$. Pues bien, eso significa que para el tiempo $t$ tenemos un término que depende aproximadamente de la $t$-ésima potencia del gradiente de las variables ocultas. Dado que este gradiente es una matriz, estamos hablando una matriz elevada a la potencia $t$. El resultado es que los valores de nuestra matriz pueden crecer demasiado. Es por esto que para evitar un crecimiento descontrolado de nuestro gradiente, se realiza lo que se conoce como *gradient clipping*. Es decir, cuando el gradiente es mayor a cierta cantidad se procede a entregar un valor fijo de gradiente por encima del umbral definido. En efecto, lo que se hace es calcular el gradiente de la siguiente manera:\n",
        "\n",
        "$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.$$\n",
        "\n",
        "De esta manera el modulo del gradiente nunca supera la cantidad $\\theta$"
      ],
      "metadata": {
        "id": "N8NmgswOeFrK"
      },
      "id": "N8NmgswOeFrK"
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients(grad_clip_val, model):\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "        if norm > grad_clip_val:\n",
        "            for param in params:\n",
        "                param.grad[:] *= grad_clip_val / norm"
      ],
      "metadata": {
        "id": "vs7OOx1sDYiX"
      },
      "id": "vs7OOx1sDYiX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "JdgiTtWfD-zM"
      },
      "id": "JdgiTtWfD-zM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos un momento como es nuestro pipeline:\n",
        "\n",
        "1. Carga de los datos\n",
        "1. Separación de los datos en lotes\n",
        "1. Inicialización de parámetros \n",
        "1. Definición del modelo\n",
        "1. Definición de la función de pérdida\n",
        "1. Definición del algoritmo de optimización\n",
        "\n",
        "En líneas generales hemos presentado todos los pasos de nuestro pipeline, pero debemos tener cuidado con el proceso de tokenización, como se explicó anteriormente. Por eso presentaremos algunas herramientas muy sencillas de tokenización.\n",
        "\n",
        "Nuestra tarea, en este caso, sera tratar de enseñarle a nuestra red a escribir correctamente en español letra por letra. Para esto hemos elegido \"El ingenioso hidalgo Don Quijote de la Mancha\" como texto de referencia. Usaramos la letras del mismo texto para enseñarle a nuestro modelo a escribir palabras en español. Para eso tokenizaremos las letras del español."
      ],
      "metadata": {
        "id": "PUxE3FQ0EiIH"
      },
      "id": "PUxE3FQ0EiIH"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import collections\n",
        "import torchtext\n",
        "\n",
        "def make_vocab(fn, skip=0):\n",
        "  data = None\n",
        "  with open(fn, \"r\") as f: \n",
        "    f.seek(skip)\n",
        "    data = f.read()\n",
        "  if data == None:\n",
        "    return None, None\n",
        "    # \"..\" match \" \"\n",
        "    # \".;\" match \" \"\n",
        "    # \".a\" match \" a\"\n",
        "    # \" . \" match \".\" reemplaza \"   \"\n",
        "    # \" .. \" match \"..\" reemplaza \"   \"\n",
        "    # guía no machea nada \"guía\"\n",
        "  tokens = re.sub('[^A-Za-záéíóúÁÉÍÓÚñÑüÜ]+', ' ', data).lower()\n",
        "  tokens = [token for line in list(tokens) for token in line]\n",
        "  counter = collections.Counter(tokens)\n",
        "  freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  ordered_dict = collections.OrderedDict(freq_tuples)\n",
        "  result = torchtext.vocab.vocab(ordered_dict)\n",
        "  corpus = [result[token] for token in tokens]\n",
        "  return result, ordered_dict, corpus\n",
        "\n",
        "!wget https://www.gutenberg.org/files/2000/2000-0.txt\n",
        "vocab, dictionary, corpus = make_vocab(\"2000-0.txt\", 41508)"
      ],
      "metadata": {
        "id": "3-jrPipb3AP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ba5d4b-e414-49b2-ebde-ac6163866c49"
      },
      "id": "3-jrPipb3AP_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-17 15:21:45--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2226045 (2.1M) [text/plain]\n",
            "Saving to: ‘2000-0.txt.1’\n",
            "\n",
            "\r2000-0.txt.1          0%[                    ]       0  --.-KB/s               \r2000-0.txt.1        100%[===================>]   2.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-17 15:21:45 (19.6 MB/s) - ‘2000-0.txt.1’ saved [2226045/2226045]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "num_steps = 32\n",
        "array = torch.tensor([corpus[i:i+num_steps+1]\n",
        "                            for i in range(0, len(corpus)-num_steps-1)])\n",
        "# qwert y (i = 0)\n",
        "# q werty (i = 1)\n",
        "features, tags = array[:,:-1], array[:,1:]\n",
        "\n",
        "num_train = 20480 \n",
        "num_val = 5120 \n",
        "def get_tensorloader(tensors, train, indices=slice(0, None)):\n",
        "    tensors = tuple(a[indices] for a in tensors)\n",
        "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size,\n",
        "                                        shuffle=train)\n",
        "\n",
        "train_iter = get_tensorloader([features, tags], True, indices=slice(0, num_train))\n",
        "test_iter = get_tensorloader([features, tags], False,\n",
        "                             indices=slice(num_train, num_train + num_val))\n"
      ],
      "metadata": {
        "id": "pZta_lYcq8Gl"
      },
      "id": "pZta_lYcq8Gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab[\" \"])\n",
        "print(vocab[\"e\"])\n",
        "print(vocab[\"a\"])\n",
        "print(vocab[\"á\"])\n",
        "print(vocab[\"ñ\"])\n",
        "print(type(vocab.get_itos()) is list)\n",
        "print(vocab.get_itos())\n",
        "print(vocab.get_itos()[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH01WOr3rwfc",
        "outputId": "7f01d4f5-2174-4f03-d5ec-04c0400e89f9"
      },
      "id": "QH01WOr3rwfc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "26\n",
            "28\n",
            "True\n",
            "[' ', 'e', 'a', 'o', 's', 'n', 'r', 'l', 'd', 'u', 'i', 't', 'c', 'm', 'p', 'q', 'y', 'b', 'h', 'v', 'g', 'í', 'j', 'ó', 'f', 'é', 'á', 'z', 'ñ', 'ú', 'x', 'w', 'k', 'ü']\n",
            "u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haremos unas pequeña redifinición a nuestra función de pérdida, dado que estamos tamos trabajando con tensores con 3 dimensiones"
      ],
      "metadata": {
        "id": "MZMoLXGHdt_v"
      },
      "id": "MZMoLXGHdt_v"
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_NLP(Y_hat, Y):\n",
        "    Y_hat = torch.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
        "    Y = torch.reshape(Y, (-1,))\n",
        "    return torch.nn.functional.cross_entropy(\n",
        "        Y_hat, Y, reduction='none')"
      ],
      "metadata": {
        "id": "BOPS_wFNWhmZ"
      },
      "id": "BOPS_wFNWhmZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "BuXxr9ICeJ62"
      },
      "id": "BuXxr9ICeJ62"
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_scrt =  RNNScratch(num_inputs=len(vocab), num_hiddens=32)\n",
        "net1 = RNNLMScratch(rnn_scrt, vocab_size=len(vocab))\n",
        "loss = loss_NLP\n",
        "trainer = torch.optim.Adadelta(net1.parameters(), lr=1)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net1(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net1)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpdS7wokJTU6",
        "outputId": "1baec75b-510d-4096-8d5f-967fec46eabf"
      },
      "id": "fpdS7wokJTU6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 3.480818,\n",
            "    train perplexity 32.486294,\n",
            "    test perplexity 29.455894.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.951564,\n",
            "    train perplexity 19.135860,\n",
            "    test perplexity 17.336414.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.847047,\n",
            "    train perplexity 17.236809,\n",
            "    test perplexity 17.144173.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.831578,\n",
            "    train perplexity 16.972218,\n",
            "    test perplexity 16.727331.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.797822,\n",
            "    train perplexity 16.408869,\n",
            "    test perplexity 15.961906.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.739798,\n",
            "    train perplexity 15.483862,\n",
            "    test perplexity 14.872507.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.641134,\n",
            "    train perplexity 14.029107,\n",
            "    test perplexity 13.375977.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.551752,\n",
            "    train perplexity 12.829556,\n",
            "    test perplexity 12.329431.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.480552,\n",
            "    train perplexity 11.947857,\n",
            "    test perplexity 11.563040.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.421227,\n",
            "    train perplexity 11.259661,\n",
            "    test perplexity 11.026072.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.378724,\n",
            "    train perplexity 10.791121,\n",
            "    test perplexity 10.488839.\n",
            "\n",
            "epoch 12\n",
            "    loss 2.346554,\n",
            "    train perplexity 10.449499,\n",
            "    test perplexity 10.308448.\n",
            "\n",
            "epoch 13\n",
            "    loss 2.320248,\n",
            "    train perplexity 10.178200,\n",
            "    test perplexity 10.074756.\n",
            "\n",
            "epoch 14\n",
            "    loss 2.296426,\n",
            "    train perplexity 9.938594,\n",
            "    test perplexity 9.842315.\n",
            "\n",
            "epoch 15\n",
            "    loss 2.271938,\n",
            "    train perplexity 9.698176,\n",
            "    test perplexity 9.692072.\n",
            "\n",
            "epoch 16\n",
            "    loss 2.252590,\n",
            "    train perplexity 9.512338,\n",
            "    test perplexity 9.357094.\n",
            "\n",
            "epoch 17\n",
            "    loss 2.231834,\n",
            "    train perplexity 9.316937,\n",
            "    test perplexity 9.314969.\n",
            "\n",
            "epoch 18\n",
            "    loss 2.214262,\n",
            "    train perplexity 9.154649,\n",
            "    test perplexity 9.161817.\n",
            "\n",
            "epoch 19\n",
            "    loss 2.199221,\n",
            "    train perplexity 9.017987,\n",
            "    test perplexity 9.035725.\n",
            "\n",
            "epoch 20\n",
            "    loss 2.185737,\n",
            "    train perplexity 8.897201,\n",
            "    test perplexity 8.950548.\n",
            "\n",
            "epoch 21\n",
            "    loss 2.173068,\n",
            "    train perplexity 8.785192,\n",
            "    test perplexity 8.701049.\n",
            "\n",
            "epoch 22\n",
            "    loss 2.160904,\n",
            "    train perplexity 8.678982,\n",
            "    test perplexity 8.586212.\n",
            "\n",
            "epoch 23\n",
            "    loss 2.149411,\n",
            "    train perplexity 8.579803,\n",
            "    test perplexity 8.576083.\n",
            "\n",
            "epoch 24\n",
            "    loss 2.138429,\n",
            "    train perplexity 8.486097,\n",
            "    test perplexity 8.453168.\n",
            "\n",
            "epoch 25\n",
            "    loss 2.126897,\n",
            "    train perplexity 8.388799,\n",
            "    test perplexity 8.371127.\n",
            "\n",
            "epoch 26\n",
            "    loss 2.115412,\n",
            "    train perplexity 8.293000,\n",
            "    test perplexity 8.258894.\n",
            "\n",
            "epoch 27\n",
            "    loss 2.105083,\n",
            "    train perplexity 8.207787,\n",
            "    test perplexity 8.151181.\n",
            "\n",
            "epoch 28\n",
            "    loss 2.093550,\n",
            "    train perplexity 8.113669,\n",
            "    test perplexity 8.066223.\n",
            "\n",
            "epoch 29\n",
            "    loss 2.082129,\n",
            "    train perplexity 8.021526,\n",
            "    test perplexity 8.035448.\n",
            "\n",
            "epoch 30\n",
            "    loss 2.072023,\n",
            "    train perplexity 7.940873,\n",
            "    test perplexity 7.999517.\n",
            "\n",
            "epoch 31\n",
            "    loss 2.061280,\n",
            "    train perplexity 7.856019,\n",
            "    test perplexity 7.787971.\n",
            "\n",
            "epoch 32\n",
            "    loss 2.052146,\n",
            "    train perplexity 7.784592,\n",
            "    test perplexity 7.767421.\n",
            "\n",
            "epoch 33\n",
            "    loss 2.042401,\n",
            "    train perplexity 7.709094,\n",
            "    test perplexity 7.723537.\n",
            "\n",
            "epoch 34\n",
            "    loss 2.032445,\n",
            "    train perplexity 7.632723,\n",
            "    test perplexity 7.595438.\n",
            "\n",
            "epoch 35\n",
            "    loss 2.022107,\n",
            "    train perplexity 7.554222,\n",
            "    test perplexity 7.579494.\n",
            "\n",
            "epoch 36\n",
            "    loss 2.013632,\n",
            "    train perplexity 7.490474,\n",
            "    test perplexity 7.501102.\n",
            "\n",
            "epoch 37\n",
            "    loss 2.005230,\n",
            "    train perplexity 7.427802,\n",
            "    test perplexity 7.442682.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.997736,\n",
            "    train perplexity 7.372344,\n",
            "    test perplexity 7.319477.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.987918,\n",
            "    train perplexity 7.300315,\n",
            "    test perplexity 7.355786.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.980890,\n",
            "    train perplexity 7.249194,\n",
            "    test perplexity 7.207972.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.972494,\n",
            "    train perplexity 7.188582,\n",
            "    test perplexity 7.217495.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.966508,\n",
            "    train perplexity 7.145682,\n",
            "    test perplexity 7.176851.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.958232,\n",
            "    train perplexity 7.086784,\n",
            "    test perplexity 7.046203.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.951868,\n",
            "    train perplexity 7.041830,\n",
            "    test perplexity 7.016865.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.944957,\n",
            "    train perplexity 6.993335,\n",
            "    test perplexity 6.969848.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.938995,\n",
            "    train perplexity 6.951759,\n",
            "    test perplexity 6.926400.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.932223,\n",
            "    train perplexity 6.904840,\n",
            "    test perplexity 6.865941.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.926523,\n",
            "    train perplexity 6.865599,\n",
            "    test perplexity 6.929660.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.920139,\n",
            "    train perplexity 6.821907,\n",
            "    test perplexity 6.763570.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.915954,\n",
            "    train perplexity 6.793416,\n",
            "    test perplexity 6.801374.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.908654,\n",
            "    train perplexity 6.744003,\n",
            "    test perplexity 6.737766.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.903277,\n",
            "    train perplexity 6.707841,\n",
            "    test perplexity 6.651716.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.898571,\n",
            "    train perplexity 6.676345,\n",
            "    test perplexity 6.728743.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.891650,\n",
            "    train perplexity 6.630299,\n",
            "    test perplexity 6.616040.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.887367,\n",
            "    train perplexity 6.601963,\n",
            "    test perplexity 6.624705.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.882594,\n",
            "    train perplexity 6.570530,\n",
            "    test perplexity 6.526902.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.877101,\n",
            "    train perplexity 6.534534,\n",
            "    test perplexity 6.581092.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.872990,\n",
            "    train perplexity 6.507726,\n",
            "    test perplexity 6.523653.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.867380,\n",
            "    train perplexity 6.471323,\n",
            "    test perplexity 6.485111.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.862895,\n",
            "    train perplexity 6.442362,\n",
            "    test perplexity 6.468328.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.858651,\n",
            "    train perplexity 6.415076,\n",
            "    test perplexity 6.415345.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.854331,\n",
            "    train perplexity 6.387422,\n",
            "    test perplexity 6.455813.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.850783,\n",
            "    train perplexity 6.364799,\n",
            "    test perplexity 6.459053.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.846222,\n",
            "    train perplexity 6.335839,\n",
            "    test perplexity 6.337829.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.842717,\n",
            "    train perplexity 6.313666,\n",
            "    test perplexity 6.266517.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.839074,\n",
            "    train perplexity 6.290713,\n",
            "    test perplexity 6.313444.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.835765,\n",
            "    train perplexity 6.269930,\n",
            "    test perplexity 6.223323.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.831062,\n",
            "    train perplexity 6.240508,\n",
            "    test perplexity 6.184623.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.828182,\n",
            "    train perplexity 6.222567,\n",
            "    test perplexity 6.231843.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.824513,\n",
            "    train perplexity 6.199774,\n",
            "    test perplexity 6.152614.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.820000,\n",
            "    train perplexity 6.171860,\n",
            "    test perplexity 6.155549.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.818912,\n",
            "    train perplexity 6.165149,\n",
            "    test perplexity 6.154429.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.813793,\n",
            "    train perplexity 6.133668,\n",
            "    test perplexity 6.053696.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.811762,\n",
            "    train perplexity 6.121226,\n",
            "    test perplexity 6.123415.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.808674,\n",
            "    train perplexity 6.102353,\n",
            "    test perplexity 6.056356.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.805377,\n",
            "    train perplexity 6.082265,\n",
            "    test perplexity 6.084270.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.802813,\n",
            "    train perplexity 6.066688,\n",
            "    test perplexity 6.023031.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.800999,\n",
            "    train perplexity 6.055694,\n",
            "    test perplexity 5.961385.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.797676,\n",
            "    train perplexity 6.035607,\n",
            "    test perplexity 5.946260.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.796221,\n",
            "    train perplexity 6.026828,\n",
            "    test perplexity 5.962510.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.790795,\n",
            "    train perplexity 5.994215,\n",
            "    test perplexity 5.937006.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.789446,\n",
            "    train perplexity 5.986135,\n",
            "    test perplexity 5.993428.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.788205,\n",
            "    train perplexity 5.978712,\n",
            "    test perplexity 6.023189.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.785220,\n",
            "    train perplexity 5.960890,\n",
            "    test perplexity 5.949997.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.782430,\n",
            "    train perplexity 5.944286,\n",
            "    test perplexity 5.956501.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.781101,\n",
            "    train perplexity 5.936390,\n",
            "    test perplexity 6.007568.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.778648,\n",
            "    train perplexity 5.921845,\n",
            "    test perplexity 5.960442.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.776336,\n",
            "    train perplexity 5.908167,\n",
            "    test perplexity 5.898792.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.773830,\n",
            "    train perplexity 5.893384,\n",
            "    test perplexity 5.831850.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.772421,\n",
            "    train perplexity 5.885085,\n",
            "    test perplexity 5.796104.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.770617,\n",
            "    train perplexity 5.874474,\n",
            "    test perplexity 5.886501.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.768872,\n",
            "    train perplexity 5.864234,\n",
            "    test perplexity 5.876736.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.764835,\n",
            "    train perplexity 5.840609,\n",
            "    test perplexity 5.849633.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.763408,\n",
            "    train perplexity 5.832280,\n",
            "    test perplexity 5.817215.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.761775,\n",
            "    train perplexity 5.822762,\n",
            "    test perplexity 5.843841.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.760988,\n",
            "    train perplexity 5.818186,\n",
            "    test perplexity 5.838379.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.756883,\n",
            "    train perplexity 5.794350,\n",
            "    test perplexity 5.792860.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.755038,\n",
            "    train perplexity 5.783666,\n",
            "    test perplexity 5.783478.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.753411,\n",
            "    train perplexity 5.774267,\n",
            "    test perplexity 5.775193.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.752843,\n",
            "    train perplexity 5.770987,\n",
            "    test perplexity 5.768532.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el resultado final."
      ],
      "metadata": {
        "id": "-KuMNUXneaVj"
      },
      "id": "-KuMNUXneaVj"
    },
    {
      "cell_type": "code",
      "source": [
        "net1.predict(\"quijote y sancho \", 40, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hhVqTonURjlN",
        "outputId": "f7b51c63-63c7-4002-ea64-874dd708bc6c"
      },
      "id": "hhVqTonURjlN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quijote y sancho de la mantro de la mante de la mante de '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación concisa de RNN\n",
        "\n",
        "En función a los dos problemas asociados a Backpropagation en el tiempo y al crecieminto descontrolado de los gradientes, vemos que es preferible usar las herramientas que ya trae `torch`. Para llamar a una RNN que entrega estados ocultos deberemos hacer lo que se muestra en el siguiente código"
      ],
      "metadata": {
        "id": "pnIU4Ow8tWgF"
      },
      "id": "pnIU4Ow8tWgF"
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.RNN(num_inputs, num_hiddens)\n",
        "\n",
        "    def forward(self, inputs, H=None):\n",
        "        return self.rnn(inputs, H)"
      ],
      "metadata": {
        "id": "2ab1SpWuaJRm"
      },
      "id": "2ab1SpWuaJRm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLM(RNNLMScratch):\n",
        "    def init_params(self):\n",
        "        self.linear = torch.nn.LazyLinear(self.vocab_size)\n",
        "    def output_layer(self, hiddens):\n",
        "        return self.linear(hiddens).swapaxes(0, 1)"
      ],
      "metadata": {
        "id": "n4QXkbezaJDv"
      },
      "id": "n4QXkbezaJDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn =  RNN(num_inputs=len(vocab), num_hiddens=32)\n",
        "net2 = RNNLM(rnn, vocab_size=len(vocab))\n",
        "loss = loss_NLP\n",
        "trainer = torch.optim.Adadelta(net2.parameters(), lr=1)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net2(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net2)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Egmoe2JZtKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b1b5b3-ffc9-4d2e-c5d0-1395a7115a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 3.119849,\n",
            "    train perplexity 22.642954,\n",
            "    test perplexity 17.419270.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.822018,\n",
            "    train perplexity 16.810736,\n",
            "    test perplexity 16.244394.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.732061,\n",
            "    train perplexity 15.364519,\n",
            "    test perplexity 14.569115.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.607428,\n",
            "    train perplexity 13.564120,\n",
            "    test perplexity 12.738925.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.500514,\n",
            "    train perplexity 12.188758,\n",
            "    test perplexity 11.661670.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.417577,\n",
            "    train perplexity 11.218647,\n",
            "    test perplexity 10.803309.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.353830,\n",
            "    train perplexity 10.525803,\n",
            "    test perplexity 10.169787.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.307568,\n",
            "    train perplexity 10.049956,\n",
            "    test perplexity 9.867966.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.270337,\n",
            "    train perplexity 9.682662,\n",
            "    test perplexity 9.505201.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.241402,\n",
            "    train perplexity 9.406511,\n",
            "    test perplexity 9.178358.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.213364,\n",
            "    train perplexity 9.146437,\n",
            "    test perplexity 8.978465.\n",
            "\n",
            "epoch 12\n",
            "    loss 2.191013,\n",
            "    train perplexity 8.944268,\n",
            "    test perplexity 8.796933.\n",
            "\n",
            "epoch 13\n",
            "    loss 2.168118,\n",
            "    train perplexity 8.741813,\n",
            "    test perplexity 8.574752.\n",
            "\n",
            "epoch 14\n",
            "    loss 2.147782,\n",
            "    train perplexity 8.565837,\n",
            "    test perplexity 8.435443.\n",
            "\n",
            "epoch 15\n",
            "    loss 2.130213,\n",
            "    train perplexity 8.416661,\n",
            "    test perplexity 8.415665.\n",
            "\n",
            "epoch 16\n",
            "    loss 2.114705,\n",
            "    train perplexity 8.287144,\n",
            "    test perplexity 8.342307.\n",
            "\n",
            "epoch 17\n",
            "    loss 2.098016,\n",
            "    train perplexity 8.149986,\n",
            "    test perplexity 8.240290.\n",
            "\n",
            "epoch 18\n",
            "    loss 2.083313,\n",
            "    train perplexity 8.031034,\n",
            "    test perplexity 8.007524.\n",
            "\n",
            "epoch 19\n",
            "    loss 2.071470,\n",
            "    train perplexity 7.936478,\n",
            "    test perplexity 7.928141.\n",
            "\n",
            "epoch 20\n",
            "    loss 2.058984,\n",
            "    train perplexity 7.838003,\n",
            "    test perplexity 7.728973.\n",
            "\n",
            "epoch 21\n",
            "    loss 2.047006,\n",
            "    train perplexity 7.744678,\n",
            "    test perplexity 7.685264.\n",
            "\n",
            "epoch 22\n",
            "    loss 2.035532,\n",
            "    train perplexity 7.656321,\n",
            "    test perplexity 7.531879.\n",
            "\n",
            "epoch 23\n",
            "    loss 2.026094,\n",
            "    train perplexity 7.584407,\n",
            "    test perplexity 7.514210.\n",
            "\n",
            "epoch 24\n",
            "    loss 2.015894,\n",
            "    train perplexity 7.507437,\n",
            "    test perplexity 7.457515.\n",
            "\n",
            "epoch 25\n",
            "    loss 2.006838,\n",
            "    train perplexity 7.439756,\n",
            "    test perplexity 7.462024.\n",
            "\n",
            "epoch 26\n",
            "    loss 1.995989,\n",
            "    train perplexity 7.359481,\n",
            "    test perplexity 7.288376.\n",
            "\n",
            "epoch 27\n",
            "    loss 1.988135,\n",
            "    train perplexity 7.301904,\n",
            "    test perplexity 7.382486.\n",
            "\n",
            "epoch 28\n",
            "    loss 1.979613,\n",
            "    train perplexity 7.239938,\n",
            "    test perplexity 7.177062.\n",
            "\n",
            "epoch 29\n",
            "    loss 1.970930,\n",
            "    train perplexity 7.177349,\n",
            "    test perplexity 7.128538.\n",
            "\n",
            "epoch 30\n",
            "    loss 1.963732,\n",
            "    train perplexity 7.125870,\n",
            "    test perplexity 7.136059.\n",
            "\n",
            "epoch 31\n",
            "    loss 1.956461,\n",
            "    train perplexity 7.074249,\n",
            "    test perplexity 7.054631.\n",
            "\n",
            "epoch 32\n",
            "    loss 1.948661,\n",
            "    train perplexity 7.019283,\n",
            "    test perplexity 7.037535.\n",
            "\n",
            "epoch 33\n",
            "    loss 1.942880,\n",
            "    train perplexity 6.978824,\n",
            "    test perplexity 6.989161.\n",
            "\n",
            "epoch 34\n",
            "    loss 1.936690,\n",
            "    train perplexity 6.935755,\n",
            "    test perplexity 6.873803.\n",
            "\n",
            "epoch 35\n",
            "    loss 1.930042,\n",
            "    train perplexity 6.889797,\n",
            "    test perplexity 6.881473.\n",
            "\n",
            "epoch 36\n",
            "    loss 1.922120,\n",
            "    train perplexity 6.835433,\n",
            "    test perplexity 6.729598.\n",
            "\n",
            "epoch 37\n",
            "    loss 1.918500,\n",
            "    train perplexity 6.810737,\n",
            "    test perplexity 6.711738.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.912181,\n",
            "    train perplexity 6.767835,\n",
            "    test perplexity 6.746334.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.906564,\n",
            "    train perplexity 6.729928,\n",
            "    test perplexity 6.728991.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.900463,\n",
            "    train perplexity 6.688987,\n",
            "    test perplexity 6.726226.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.897395,\n",
            "    train perplexity 6.668503,\n",
            "    test perplexity 6.682948.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.890992,\n",
            "    train perplexity 6.625941,\n",
            "    test perplexity 6.623825.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.886029,\n",
            "    train perplexity 6.593133,\n",
            "    test perplexity 6.640592.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.881287,\n",
            "    train perplexity 6.561942,\n",
            "    test perplexity 6.585428.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.878536,\n",
            "    train perplexity 6.543919,\n",
            "    test perplexity 6.545526.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.872171,\n",
            "    train perplexity 6.502398,\n",
            "    test perplexity 6.462782.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.866735,\n",
            "    train perplexity 6.467146,\n",
            "    test perplexity 6.459181.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.863874,\n",
            "    train perplexity 6.448669,\n",
            "    test perplexity 6.403277.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.858926,\n",
            "    train perplexity 6.416842,\n",
            "    test perplexity 6.481544.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.857085,\n",
            "    train perplexity 6.405038,\n",
            "    test perplexity 6.382272.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.850686,\n",
            "    train perplexity 6.364186,\n",
            "    test perplexity 6.349669.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.847893,\n",
            "    train perplexity 6.346430,\n",
            "    test perplexity 6.293534.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.842183,\n",
            "    train perplexity 6.310298,\n",
            "    test perplexity 6.313238.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.840616,\n",
            "    train perplexity 6.300415,\n",
            "    test perplexity 6.290326.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.835713,\n",
            "    train perplexity 6.269601,\n",
            "    test perplexity 6.216546.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.832383,\n",
            "    train perplexity 6.248762,\n",
            "    test perplexity 6.291185.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.829542,\n",
            "    train perplexity 6.231032,\n",
            "    test perplexity 6.149607.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.825759,\n",
            "    train perplexity 6.207506,\n",
            "    test perplexity 6.191162.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.824516,\n",
            "    train perplexity 6.199791,\n",
            "    test perplexity 6.133818.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.819403,\n",
            "    train perplexity 6.168173,\n",
            "    test perplexity 6.188751.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.817147,\n",
            "    train perplexity 6.154278,\n",
            "    test perplexity 6.125320.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.814676,\n",
            "    train perplexity 6.139084,\n",
            "    test perplexity 6.092082.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.811689,\n",
            "    train perplexity 6.120775,\n",
            "    test perplexity 6.193766.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.809682,\n",
            "    train perplexity 6.108506,\n",
            "    test perplexity 6.099488.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.805819,\n",
            "    train perplexity 6.084952,\n",
            "    test perplexity 6.151515.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.804341,\n",
            "    train perplexity 6.075968,\n",
            "    test perplexity 6.049878.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.801490,\n",
            "    train perplexity 6.058671,\n",
            "    test perplexity 5.960852.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.798025,\n",
            "    train perplexity 6.037710,\n",
            "    test perplexity 6.084411.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.796437,\n",
            "    train perplexity 6.028131,\n",
            "    test perplexity 6.051251.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.793172,\n",
            "    train perplexity 6.008481,\n",
            "    test perplexity 6.013831.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.790724,\n",
            "    train perplexity 5.993793,\n",
            "    test perplexity 6.011064.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.786606,\n",
            "    train perplexity 5.969159,\n",
            "    test perplexity 5.961852.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.786618,\n",
            "    train perplexity 5.969230,\n",
            "    test perplexity 5.893377.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.786483,\n",
            "    train perplexity 5.968423,\n",
            "    test perplexity 6.003788.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.781435,\n",
            "    train perplexity 5.938374,\n",
            "    test perplexity 5.925050.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.780494,\n",
            "    train perplexity 5.932786,\n",
            "    test perplexity 5.904604.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.777709,\n",
            "    train perplexity 5.916288,\n",
            "    test perplexity 5.906040.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.777446,\n",
            "    train perplexity 5.914732,\n",
            "    test perplexity 5.834775.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.773611,\n",
            "    train perplexity 5.892090,\n",
            "    test perplexity 5.971092.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.772602,\n",
            "    train perplexity 5.886150,\n",
            "    test perplexity 5.876146.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.770947,\n",
            "    train perplexity 5.876416,\n",
            "    test perplexity 5.906997.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.768426,\n",
            "    train perplexity 5.861621,\n",
            "    test perplexity 5.802626.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.767567,\n",
            "    train perplexity 5.856588,\n",
            "    test perplexity 5.852170.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.764054,\n",
            "    train perplexity 5.836049,\n",
            "    test perplexity 5.800429.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.763814,\n",
            "    train perplexity 5.834650,\n",
            "    test perplexity 5.848259.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.761514,\n",
            "    train perplexity 5.821245,\n",
            "    test perplexity 5.742138.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.760162,\n",
            "    train perplexity 5.813380,\n",
            "    test perplexity 5.798513.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.758983,\n",
            "    train perplexity 5.806531,\n",
            "    test perplexity 5.810833.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.758455,\n",
            "    train perplexity 5.803462,\n",
            "    test perplexity 5.830867.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.753326,\n",
            "    train perplexity 5.773777,\n",
            "    test perplexity 5.788473.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.753135,\n",
            "    train perplexity 5.772674,\n",
            "    test perplexity 5.713245.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.752408,\n",
            "    train perplexity 5.768479,\n",
            "    test perplexity 5.735720.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.751346,\n",
            "    train perplexity 5.762355,\n",
            "    test perplexity 5.842012.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.748901,\n",
            "    train perplexity 5.748282,\n",
            "    test perplexity 5.774274.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.748190,\n",
            "    train perplexity 5.744197,\n",
            "    test perplexity 5.746630.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.745820,\n",
            "    train perplexity 5.730597,\n",
            "    test perplexity 5.817201.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.746027,\n",
            "    train perplexity 5.731783,\n",
            "    test perplexity 5.708042.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.742007,\n",
            "    train perplexity 5.708792,\n",
            "    test perplexity 5.664074.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.741764,\n",
            "    train perplexity 5.707401,\n",
            "    test perplexity 5.682386.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.741632,\n",
            "    train perplexity 5.706648,\n",
            "    test perplexity 5.659143.\n",
            "\n"
          ]
        }
      ],
      "id": "2Egmoe2JZtKE"
    },
    {
      "cell_type": "code",
      "source": [
        "net2.predict(\"quijote y sancho \", 30, vocab)"
      ],
      "metadata": {
        "id": "6tHzBfVdbCj4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ea95222-d171-4594-caf9-56353ee4895b"
      },
      "id": "6tHzBfVdbCj4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quijote y sancho de la mando de la mando de la '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "c00601bb"
      },
      "source": [
        "# Long Short-Term Memory (LSTM)\n",
        "\n",
        "Uno de los problemas que vimos que tenían las redes recurrentes es las características de sus gradientes hacían que estos pudieran, o bien crecer de manera descontrolada, o bien achicarse hasta 0. En cualquiera de los dos casos nuestra propuesta de solución fue restringir el número de pasos hacia atras en el tiempo en los que calcularemos el gradiente. Sin embargo, este camino puede ser un problema en algunas aplicaciones.\n",
        "\n",
        "Para esto surgió una alternativa a una RNN, que es la aquitectura LSTM\n",
        "\n",
        "## Celdas de Memoria\n",
        "\n",
        "LSTM es una red recurrente: la nueva salida depende de las entradas anteriores. Sin embargo, agrega un conjunto de variables ocultas para intentar emular la memoria RAM de una PC.\n",
        "\n",
        "Una memoria RAM guarda diferente información para utilizarla luego en un cálculo. Además puede realizar un conjunto de operaciones por ejemplo:\n",
        "\n",
        "* Leer los valores guardados anteriormente\n",
        "* Escribir el valor guardado por uno nuevo \n",
        "* Borrar lo que había en memoria.\n",
        "\n",
        "En este sentido LSTM tendrá dos variables ocultas. La primera es nuestra varaible oculta convencional $\\mathbf{H}_{t-1}$. Pero la segunda es una varaible $\\mathbf{C}_{t-1}$ que guarda información como una memoria RAM. Luego usaremos es información para generar una nueva variable $\\mathbf{H}_{t}$ en el próximo paso temporal.\n",
        "\n",
        "En sintonía con lo anteior necesitaremos señales lógicas que nos ayudaran a decidir que hacer con la nueva entrada $\\mathbf{X}_t$ y la varaible oculta anterior $\\mathbf{H}_{t-1}$:\n",
        "\n",
        "* Leer el el valor de memoria $\\mathbf{C}_{t-1}$ para calcular $\\mathbf{H}_{t}$\n",
        "* Modificar el valor anterior de $\\mathbf{C}_{t-1}$, para crear uno nuevo $\\mathbf{H}_t$\n",
        "* Borrar completamente la memoria $\\mathbf{C}_{t} = 0$$\n",
        "\n",
        "En una memoria RAM real, estás señales con manejadas por señales binarias. Entonces si quisieramos borrar, pondríamos un 1 en entrada de la RAM que recibe la instrucción de borrado. Si quisieramos leer, pondríamos un 0 en la entrada de borrado y un 1 en lade leer, etc.\n",
        "\n",
        "Al estar trabajando con tensores y problemas de optimización, ahora nuestra salida puede ser continua. es decir, ya no solo podríamos borrar, sino tambien borrar parcialmente. Sin embargo, para esto debemos asegurarnos que nuestras salidas sean valores entre 0 y 1. Para esto crearemos una capa RNN con una sigmoidea como función de activación. Presentemos entonces las primeras 3 compuertas lógicas de nuestro LSTM:\n",
        "\n",
        "### Compuestas Lógicas\n",
        "\n",
        "\n",
        "![](http://d2l.ai/_images/lstm-0.svg)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\\\\\n",
        "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
        "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Es decir, tenemos 3 capas RNN, con sus respectivos pesos. En donde: \n",
        "\n",
        "* $\\mathbf{O}_{t}$ corresponde a la señal de lectura. Nos dice que tanta importacia debemos darle a los valores anteriores de nuestra variable oculta $\\mathbf{H}_{t-1}$\n",
        "* $\\mathbf{I}_{t}$ corresponde a la señal de escritura.Nos dice que tanto debe cambiar nuestra variable oculta anterior $\\mathbf{H}_{t-1}$. \n",
        "* $\\mathbf{F}_{t}$ corresponde a la señal de borrado.Nos dice que tanto eliminar nuestra variable oculta anterior $\\mathbf{H}_{t-1}$.\n",
        "\n",
        "La analogía con el lenguaje es más o menos directa:\n",
        "\n",
        "* En un libro de botánica, al describir un árbol hay una serie de oraciones relacionadas entre ellas. Nuestra red aprender a mantener la coherencia. Si hablamos de la fruta roja del árbol, no puede luego hablar de la fruta amarilla. Debemos MANTENER el tema de la conversación.\n",
        "* En una obra de teatro, un personaje puede cambiar su estado. Puede pasar de estar parado a desmayarse. En ese sentido debemos poder MODIFICAR la situación del personajes. De otra manera, sería dificil de entender poque alguien se levanto si nunca dejo de estar parado.\n",
        "* En una novela, se suceden una serie de acciones, pero no siempre están conectadas entre ellas. Si un capitulo esta centrado en el protagonista y el sigueinte centrado en el villano, debemos OLVIDAR el contexto anterior para no perder el hilo. Si antes el protagonista usaba patalones azules, debemos ignorar eso cuando los pantalones del villano son negros.\n",
        "\n",
        "Rercordemos una vez más que hemos elegido una función de activación sigmoidea en analogía a las señales de las compuertas lógicas de una memoria RAM.\n",
        "\n",
        "### Candidato de memoria\n",
        "\n",
        "Ahora, lo que haremos será calcular el nuevo valor que guardaremos en memoria. Para esto simplementa aplicamos una RNN convencional con una $\\tanh$ como función de activación.\n",
        "\n",
        "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),$$\n",
        "\n",
        "Este valor es un valor tentativo con el cual cambiaremos el valor existente en nuestra memoria $\\mathbf{C}_t$ \n",
        "\n",
        "![](http://d2l.ai/_images/lstm-1.svg)\n",
        "\n",
        "### Escribiendo en memoria.\n",
        "\n",
        "Ahora lo que haremos será modificar nuestro valor en memoria. Hay dos operaciones que pueden modificar nuestra memoria: borrado y escritura. Con lo cual haremos una combinación lineal de las dos cosas \n",
        "\n",
        "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n",
        "\n",
        "En donde hemos usado $\\odot$ para representar el **temido** producto de Haddamar. Analicemos con un ejemplo sencillo como calcular el producto de Haddamar de dos matrices:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\begin{bmatrix}2&3\\\\5&7\\end{bmatrix},\n",
        "\\mathbf{B} = \\begin{bmatrix}3&5\\\\7&2\\end{bmatrix},\\\\\n",
        "\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix}2&3\\\\5&7\\end{bmatrix} ⊙ \\begin{bmatrix}3&5\\\\7&2\\end{bmatrix}=\\begin{bmatrix}6&15\\\\35&14\\end{bmatrix}\\\\\n",
        "\\mathbf{A} \\odot \\mathbf{A} = \\begin{bmatrix}2&3\\\\5&7\\end{bmatrix} ⊙ \\begin{bmatrix}2&3\\\\5&7\\end{bmatrix}=\\begin{bmatrix}4&9\\\\25&49\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Vemos que el producto de Haddamar es en esencia no es más que multiplicar elemento a elemento de una matriz o un tensor. Es simplemente un nombre raro, para algo que es mucho más intuitivo que la multiplicación de matrices tradicionales.\n",
        "\n",
        "![](http://d2l.ai/_images/lstm-2.svg)\n",
        "\n",
        "Con viene analizar que ocurre en cada caso para $\\mathbf{F}_{t}$ y $\\mathbf{I}_{t}$\n",
        "\n",
        "||$\\mathbf{I}_{t} = 1$| $\\mathbf{I}_{t} = 0$\n",
        "|---|:-:|:-:|\n",
        "|$\\mathbf{F}_{t}=1$|Combinación lineal del valor nuevo y el viejo|Se conserva el valor viejo|\n",
        "|$\\mathbf{F}_{t}=0$|Se reemplaza el valor nuevo por el viejo|Se borra la celda de memoria|\n",
        "\n",
        "### Estado oculto.\n",
        "\n",
        "Ahora sí, leeremos la memoria para obtener nuestro nueva variable oculta\n",
        "\n",
        "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n",
        "\n",
        "Es decir, aplicamos una última transformación a nuestro valor de memoria y luego decidimos cuanto leeremos de ese valor. Si $\\mathbf{O}_t = 0$, ignoraremos lo que haya en memoria, pero si $\\mathbf{O}_t = 1$, le prestaremos total antención.\n",
        "\n",
        "![](http://d2l.ai/_images/lstm-3.svg)\n"
      ],
      "id": "c00601bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation de LSTM desde 0\n"
      ],
      "metadata": {
        "id": "5nJ5vwW9gjhF"
      },
      "id": "5nJ5vwW9gjhF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:39:30.623336Z",
          "iopub.status.busy": "2022-07-13T08:39:30.622886Z",
          "iopub.status.idle": "2022-07-13T08:39:30.629869Z",
          "shell.execute_reply": "2022-07-13T08:39:30.629064Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "20558e21"
      },
      "outputs": [],
      "source": [
        "class LSTMScratch(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        super().__init__()\n",
        "\n",
        "        init_weight = lambda *shape: torch.nn.Parameter(torch.randn(*shape) * 0.01)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          torch.nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_inputs = num_inputs\n",
        "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
        "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
        "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
        "        self.W_xc, self.W_hc, self.b_c = triple()  # Candidate memory cell\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        H, C = None, None if H_C is None else H_C\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            I = torch.sigmoid(torch.matmul(X, self.W_xi) + (\n",
        "                torch.matmul(H, self.W_hi) if H is not None else 0) + self.b_i)\n",
        "            if H is None:\n",
        "                H, C = torch.zeros_like(I), torch.zeros_like(I)\n",
        "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
        "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
        "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
        "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
        "            C_tilda = torch.tanh(torch.matmul(X, self.W_xc) +\n",
        "                              torch.matmul(H, self.W_hc) + self.b_c)\n",
        "            C = F * C + I * C_tilda # El prod. de Haddar es el producto normal!\n",
        "            H = O * torch.tanh(C)\n",
        "            outputs.append(H)\n",
        "        return outputs, (H, C)"
      ],
      "id": "20558e21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "cd282b40"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "\n"
      ],
      "id": "cd282b40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:39:30.644274Z",
          "iopub.status.busy": "2022-07-13T08:39:30.643979Z",
          "iopub.status.idle": "2022-07-13T08:40:58.543327Z",
          "shell.execute_reply": "2022-07-13T08:40:58.542112Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "680c4de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41db670f-00bd-4a0e-bb95-02df65cf8587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 3.287472,\n",
            "    train perplexity 26.775084,\n",
            "    test perplexity 17.895092.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.856624,\n",
            "    train perplexity 17.402679,\n",
            "    test perplexity 17.120613.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.839097,\n",
            "    train perplexity 17.100321,\n",
            "    test perplexity 16.994011.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.822800,\n",
            "    train perplexity 16.823891,\n",
            "    test perplexity 16.699621.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.795369,\n",
            "    train perplexity 16.368662,\n",
            "    test perplexity 16.146240.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.750924,\n",
            "    train perplexity 15.657090,\n",
            "    test perplexity 15.113146.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.656917,\n",
            "    train perplexity 14.252276,\n",
            "    test perplexity 13.472737.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.539634,\n",
            "    train perplexity 12.675031,\n",
            "    test perplexity 12.074724.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.438794,\n",
            "    train perplexity 11.459208,\n",
            "    test perplexity 11.034871.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.369293,\n",
            "    train perplexity 10.689837,\n",
            "    test perplexity 10.446251.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.315131,\n",
            "    train perplexity 10.126254,\n",
            "    test perplexity 9.908369.\n",
            "\n",
            "epoch 12\n",
            "    loss 2.278063,\n",
            "    train perplexity 9.757759,\n",
            "    test perplexity 9.465185.\n",
            "\n",
            "epoch 13\n",
            "    loss 2.245659,\n",
            "    train perplexity 9.446638,\n",
            "    test perplexity 9.302939.\n",
            "\n",
            "epoch 14\n",
            "    loss 2.218669,\n",
            "    train perplexity 9.195086,\n",
            "    test perplexity 9.253327.\n",
            "\n",
            "epoch 15\n",
            "    loss 2.193682,\n",
            "    train perplexity 8.968173,\n",
            "    test perplexity 8.908896.\n",
            "\n",
            "epoch 16\n",
            "    loss 2.173290,\n",
            "    train perplexity 8.787148,\n",
            "    test perplexity 8.569743.\n",
            "\n",
            "epoch 17\n",
            "    loss 2.155732,\n",
            "    train perplexity 8.634210,\n",
            "    test perplexity 8.596400.\n",
            "\n",
            "epoch 18\n",
            "    loss 2.139194,\n",
            "    train perplexity 8.492594,\n",
            "    test perplexity 8.414489.\n",
            "\n",
            "epoch 19\n",
            "    loss 2.120558,\n",
            "    train perplexity 8.335788,\n",
            "    test perplexity 8.317541.\n",
            "\n",
            "epoch 20\n",
            "    loss 2.104008,\n",
            "    train perplexity 8.198967,\n",
            "    test perplexity 8.092108.\n",
            "\n",
            "epoch 21\n",
            "    loss 2.088319,\n",
            "    train perplexity 8.071339,\n",
            "    test perplexity 8.019138.\n",
            "\n",
            "epoch 22\n",
            "    loss 2.072137,\n",
            "    train perplexity 7.941780,\n",
            "    test perplexity 7.915698.\n",
            "\n",
            "epoch 23\n",
            "    loss 2.058943,\n",
            "    train perplexity 7.837677,\n",
            "    test perplexity 7.833599.\n",
            "\n",
            "epoch 24\n",
            "    loss 2.043672,\n",
            "    train perplexity 7.718904,\n",
            "    test perplexity 7.597175.\n",
            "\n",
            "epoch 25\n",
            "    loss 2.032493,\n",
            "    train perplexity 7.633093,\n",
            "    test perplexity 7.469029.\n",
            "\n",
            "epoch 26\n",
            "    loss 2.015535,\n",
            "    train perplexity 7.504740,\n",
            "    test perplexity 7.395429.\n",
            "\n",
            "epoch 27\n",
            "    loss 2.005695,\n",
            "    train perplexity 7.431256,\n",
            "    test perplexity 7.455124.\n",
            "\n",
            "epoch 28\n",
            "    loss 1.993375,\n",
            "    train perplexity 7.340264,\n",
            "    test perplexity 7.280827.\n",
            "\n",
            "epoch 29\n",
            "    loss 1.980610,\n",
            "    train perplexity 7.247164,\n",
            "    test perplexity 7.180842.\n",
            "\n",
            "epoch 30\n",
            "    loss 1.969954,\n",
            "    train perplexity 7.170349,\n",
            "    test perplexity 7.082474.\n",
            "\n",
            "epoch 31\n",
            "    loss 1.957519,\n",
            "    train perplexity 7.081738,\n",
            "    test perplexity 7.053024.\n",
            "\n",
            "epoch 32\n",
            "    loss 1.948275,\n",
            "    train perplexity 7.016573,\n",
            "    test perplexity 7.006990.\n",
            "\n",
            "epoch 33\n",
            "    loss 1.936816,\n",
            "    train perplexity 6.936628,\n",
            "    test perplexity 6.862895.\n",
            "\n",
            "epoch 34\n",
            "    loss 1.927494,\n",
            "    train perplexity 6.872267,\n",
            "    test perplexity 6.825919.\n",
            "\n",
            "epoch 35\n",
            "    loss 1.919274,\n",
            "    train perplexity 6.816006,\n",
            "    test perplexity 6.781243.\n",
            "\n",
            "epoch 36\n",
            "    loss 1.909467,\n",
            "    train perplexity 6.749487,\n",
            "    test perplexity 6.666992.\n",
            "\n",
            "epoch 37\n",
            "    loss 1.903225,\n",
            "    train perplexity 6.707491,\n",
            "    test perplexity 6.730814.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.892504,\n",
            "    train perplexity 6.635965,\n",
            "    test perplexity 6.540966.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.887099,\n",
            "    train perplexity 6.600191,\n",
            "    test perplexity 6.619690.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.878323,\n",
            "    train perplexity 6.542521,\n",
            "    test perplexity 6.529294.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.872984,\n",
            "    train perplexity 6.507688,\n",
            "    test perplexity 6.402797.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.864950,\n",
            "    train perplexity 6.455612,\n",
            "    test perplexity 6.346341.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.860043,\n",
            "    train perplexity 6.424010,\n",
            "    test perplexity 6.407211.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.850973,\n",
            "    train perplexity 6.366012,\n",
            "    test perplexity 6.380381.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.847987,\n",
            "    train perplexity 6.347029,\n",
            "    test perplexity 6.401110.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.839156,\n",
            "    train perplexity 6.291225,\n",
            "    test perplexity 6.339397.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.836129,\n",
            "    train perplexity 6.272211,\n",
            "    test perplexity 6.275253.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.828631,\n",
            "    train perplexity 6.225356,\n",
            "    test perplexity 6.186851.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.825067,\n",
            "    train perplexity 6.203209,\n",
            "    test perplexity 6.222110.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.818291,\n",
            "    train perplexity 6.161322,\n",
            "    test perplexity 6.153477.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.812090,\n",
            "    train perplexity 6.123229,\n",
            "    test perplexity 6.132534.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.810391,\n",
            "    train perplexity 6.112838,\n",
            "    test perplexity 6.047153.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.800680,\n",
            "    train perplexity 6.053762,\n",
            "    test perplexity 6.031478.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.800407,\n",
            "    train perplexity 6.052109,\n",
            "    test perplexity 5.989316.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.793490,\n",
            "    train perplexity 6.010390,\n",
            "    test perplexity 5.920225.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.789925,\n",
            "    train perplexity 5.989002,\n",
            "    test perplexity 5.965362.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.784112,\n",
            "    train perplexity 5.954288,\n",
            "    test perplexity 5.989659.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.776502,\n",
            "    train perplexity 5.909152,\n",
            "    test perplexity 5.881778.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.779209,\n",
            "    train perplexity 5.925165,\n",
            "    test perplexity 5.851916.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.769114,\n",
            "    train perplexity 5.865653,\n",
            "    test perplexity 5.862514.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.766869,\n",
            "    train perplexity 5.852501,\n",
            "    test perplexity 5.815579.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.761685,\n",
            "    train perplexity 5.822237,\n",
            "    test perplexity 5.865172.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.758610,\n",
            "    train perplexity 5.804367,\n",
            "    test perplexity 5.764851.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.753380,\n",
            "    train perplexity 5.774084,\n",
            "    test perplexity 5.780340.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.747377,\n",
            "    train perplexity 5.739528,\n",
            "    test perplexity 5.733695.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.744325,\n",
            "    train perplexity 5.722036,\n",
            "    test perplexity 5.703724.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.739924,\n",
            "    train perplexity 5.696911,\n",
            "    test perplexity 5.652539.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.735827,\n",
            "    train perplexity 5.673621,\n",
            "    test perplexity 5.681551.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.731997,\n",
            "    train perplexity 5.651932,\n",
            "    test perplexity 5.662017.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.730105,\n",
            "    train perplexity 5.641248,\n",
            "    test perplexity 5.597823.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.724203,\n",
            "    train perplexity 5.608047,\n",
            "    test perplexity 5.590866.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.719147,\n",
            "    train perplexity 5.579767,\n",
            "    test perplexity 5.621481.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.716675,\n",
            "    train perplexity 5.565989,\n",
            "    test perplexity 5.632956.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.709365,\n",
            "    train perplexity 5.525453,\n",
            "    test perplexity 5.544595.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.707730,\n",
            "    train perplexity 5.516427,\n",
            "    test perplexity 5.547126.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.704652,\n",
            "    train perplexity 5.499473,\n",
            "    test perplexity 5.485042.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.698682,\n",
            "    train perplexity 5.466736,\n",
            "    test perplexity 5.436772.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.694852,\n",
            "    train perplexity 5.445837,\n",
            "    test perplexity 5.311856.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.695546,\n",
            "    train perplexity 5.449618,\n",
            "    test perplexity 5.510249.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.686340,\n",
            "    train perplexity 5.399680,\n",
            "    test perplexity 5.289873.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.684597,\n",
            "    train perplexity 5.390280,\n",
            "    test perplexity 5.368207.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.684956,\n",
            "    train perplexity 5.392215,\n",
            "    test perplexity 5.371344.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.677188,\n",
            "    train perplexity 5.350490,\n",
            "    test perplexity 5.368958.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.675306,\n",
            "    train perplexity 5.340427,\n",
            "    test perplexity 5.292367.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.672689,\n",
            "    train perplexity 5.326470,\n",
            "    test perplexity 5.373497.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.665605,\n",
            "    train perplexity 5.288870,\n",
            "    test perplexity 5.226862.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.665544,\n",
            "    train perplexity 5.288548,\n",
            "    test perplexity 5.279040.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.663993,\n",
            "    train perplexity 5.280356,\n",
            "    test perplexity 5.202784.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.658030,\n",
            "    train perplexity 5.248961,\n",
            "    test perplexity 5.181577.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.654897,\n",
            "    train perplexity 5.232540,\n",
            "    test perplexity 5.201667.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.653853,\n",
            "    train perplexity 5.227082,\n",
            "    test perplexity 5.211757.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.648630,\n",
            "    train perplexity 5.199854,\n",
            "    test perplexity 5.307533.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.648864,\n",
            "    train perplexity 5.201067,\n",
            "    test perplexity 5.165156.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.645801,\n",
            "    train perplexity 5.185163,\n",
            "    test perplexity 5.243313.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.642018,\n",
            "    train perplexity 5.165583,\n",
            "    test perplexity 5.170674.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.637163,\n",
            "    train perplexity 5.140564,\n",
            "    test perplexity 5.154746.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.634835,\n",
            "    train perplexity 5.128610,\n",
            "    test perplexity 5.046515.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.633951,\n",
            "    train perplexity 5.124079,\n",
            "    test perplexity 5.148705.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.631666,\n",
            "    train perplexity 5.112387,\n",
            "    test perplexity 5.082191.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.628161,\n",
            "    train perplexity 5.094497,\n",
            "    test perplexity 5.081319.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lstm_scrt = LSTMScratch(num_inputs=len(vocab), num_hiddens=32)\n",
        "net3 = RNNLMScratch(lstm_scrt, vocab_size=len(vocab))\n",
        "trainer = torch.optim.Adadelta(net3.parameters(), lr=4)\n",
        "loss = loss_NLP\n",
        "\n",
        "num_epochs = 100 \n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net3(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net3)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()\n"
      ],
      "id": "680c4de0"
    },
    {
      "cell_type": "code",
      "source": [
        "net3.predict(\"sancho y quijote\", 30, vocab)"
      ],
      "metadata": {
        "id": "MWk4IA0bjmzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce2bd018-3e41-4873-ab12-96b0cbf76948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sancho y quijote de de de de de de de de de de'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "id": "MWk4IA0bjmzR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "2192472f"
      },
      "source": [
        "## Implementación Concisa\n"
      ],
      "id": "2192472f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.550820Z",
          "iopub.status.busy": "2022-07-13T08:40:58.550482Z",
          "iopub.status.idle": "2022-07-13T08:40:58.556222Z",
          "shell.execute_reply": "2022-07-13T08:40:58.555331Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "b1fe8390"
      },
      "outputs": [],
      "source": [
        "class LSTM(RNN):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.rnn = torch.nn.LSTM(num_inputs, num_hiddens)\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        return self.rnn(inputs, H_C)"
      ],
      "id": "b1fe8390"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.559697Z",
          "iopub.status.busy": "2022-07-13T08:40:58.559412Z",
          "iopub.status.idle": "2022-07-13T08:41:51.293011Z",
          "shell.execute_reply": "2022-07-13T08:41:51.292187Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "dc3ab5a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a25d0f4-76a3-498d-f227-cd23060a8ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 3.037571,\n",
            "    train perplexity 20.854521,\n",
            "    test perplexity 17.166391.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.824945,\n",
            "    train perplexity 16.860025,\n",
            "    test perplexity 16.526625.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.743249,\n",
            "    train perplexity 15.537383,\n",
            "    test perplexity 14.544025.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.580240,\n",
            "    train perplexity 13.200306,\n",
            "    test perplexity 12.303865.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.452065,\n",
            "    train perplexity 11.612301,\n",
            "    test perplexity 11.045283.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.366464,\n",
            "    train perplexity 10.659632,\n",
            "    test perplexity 10.206488.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.305007,\n",
            "    train perplexity 10.024243,\n",
            "    test perplexity 9.596799.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.249058,\n",
            "    train perplexity 9.478807,\n",
            "    test perplexity 9.282849.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.213953,\n",
            "    train perplexity 9.151822,\n",
            "    test perplexity 8.921330.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.183291,\n",
            "    train perplexity 8.875464,\n",
            "    test perplexity 8.815040.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.160307,\n",
            "    train perplexity 8.673803,\n",
            "    test perplexity 8.539734.\n",
            "\n",
            "epoch 12\n",
            "    loss 2.133224,\n",
            "    train perplexity 8.442037,\n",
            "    test perplexity 8.349775.\n",
            "\n",
            "epoch 13\n",
            "    loss 2.116596,\n",
            "    train perplexity 8.302831,\n",
            "    test perplexity 8.128507.\n",
            "\n",
            "epoch 14\n",
            "    loss 2.091534,\n",
            "    train perplexity 8.097326,\n",
            "    test perplexity 7.987008.\n",
            "\n",
            "epoch 15\n",
            "    loss 2.080458,\n",
            "    train perplexity 8.008137,\n",
            "    test perplexity 7.983844.\n",
            "\n",
            "epoch 16\n",
            "    loss 2.061353,\n",
            "    train perplexity 7.856592,\n",
            "    test perplexity 7.720460.\n",
            "\n",
            "epoch 17\n",
            "    loss 2.045285,\n",
            "    train perplexity 7.731363,\n",
            "    test perplexity 7.753431.\n",
            "\n",
            "epoch 18\n",
            "    loss 2.024043,\n",
            "    train perplexity 7.568865,\n",
            "    test perplexity 7.482668.\n",
            "\n",
            "epoch 19\n",
            "    loss 2.013767,\n",
            "    train perplexity 7.491483,\n",
            "    test perplexity 7.413515.\n",
            "\n",
            "epoch 20\n",
            "    loss 1.996058,\n",
            "    train perplexity 7.359989,\n",
            "    test perplexity 7.268968.\n",
            "\n",
            "epoch 21\n",
            "    loss 1.982734,\n",
            "    train perplexity 7.262572,\n",
            "    test perplexity 7.184996.\n",
            "\n",
            "epoch 22\n",
            "    loss 1.970867,\n",
            "    train perplexity 7.176895,\n",
            "    test perplexity 7.173620.\n",
            "\n",
            "epoch 23\n",
            "    loss 1.955291,\n",
            "    train perplexity 7.065978,\n",
            "    test perplexity 7.030769.\n",
            "\n",
            "epoch 24\n",
            "    loss 1.945536,\n",
            "    train perplexity 6.997381,\n",
            "    test perplexity 6.951468.\n",
            "\n",
            "epoch 25\n",
            "    loss 1.936616,\n",
            "    train perplexity 6.935242,\n",
            "    test perplexity 6.919140.\n",
            "\n",
            "epoch 26\n",
            "    loss 1.923587,\n",
            "    train perplexity 6.845468,\n",
            "    test perplexity 6.871498.\n",
            "\n",
            "epoch 27\n",
            "    loss 1.919875,\n",
            "    train perplexity 6.820105,\n",
            "    test perplexity 6.800812.\n",
            "\n",
            "epoch 28\n",
            "    loss 1.911372,\n",
            "    train perplexity 6.762359,\n",
            "    test perplexity 6.749184.\n",
            "\n",
            "epoch 29\n",
            "    loss 1.902774,\n",
            "    train perplexity 6.704470,\n",
            "    test perplexity 6.699750.\n",
            "\n",
            "epoch 30\n",
            "    loss 1.895354,\n",
            "    train perplexity 6.654902,\n",
            "    test perplexity 6.605030.\n",
            "\n",
            "epoch 31\n",
            "    loss 1.888695,\n",
            "    train perplexity 6.610736,\n",
            "    test perplexity 6.569150.\n",
            "\n",
            "epoch 32\n",
            "    loss 1.879667,\n",
            "    train perplexity 6.551321,\n",
            "    test perplexity 6.599177.\n",
            "\n",
            "epoch 33\n",
            "    loss 1.872047,\n",
            "    train perplexity 6.501590,\n",
            "    test perplexity 6.591155.\n",
            "\n",
            "epoch 34\n",
            "    loss 1.864419,\n",
            "    train perplexity 6.452186,\n",
            "    test perplexity 6.513971.\n",
            "\n",
            "epoch 35\n",
            "    loss 1.861783,\n",
            "    train perplexity 6.435202,\n",
            "    test perplexity 6.330029.\n",
            "\n",
            "epoch 36\n",
            "    loss 1.851039,\n",
            "    train perplexity 6.366428,\n",
            "    test perplexity 6.347154.\n",
            "\n",
            "epoch 37\n",
            "    loss 1.843513,\n",
            "    train perplexity 6.318698,\n",
            "    test perplexity 6.372873.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.841274,\n",
            "    train perplexity 6.304562,\n",
            "    test perplexity 6.262600.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.831883,\n",
            "    train perplexity 6.245637,\n",
            "    test perplexity 6.154557.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.831100,\n",
            "    train perplexity 6.240749,\n",
            "    test perplexity 6.254273.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.824126,\n",
            "    train perplexity 6.197376,\n",
            "    test perplexity 6.181163.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.813262,\n",
            "    train perplexity 6.130412,\n",
            "    test perplexity 6.162338.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.813235,\n",
            "    train perplexity 6.130244,\n",
            "    test perplexity 6.065641.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.806945,\n",
            "    train perplexity 6.091807,\n",
            "    test perplexity 6.050091.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.797736,\n",
            "    train perplexity 6.035966,\n",
            "    test perplexity 6.061568.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.799605,\n",
            "    train perplexity 6.047261,\n",
            "    test perplexity 6.082109.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.790110,\n",
            "    train perplexity 5.990110,\n",
            "    test perplexity 5.932230.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.785288,\n",
            "    train perplexity 5.961298,\n",
            "    test perplexity 6.018226.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.787888,\n",
            "    train perplexity 5.976818,\n",
            "    test perplexity 5.926877.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.776075,\n",
            "    train perplexity 5.906626,\n",
            "    test perplexity 5.901589.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.774405,\n",
            "    train perplexity 5.896771,\n",
            "    test perplexity 5.824474.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.768005,\n",
            "    train perplexity 5.859155,\n",
            "    test perplexity 5.872625.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.764681,\n",
            "    train perplexity 5.839712,\n",
            "    test perplexity 5.905724.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.763333,\n",
            "    train perplexity 5.831843,\n",
            "    test perplexity 5.877367.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.757655,\n",
            "    train perplexity 5.798825,\n",
            "    test perplexity 5.818584.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.752620,\n",
            "    train perplexity 5.769697,\n",
            "    test perplexity 5.754200.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.747187,\n",
            "    train perplexity 5.738440,\n",
            "    test perplexity 5.705078.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.747108,\n",
            "    train perplexity 5.737983,\n",
            "    test perplexity 5.706017.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.739472,\n",
            "    train perplexity 5.694338,\n",
            "    test perplexity 5.708741.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.741028,\n",
            "    train perplexity 5.703206,\n",
            "    test perplexity 5.650420.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.732568,\n",
            "    train perplexity 5.655157,\n",
            "    test perplexity 5.678225.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.729475,\n",
            "    train perplexity 5.637695,\n",
            "    test perplexity 5.663235.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.727132,\n",
            "    train perplexity 5.624502,\n",
            "    test perplexity 5.652131.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.719808,\n",
            "    train perplexity 5.583457,\n",
            "    test perplexity 5.579107.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.720505,\n",
            "    train perplexity 5.587352,\n",
            "    test perplexity 5.598355.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.715117,\n",
            "    train perplexity 5.557325,\n",
            "    test perplexity 5.683386.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.710484,\n",
            "    train perplexity 5.531637,\n",
            "    test perplexity 5.531764.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.706126,\n",
            "    train perplexity 5.507585,\n",
            "    test perplexity 5.512094.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.706279,\n",
            "    train perplexity 5.508427,\n",
            "    test perplexity 5.491862.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.697255,\n",
            "    train perplexity 5.458941,\n",
            "    test perplexity 5.492513.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.698654,\n",
            "    train perplexity 5.466587,\n",
            "    test perplexity 5.475130.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.692460,\n",
            "    train perplexity 5.432829,\n",
            "    test perplexity 5.474215.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.689340,\n",
            "    train perplexity 5.415906,\n",
            "    test perplexity 5.428674.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.689208,\n",
            "    train perplexity 5.415188,\n",
            "    test perplexity 5.385579.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.683299,\n",
            "    train perplexity 5.383285,\n",
            "    test perplexity 5.352387.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.680061,\n",
            "    train perplexity 5.365883,\n",
            "    test perplexity 5.356486.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.677454,\n",
            "    train perplexity 5.351912,\n",
            "    test perplexity 5.416831.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.673595,\n",
            "    train perplexity 5.331298,\n",
            "    test perplexity 5.316706.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.672234,\n",
            "    train perplexity 5.324048,\n",
            "    test perplexity 5.299723.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.668324,\n",
            "    train perplexity 5.303273,\n",
            "    test perplexity 5.327907.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.665228,\n",
            "    train perplexity 5.286876,\n",
            "    test perplexity 5.310725.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.660387,\n",
            "    train perplexity 5.261348,\n",
            "    test perplexity 5.219879.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.661286,\n",
            "    train perplexity 5.266078,\n",
            "    test perplexity 5.209955.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.658333,\n",
            "    train perplexity 5.250549,\n",
            "    test perplexity 5.222127.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.651487,\n",
            "    train perplexity 5.214726,\n",
            "    test perplexity 5.206612.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.649884,\n",
            "    train perplexity 5.206378,\n",
            "    test perplexity 5.241406.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.651186,\n",
            "    train perplexity 5.213161,\n",
            "    test perplexity 5.216254.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.645779,\n",
            "    train perplexity 5.185047,\n",
            "    test perplexity 5.202965.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.642739,\n",
            "    train perplexity 5.169311,\n",
            "    test perplexity 5.053801.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.641505,\n",
            "    train perplexity 5.162936,\n",
            "    test perplexity 5.041804.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.637911,\n",
            "    train perplexity 5.144411,\n",
            "    test perplexity 5.164500.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.637573,\n",
            "    train perplexity 5.142671,\n",
            "    test perplexity 5.108254.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.632157,\n",
            "    train perplexity 5.114894,\n",
            "    test perplexity 5.039547.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.629514,\n",
            "    train perplexity 5.101392,\n",
            "    test perplexity 5.122705.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.630178,\n",
            "    train perplexity 5.104782,\n",
            "    test perplexity 5.080302.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.625619,\n",
            "    train perplexity 5.081563,\n",
            "    test perplexity 5.030560.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.622403,\n",
            "    train perplexity 5.065247,\n",
            "    test perplexity 5.093283.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.622524,\n",
            "    train perplexity 5.065859,\n",
            "    test perplexity 5.018547.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.617685,\n",
            "    train perplexity 5.041406,\n",
            "    test perplexity 5.002402.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.617823,\n",
            "    train perplexity 5.042100,\n",
            "    test perplexity 5.034860.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lstm = LSTM(num_inputs=len(vocab), num_hiddens=32)\n",
        "net4 = RNNLM(lstm, vocab_size=len(vocab))\n",
        "trainer = torch.optim.Adadelta(net4.parameters(), lr=4)\n",
        "loss = loss_NLP\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net4(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net4)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()\n"
      ],
      "id": "dc3ab5a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:41:51.302164Z",
          "iopub.status.busy": "2022-07-13T08:41:51.301541Z",
          "iopub.status.idle": "2022-07-13T08:41:51.320573Z",
          "shell.execute_reply": "2022-07-13T08:41:51.319800Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "9e0d576e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44eb1ee4-b6da-436b-a2ad-01aed9b20f6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'levantose a la mantas de la ma'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "net4.predict('levantose ', 20, vocab)"
      ],
      "id": "9e0d576e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "033036f2"
      },
      "source": [
        "# Gated Recurrent Units (GRU)\n",
        "\n",
        "La arquitectura de LSTM es una arquitectura de la década de 1990. Sin embargo, en 2014, de desarrolló una alternativa más simple que LSTM y que tiene un comportamiento similar. Esta es GRU.\n",
        "\n",
        "### Compuertas lógicas\n",
        "\n",
        "Al igual que LSTM, GRU tambien usa unas compuertas lógicas con salidas entre 0 y 1\n",
        "\n",
        "![Computing the reset gate and the update gate in a GRU model.](https://d2l.ai/_images/gru-1.svg)\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n",
        "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### Candidato de variable oculta\n",
        "\n",
        "Luego, en lugar de calcular el valor de una celda de memoria, GRU directamente propone un nuevo valor de variable oculta.\n",
        "\n",
        "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$\n",
        "\n",
        "En donde, si $\\mathbf{R}_t = 0$ ignoramos o *reseteamos* la varaible oculta. Pero si $\\mathbf{R}_t =1$ conservamos toda su información.\n",
        "\n",
        "![](https://d2l.ai/_images/gru-2.svg)\n",
        "\n",
        "Destacamos que hemos vuelto a usar el producto de Haddamar o producto elemento a elemento.\n",
        "\n",
        "### Variable oculta\n",
        "\n",
        "Finalmente usamos nuestra otra compurta lógica para definir que tanta importación le damos al candidato nuevo actual con respecto al valor anteior anterior.\n",
        "\n",
        "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
        "\n",
        "Es decir, si $\\mathbf{Z}_t = 1$  conservamos el valor anteior y no actualizamos nuestra variable. Pero si $\\mathbf{Z}_t = 0$, ignoramos el valor anterior y conservamos al candidato.\n",
        "\n",
        "![](https://d2l.ai/_images/gru-3.svg)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "033036f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "b39c1933"
      },
      "source": [
        "## Implementación de GRU desde 0"
      ],
      "id": "b39c1933"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:55:27.246509Z",
          "iopub.status.busy": "2022-07-13T08:55:27.245839Z",
          "iopub.status.idle": "2022-07-13T08:55:27.251929Z",
          "shell.execute_reply": "2022-07-13T08:55:27.251118Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "15b28ef2"
      },
      "outputs": [],
      "source": [
        "class GRUScratch(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        super().__init__()\n",
        "\n",
        "        init_weight = lambda *shape: torch.nn.Parameter(torch.randn(*shape) * 0.01)\n",
        "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
        "                          init_weight(num_hiddens, num_hiddens),\n",
        "                          torch.nn.Parameter(torch.zeros(num_hiddens)))\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_inputs = num_inputs\n",
        "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
        "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
        "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
        "\n",
        "    def forward(self, inputs, H=None):\n",
        "        matmul_H = lambda A, B: torch.matmul(A, B) if H is not None else 0\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) + (\n",
        "                torch.matmul(H, self.W_hz) if H is not None else 0) + self.b_z)\n",
        "            if H is None: H = torch.zeros_like(Z)\n",
        "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
        "                            torch.matmul(H, self.W_hr) + self.b_r)\n",
        "            # R * H es otro producto de Haddamar!!\n",
        "            H_tilda = torch.tanh(torch.matmul(X, self.W_xh) +\n",
        "                              torch.matmul(R * H, self.W_hh) + self.b_h)\n",
        "            H = Z * H + (1 - Z) * H_tilda # Mas prod. de Haddamar!\n",
        "            outputs.append(H)\n",
        "        return outputs, H"
      ],
      "id": "15b28ef2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "1475048b"
      },
      "source": [
        "### Entrenamiento\n"
      ],
      "id": "1475048b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:55:27.266705Z",
          "iopub.status.busy": "2022-07-13T08:55:27.265961Z",
          "iopub.status.idle": "2022-07-13T08:56:47.714805Z",
          "shell.execute_reply": "2022-07-13T08:56:47.713801Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "26b111d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6471de-f548-4900-9a5b-deeb38c162d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 3.153552,\n",
            "    train perplexity 23.419109,\n",
            "    test perplexity 17.598305.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.843293,\n",
            "    train perplexity 17.172220,\n",
            "    test perplexity 16.723484.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.763442,\n",
            "    train perplexity 15.854324,\n",
            "    test perplexity 14.773314.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.590887,\n",
            "    train perplexity 13.341599,\n",
            "    test perplexity 12.117282.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.421953,\n",
            "    train perplexity 11.267843,\n",
            "    test perplexity 10.668628.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.328208,\n",
            "    train perplexity 10.259538,\n",
            "    test perplexity 9.914872.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.261597,\n",
            "    train perplexity 9.598407,\n",
            "    test perplexity 9.268740.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.211366,\n",
            "    train perplexity 9.128179,\n",
            "    test perplexity 8.863169.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.170749,\n",
            "    train perplexity 8.764846,\n",
            "    test perplexity 8.586558.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.138959,\n",
            "    train perplexity 8.490598,\n",
            "    test perplexity 8.379647.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.114616,\n",
            "    train perplexity 8.286401,\n",
            "    test perplexity 8.211603.\n",
            "\n",
            "epoch 12\n",
            "    loss 2.086663,\n",
            "    train perplexity 8.057984,\n",
            "    test perplexity 7.974918.\n",
            "\n",
            "epoch 13\n",
            "    loss 2.062006,\n",
            "    train perplexity 7.861721,\n",
            "    test perplexity 7.826727.\n",
            "\n",
            "epoch 14\n",
            "    loss 2.041099,\n",
            "    train perplexity 7.699066,\n",
            "    test perplexity 7.533711.\n",
            "\n",
            "epoch 15\n",
            "    loss 2.017975,\n",
            "    train perplexity 7.523072,\n",
            "    test perplexity 7.498556.\n",
            "\n",
            "epoch 16\n",
            "    loss 1.993718,\n",
            "    train perplexity 7.342780,\n",
            "    test perplexity 7.324958.\n",
            "\n",
            "epoch 17\n",
            "    loss 1.978744,\n",
            "    train perplexity 7.233651,\n",
            "    test perplexity 7.285919.\n",
            "\n",
            "epoch 18\n",
            "    loss 1.959464,\n",
            "    train perplexity 7.095526,\n",
            "    test perplexity 7.066872.\n",
            "\n",
            "epoch 19\n",
            "    loss 1.944127,\n",
            "    train perplexity 6.987532,\n",
            "    test perplexity 6.889004.\n",
            "\n",
            "epoch 20\n",
            "    loss 1.929321,\n",
            "    train perplexity 6.884832,\n",
            "    test perplexity 6.849345.\n",
            "\n",
            "epoch 21\n",
            "    loss 1.914024,\n",
            "    train perplexity 6.780320,\n",
            "    test perplexity 6.766822.\n",
            "\n",
            "epoch 22\n",
            "    loss 1.903014,\n",
            "    train perplexity 6.706075,\n",
            "    test perplexity 6.658367.\n",
            "\n",
            "epoch 23\n",
            "    loss 1.890097,\n",
            "    train perplexity 6.620010,\n",
            "    test perplexity 6.632054.\n",
            "\n",
            "epoch 24\n",
            "    loss 1.876503,\n",
            "    train perplexity 6.530625,\n",
            "    test perplexity 6.519930.\n",
            "\n",
            "epoch 25\n",
            "    loss 1.866741,\n",
            "    train perplexity 6.467185,\n",
            "    test perplexity 6.350042.\n",
            "\n",
            "epoch 26\n",
            "    loss 1.854260,\n",
            "    train perplexity 6.386971,\n",
            "    test perplexity 6.366450.\n",
            "\n",
            "epoch 27\n",
            "    loss 1.845690,\n",
            "    train perplexity 6.332466,\n",
            "    test perplexity 6.348032.\n",
            "\n",
            "epoch 28\n",
            "    loss 1.834747,\n",
            "    train perplexity 6.263547,\n",
            "    test perplexity 6.228025.\n",
            "\n",
            "epoch 29\n",
            "    loss 1.824125,\n",
            "    train perplexity 6.197370,\n",
            "    test perplexity 6.166739.\n",
            "\n",
            "epoch 30\n",
            "    loss 1.815920,\n",
            "    train perplexity 6.146727,\n",
            "    test perplexity 6.152195.\n",
            "\n",
            "epoch 31\n",
            "    loss 1.806917,\n",
            "    train perplexity 6.091636,\n",
            "    test perplexity 6.064338.\n",
            "\n",
            "epoch 32\n",
            "    loss 1.798208,\n",
            "    train perplexity 6.038818,\n",
            "    test perplexity 5.962721.\n",
            "\n",
            "epoch 33\n",
            "    loss 1.788095,\n",
            "    train perplexity 5.978051,\n",
            "    test perplexity 5.914168.\n",
            "\n",
            "epoch 34\n",
            "    loss 1.778230,\n",
            "    train perplexity 5.919370,\n",
            "    test perplexity 5.949411.\n",
            "\n",
            "epoch 35\n",
            "    loss 1.768422,\n",
            "    train perplexity 5.861596,\n",
            "    test perplexity 5.755071.\n",
            "\n",
            "epoch 36\n",
            "    loss 1.761325,\n",
            "    train perplexity 5.820146,\n",
            "    test perplexity 5.841923.\n",
            "\n",
            "epoch 37\n",
            "    loss 1.753624,\n",
            "    train perplexity 5.775496,\n",
            "    test perplexity 5.673960.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.742876,\n",
            "    train perplexity 5.713753,\n",
            "    test perplexity 5.737633.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.736726,\n",
            "    train perplexity 5.678719,\n",
            "    test perplexity 5.652152.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.727495,\n",
            "    train perplexity 5.626540,\n",
            "    test perplexity 5.660522.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.718493,\n",
            "    train perplexity 5.576118,\n",
            "    test perplexity 5.562661.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.712394,\n",
            "    train perplexity 5.542213,\n",
            "    test perplexity 5.554974.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.704416,\n",
            "    train perplexity 5.498171,\n",
            "    test perplexity 5.435397.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.700266,\n",
            "    train perplexity 5.475403,\n",
            "    test perplexity 5.440147.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.689992,\n",
            "    train perplexity 5.419438,\n",
            "    test perplexity 5.385819.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.682989,\n",
            "    train perplexity 5.381618,\n",
            "    test perplexity 5.339380.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.679487,\n",
            "    train perplexity 5.362802,\n",
            "    test perplexity 5.382017.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.670941,\n",
            "    train perplexity 5.317167,\n",
            "    test perplexity 5.264462.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.666905,\n",
            "    train perplexity 5.295754,\n",
            "    test perplexity 5.288335.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.657437,\n",
            "    train perplexity 5.245846,\n",
            "    test perplexity 5.259459.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.654044,\n",
            "    train perplexity 5.228082,\n",
            "    test perplexity 5.109931.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.645595,\n",
            "    train perplexity 5.184092,\n",
            "    test perplexity 5.126878.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.642845,\n",
            "    train perplexity 5.169859,\n",
            "    test perplexity 5.087299.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.636882,\n",
            "    train perplexity 5.139123,\n",
            "    test perplexity 5.104893.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.625669,\n",
            "    train perplexity 5.081820,\n",
            "    test perplexity 5.036142.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.625591,\n",
            "    train perplexity 5.081419,\n",
            "    test perplexity 5.108939.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.617903,\n",
            "    train perplexity 5.042503,\n",
            "    test perplexity 5.004602.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.611482,\n",
            "    train perplexity 5.010229,\n",
            "    test perplexity 5.107834.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.609032,\n",
            "    train perplexity 4.997973,\n",
            "    test perplexity 4.909565.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.602861,\n",
            "    train perplexity 4.967222,\n",
            "    test perplexity 4.883026.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.596603,\n",
            "    train perplexity 4.936236,\n",
            "    test perplexity 4.881537.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.592864,\n",
            "    train perplexity 4.917814,\n",
            "    test perplexity 4.883326.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.586753,\n",
            "    train perplexity 4.887854,\n",
            "    test perplexity 4.846778.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.582808,\n",
            "    train perplexity 4.868607,\n",
            "    test perplexity 4.856712.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.580308,\n",
            "    train perplexity 4.856452,\n",
            "    test perplexity 4.849155.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.572504,\n",
            "    train perplexity 4.818699,\n",
            "    test perplexity 4.758304.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.571203,\n",
            "    train perplexity 4.812433,\n",
            "    test perplexity 4.765589.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.564655,\n",
            "    train perplexity 4.781023,\n",
            "    test perplexity 4.747016.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.560686,\n",
            "    train perplexity 4.762087,\n",
            "    test perplexity 4.797225.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.555340,\n",
            "    train perplexity 4.736697,\n",
            "    test perplexity 4.794712.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.552630,\n",
            "    train perplexity 4.723880,\n",
            "    test perplexity 4.726280.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.548138,\n",
            "    train perplexity 4.702704,\n",
            "    test perplexity 4.701381.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.545661,\n",
            "    train perplexity 4.691072,\n",
            "    test perplexity 4.654873.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.537525,\n",
            "    train perplexity 4.653060,\n",
            "    test perplexity 4.675888.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.536366,\n",
            "    train perplexity 4.647669,\n",
            "    test perplexity 4.662241.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.533886,\n",
            "    train perplexity 4.636156,\n",
            "    test perplexity 4.649971.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.527873,\n",
            "    train perplexity 4.608366,\n",
            "    test perplexity 4.584220.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.523674,\n",
            "    train perplexity 4.589053,\n",
            "    test perplexity 4.698828.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.525065,\n",
            "    train perplexity 4.595443,\n",
            "    test perplexity 4.604376.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.518053,\n",
            "    train perplexity 4.563334,\n",
            "    test perplexity 4.523601.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.513031,\n",
            "    train perplexity 4.540471,\n",
            "    test perplexity 4.507994.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.512176,\n",
            "    train perplexity 4.536593,\n",
            "    test perplexity 4.564290.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.507674,\n",
            "    train perplexity 4.516215,\n",
            "    test perplexity 4.475062.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.506830,\n",
            "    train perplexity 4.512402,\n",
            "    test perplexity 4.523682.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.504709,\n",
            "    train perplexity 4.502845,\n",
            "    test perplexity 4.452351.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.497451,\n",
            "    train perplexity 4.470279,\n",
            "    test perplexity 4.476763.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.494406,\n",
            "    train perplexity 4.456688,\n",
            "    test perplexity 4.468818.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.493399,\n",
            "    train perplexity 4.452205,\n",
            "    test perplexity 4.451450.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.494108,\n",
            "    train perplexity 4.455359,\n",
            "    test perplexity 4.449797.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.484418,\n",
            "    train perplexity 4.412395,\n",
            "    test perplexity 4.354167.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.483446,\n",
            "    train perplexity 4.408112,\n",
            "    test perplexity 4.386680.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.482953,\n",
            "    train perplexity 4.405936,\n",
            "    test perplexity 4.452864.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.481318,\n",
            "    train perplexity 4.398740,\n",
            "    test perplexity 4.376301.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.478611,\n",
            "    train perplexity 4.386847,\n",
            "    test perplexity 4.355805.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.472901,\n",
            "    train perplexity 4.361871,\n",
            "    test perplexity 4.311585.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.471656,\n",
            "    train perplexity 4.356444,\n",
            "    test perplexity 4.379514.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.470656,\n",
            "    train perplexity 4.352088,\n",
            "    test perplexity 4.386024.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.467339,\n",
            "    train perplexity 4.337678,\n",
            "    test perplexity 4.349865.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.464317,\n",
            "    train perplexity 4.324591,\n",
            "    test perplexity 4.288945.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.463283,\n",
            "    train perplexity 4.320117,\n",
            "    test perplexity 4.304298.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gru_scrt = GRUScratch(num_inputs=len(vocab), num_hiddens=32)\n",
        "net5 = RNNLMScratch(gru_scrt, vocab_size=len(vocab))\n",
        "trainer = torch.optim.Adadelta(net5.parameters(), lr=4)\n",
        "loss = loss_NLP\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net5(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net5)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()"
      ],
      "id": "26b111d1"
    },
    {
      "cell_type": "code",
      "source": [
        "net5.predict(\"levantose \", 30, vocab)"
      ],
      "metadata": {
        "id": "szXzzsbdlgEs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "877bde0a-00f2-46c8-98e7-7fde5aee0cce"
      },
      "id": "szXzzsbdlgEs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'levantose en el mucho y a su parecer en '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "efbd713d"
      },
      "source": [
        "## Implementación concisa"
      ],
      "id": "efbd713d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:56:47.723067Z",
          "iopub.status.busy": "2022-07-13T08:56:47.722418Z",
          "iopub.status.idle": "2022-07-13T08:56:47.727677Z",
          "shell.execute_reply": "2022-07-13T08:56:47.726645Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "66363a77"
      },
      "outputs": [],
      "source": [
        "class GRU(RNN):\n",
        "    def __init__(self, num_inputs, num_hiddens):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.rnn = torch.nn.GRU(num_inputs, num_hiddens)"
      ],
      "id": "66363a77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:56:47.731844Z",
          "iopub.status.busy": "2022-07-13T08:56:47.731080Z",
          "iopub.status.idle": "2022-07-13T08:57:40.076047Z",
          "shell.execute_reply": "2022-07-13T08:57:40.075231Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "5dda1a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a9d3f1-7a43-4134-d3fb-a28813f46195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "    loss 2.996911,\n",
            "    train perplexity 20.023594,\n",
            "    test perplexity 16.715647.\n",
            "\n",
            "epoch 2\n",
            "    loss 2.685000,\n",
            "    train perplexity 14.658208,\n",
            "    test perplexity 12.532542.\n",
            "\n",
            "epoch 3\n",
            "    loss 2.425869,\n",
            "    train perplexity 11.312056,\n",
            "    test perplexity 10.439892.\n",
            "\n",
            "epoch 4\n",
            "    loss 2.286630,\n",
            "    train perplexity 9.841719,\n",
            "    test perplexity 9.436526.\n",
            "\n",
            "epoch 5\n",
            "    loss 2.216136,\n",
            "    train perplexity 9.171826,\n",
            "    test perplexity 8.849507.\n",
            "\n",
            "epoch 6\n",
            "    loss 2.164240,\n",
            "    train perplexity 8.707982,\n",
            "    test perplexity 8.512407.\n",
            "\n",
            "epoch 7\n",
            "    loss 2.127467,\n",
            "    train perplexity 8.393582,\n",
            "    test perplexity 8.233907.\n",
            "\n",
            "epoch 8\n",
            "    loss 2.091887,\n",
            "    train perplexity 8.100182,\n",
            "    test perplexity 7.954741.\n",
            "\n",
            "epoch 9\n",
            "    loss 2.062250,\n",
            "    train perplexity 7.863642,\n",
            "    test perplexity 7.702979.\n",
            "\n",
            "epoch 10\n",
            "    loss 2.032354,\n",
            "    train perplexity 7.632034,\n",
            "    test perplexity 7.624239.\n",
            "\n",
            "epoch 11\n",
            "    loss 2.006264,\n",
            "    train perplexity 7.435490,\n",
            "    test perplexity 7.454623.\n",
            "\n",
            "epoch 12\n",
            "    loss 1.981032,\n",
            "    train perplexity 7.250220,\n",
            "    test perplexity 7.105975.\n",
            "\n",
            "epoch 13\n",
            "    loss 1.957195,\n",
            "    train perplexity 7.079443,\n",
            "    test perplexity 7.003142.\n",
            "\n",
            "epoch 14\n",
            "    loss 1.940930,\n",
            "    train perplexity 6.965226,\n",
            "    test perplexity 6.875650.\n",
            "\n",
            "epoch 15\n",
            "    loss 1.920641,\n",
            "    train perplexity 6.825332,\n",
            "    test perplexity 6.784597.\n",
            "\n",
            "epoch 16\n",
            "    loss 1.903858,\n",
            "    train perplexity 6.711741,\n",
            "    test perplexity 6.621181.\n",
            "\n",
            "epoch 17\n",
            "    loss 1.887100,\n",
            "    train perplexity 6.600204,\n",
            "    test perplexity 6.540121.\n",
            "\n",
            "epoch 18\n",
            "    loss 1.873213,\n",
            "    train perplexity 6.509176,\n",
            "    test perplexity 6.392303.\n",
            "\n",
            "epoch 19\n",
            "    loss 1.854164,\n",
            "    train perplexity 6.386356,\n",
            "    test perplexity 6.371558.\n",
            "\n",
            "epoch 20\n",
            "    loss 1.847286,\n",
            "    train perplexity 6.342579,\n",
            "    test perplexity 6.363527.\n",
            "\n",
            "epoch 21\n",
            "    loss 1.831082,\n",
            "    train perplexity 6.240634,\n",
            "    test perplexity 6.244940.\n",
            "\n",
            "epoch 22\n",
            "    loss 1.821120,\n",
            "    train perplexity 6.178776,\n",
            "    test perplexity 6.142768.\n",
            "\n",
            "epoch 23\n",
            "    loss 1.806319,\n",
            "    train perplexity 6.087999,\n",
            "    test perplexity 6.018593.\n",
            "\n",
            "epoch 24\n",
            "    loss 1.798409,\n",
            "    train perplexity 6.040027,\n",
            "    test perplexity 6.010517.\n",
            "\n",
            "epoch 25\n",
            "    loss 1.786318,\n",
            "    train perplexity 5.967443,\n",
            "    test perplexity 5.983117.\n",
            "\n",
            "epoch 26\n",
            "    loss 1.776561,\n",
            "    train perplexity 5.909501,\n",
            "    test perplexity 5.910085.\n",
            "\n",
            "epoch 27\n",
            "    loss 1.766431,\n",
            "    train perplexity 5.849939,\n",
            "    test perplexity 5.912456.\n",
            "\n",
            "epoch 28\n",
            "    loss 1.755662,\n",
            "    train perplexity 5.787276,\n",
            "    test perplexity 5.724391.\n",
            "\n",
            "epoch 29\n",
            "    loss 1.744164,\n",
            "    train perplexity 5.721115,\n",
            "    test perplexity 5.729762.\n",
            "\n",
            "epoch 30\n",
            "    loss 1.739767,\n",
            "    train perplexity 5.696015,\n",
            "    test perplexity 5.695616.\n",
            "\n",
            "epoch 31\n",
            "    loss 1.725624,\n",
            "    train perplexity 5.616026,\n",
            "    test perplexity 5.603449.\n",
            "\n",
            "epoch 32\n",
            "    loss 1.716505,\n",
            "    train perplexity 5.565045,\n",
            "    test perplexity 5.464019.\n",
            "\n",
            "epoch 33\n",
            "    loss 1.712033,\n",
            "    train perplexity 5.540212,\n",
            "    test perplexity 5.484822.\n",
            "\n",
            "epoch 34\n",
            "    loss 1.702849,\n",
            "    train perplexity 5.489564,\n",
            "    test perplexity 5.483429.\n",
            "\n",
            "epoch 35\n",
            "    loss 1.695503,\n",
            "    train perplexity 5.449386,\n",
            "    test perplexity 5.440507.\n",
            "\n",
            "epoch 36\n",
            "    loss 1.683418,\n",
            "    train perplexity 5.383924,\n",
            "    test perplexity 5.405698.\n",
            "\n",
            "epoch 37\n",
            "    loss 1.677320,\n",
            "    train perplexity 5.351195,\n",
            "    test perplexity 5.335610.\n",
            "\n",
            "epoch 38\n",
            "    loss 1.672492,\n",
            "    train perplexity 5.325424,\n",
            "    test perplexity 5.348231.\n",
            "\n",
            "epoch 39\n",
            "    loss 1.660426,\n",
            "    train perplexity 5.261554,\n",
            "    test perplexity 5.300770.\n",
            "\n",
            "epoch 40\n",
            "    loss 1.655849,\n",
            "    train perplexity 5.237527,\n",
            "    test perplexity 5.249199.\n",
            "\n",
            "epoch 41\n",
            "    loss 1.653440,\n",
            "    train perplexity 5.224925,\n",
            "    test perplexity 5.204574.\n",
            "\n",
            "epoch 42\n",
            "    loss 1.641126,\n",
            "    train perplexity 5.160979,\n",
            "    test perplexity 5.135737.\n",
            "\n",
            "epoch 43\n",
            "    loss 1.635138,\n",
            "    train perplexity 5.130165,\n",
            "    test perplexity 5.088174.\n",
            "\n",
            "epoch 44\n",
            "    loss 1.629416,\n",
            "    train perplexity 5.100893,\n",
            "    test perplexity 5.091053.\n",
            "\n",
            "epoch 45\n",
            "    loss 1.629423,\n",
            "    train perplexity 5.100931,\n",
            "    test perplexity 5.040555.\n",
            "\n",
            "epoch 46\n",
            "    loss 1.618826,\n",
            "    train perplexity 5.047160,\n",
            "    test perplexity 5.027041.\n",
            "\n",
            "epoch 47\n",
            "    loss 1.611298,\n",
            "    train perplexity 5.009310,\n",
            "    test perplexity 4.991121.\n",
            "\n",
            "epoch 48\n",
            "    loss 1.609921,\n",
            "    train perplexity 5.002415,\n",
            "    test perplexity 4.909197.\n",
            "\n",
            "epoch 49\n",
            "    loss 1.607624,\n",
            "    train perplexity 4.990939,\n",
            "    test perplexity 5.033241.\n",
            "\n",
            "epoch 50\n",
            "    loss 1.598280,\n",
            "    train perplexity 4.944523,\n",
            "    test perplexity 4.932590.\n",
            "\n",
            "epoch 51\n",
            "    loss 1.593152,\n",
            "    train perplexity 4.919231,\n",
            "    test perplexity 4.984792.\n",
            "\n",
            "epoch 52\n",
            "    loss 1.589186,\n",
            "    train perplexity 4.899761,\n",
            "    test perplexity 4.919560.\n",
            "\n",
            "epoch 53\n",
            "    loss 1.588738,\n",
            "    train perplexity 4.897563,\n",
            "    test perplexity 4.843237.\n",
            "\n",
            "epoch 54\n",
            "    loss 1.581954,\n",
            "    train perplexity 4.864452,\n",
            "    test perplexity 4.777364.\n",
            "\n",
            "epoch 55\n",
            "    loss 1.576568,\n",
            "    train perplexity 4.838323,\n",
            "    test perplexity 4.891884.\n",
            "\n",
            "epoch 56\n",
            "    loss 1.574995,\n",
            "    train perplexity 4.830717,\n",
            "    test perplexity 4.822548.\n",
            "\n",
            "epoch 57\n",
            "    loss 1.569886,\n",
            "    train perplexity 4.806100,\n",
            "    test perplexity 4.776503.\n",
            "\n",
            "epoch 58\n",
            "    loss 1.563681,\n",
            "    train perplexity 4.776372,\n",
            "    test perplexity 4.832943.\n",
            "\n",
            "epoch 59\n",
            "    loss 1.563893,\n",
            "    train perplexity 4.777381,\n",
            "    test perplexity 4.759604.\n",
            "\n",
            "epoch 60\n",
            "    loss 1.558924,\n",
            "    train perplexity 4.753706,\n",
            "    test perplexity 4.843454.\n",
            "\n",
            "epoch 61\n",
            "    loss 1.555866,\n",
            "    train perplexity 4.739190,\n",
            "    test perplexity 4.671595.\n",
            "\n",
            "epoch 62\n",
            "    loss 1.552038,\n",
            "    train perplexity 4.721083,\n",
            "    test perplexity 4.781755.\n",
            "\n",
            "epoch 63\n",
            "    loss 1.551548,\n",
            "    train perplexity 4.718768,\n",
            "    test perplexity 4.740389.\n",
            "\n",
            "epoch 64\n",
            "    loss 1.543897,\n",
            "    train perplexity 4.682803,\n",
            "    test perplexity 4.691797.\n",
            "\n",
            "epoch 65\n",
            "    loss 1.542886,\n",
            "    train perplexity 4.678073,\n",
            "    test perplexity 4.720191.\n",
            "\n",
            "epoch 66\n",
            "    loss 1.537188,\n",
            "    train perplexity 4.651491,\n",
            "    test perplexity 4.639112.\n",
            "\n",
            "epoch 67\n",
            "    loss 1.540366,\n",
            "    train perplexity 4.666300,\n",
            "    test perplexity 4.722024.\n",
            "\n",
            "epoch 68\n",
            "    loss 1.538100,\n",
            "    train perplexity 4.655736,\n",
            "    test perplexity 4.600206.\n",
            "\n",
            "epoch 69\n",
            "    loss 1.528730,\n",
            "    train perplexity 4.612316,\n",
            "    test perplexity 4.632687.\n",
            "\n",
            "epoch 70\n",
            "    loss 1.528967,\n",
            "    train perplexity 4.613411,\n",
            "    test perplexity 4.586331.\n",
            "\n",
            "epoch 71\n",
            "    loss 1.524571,\n",
            "    train perplexity 4.593174,\n",
            "    test perplexity 4.579414.\n",
            "\n",
            "epoch 72\n",
            "    loss 1.519076,\n",
            "    train perplexity 4.568003,\n",
            "    test perplexity 4.480935.\n",
            "\n",
            "epoch 73\n",
            "    loss 1.522463,\n",
            "    train perplexity 4.583499,\n",
            "    test perplexity 4.599107.\n",
            "\n",
            "epoch 74\n",
            "    loss 1.516614,\n",
            "    train perplexity 4.556770,\n",
            "    test perplexity 4.526615.\n",
            "\n",
            "epoch 75\n",
            "    loss 1.515960,\n",
            "    train perplexity 4.553791,\n",
            "    test perplexity 4.503514.\n",
            "\n",
            "epoch 76\n",
            "    loss 1.512632,\n",
            "    train perplexity 4.538661,\n",
            "    test perplexity 4.558209.\n",
            "\n",
            "epoch 77\n",
            "    loss 1.514247,\n",
            "    train perplexity 4.545996,\n",
            "    test perplexity 4.562887.\n",
            "\n",
            "epoch 78\n",
            "    loss 1.507254,\n",
            "    train perplexity 4.514316,\n",
            "    test perplexity 4.550314.\n",
            "\n",
            "epoch 79\n",
            "    loss 1.500585,\n",
            "    train perplexity 4.484313,\n",
            "    test perplexity 4.536783.\n",
            "\n",
            "epoch 80\n",
            "    loss 1.500939,\n",
            "    train perplexity 4.485900,\n",
            "    test perplexity 4.450533.\n",
            "\n",
            "epoch 81\n",
            "    loss 1.506267,\n",
            "    train perplexity 4.509866,\n",
            "    test perplexity 4.518363.\n",
            "\n",
            "epoch 82\n",
            "    loss 1.497085,\n",
            "    train perplexity 4.468643,\n",
            "    test perplexity 4.488873.\n",
            "\n",
            "epoch 83\n",
            "    loss 1.493458,\n",
            "    train perplexity 4.452464,\n",
            "    test perplexity 4.483041.\n",
            "\n",
            "epoch 84\n",
            "    loss 1.492053,\n",
            "    train perplexity 4.446216,\n",
            "    test perplexity 4.396665.\n",
            "\n",
            "epoch 85\n",
            "    loss 1.490739,\n",
            "    train perplexity 4.440374,\n",
            "    test perplexity 4.426117.\n",
            "\n",
            "epoch 86\n",
            "    loss 1.490088,\n",
            "    train perplexity 4.437487,\n",
            "    test perplexity 4.458566.\n",
            "\n",
            "epoch 87\n",
            "    loss 1.486358,\n",
            "    train perplexity 4.420963,\n",
            "    test perplexity 4.436260.\n",
            "\n",
            "epoch 88\n",
            "    loss 1.484783,\n",
            "    train perplexity 4.414009,\n",
            "    test perplexity 4.357737.\n",
            "\n",
            "epoch 89\n",
            "    loss 1.479886,\n",
            "    train perplexity 4.392443,\n",
            "    test perplexity 4.406186.\n",
            "\n",
            "epoch 90\n",
            "    loss 1.483045,\n",
            "    train perplexity 4.406342,\n",
            "    test perplexity 4.401596.\n",
            "\n",
            "epoch 91\n",
            "    loss 1.477357,\n",
            "    train perplexity 4.381351,\n",
            "    test perplexity 4.321055.\n",
            "\n",
            "epoch 92\n",
            "    loss 1.476343,\n",
            "    train perplexity 4.376908,\n",
            "    test perplexity 4.457900.\n",
            "\n",
            "epoch 93\n",
            "    loss 1.474940,\n",
            "    train perplexity 4.370773,\n",
            "    test perplexity 4.368808.\n",
            "\n",
            "epoch 94\n",
            "    loss 1.474463,\n",
            "    train perplexity 4.368690,\n",
            "    test perplexity 4.380787.\n",
            "\n",
            "epoch 95\n",
            "    loss 1.468609,\n",
            "    train perplexity 4.343189,\n",
            "    test perplexity 4.286490.\n",
            "\n",
            "epoch 96\n",
            "    loss 1.469869,\n",
            "    train perplexity 4.348664,\n",
            "    test perplexity 4.311930.\n",
            "\n",
            "epoch 97\n",
            "    loss 1.464615,\n",
            "    train perplexity 4.325878,\n",
            "    test perplexity 4.398400.\n",
            "\n",
            "epoch 98\n",
            "    loss 1.464973,\n",
            "    train perplexity 4.327425,\n",
            "    test perplexity 4.295727.\n",
            "\n",
            "epoch 99\n",
            "    loss 1.465184,\n",
            "    train perplexity 4.328340,\n",
            "    test perplexity 4.297966.\n",
            "\n",
            "epoch 100\n",
            "    loss 1.462551,\n",
            "    train perplexity 4.316958,\n",
            "    test perplexity 4.358413.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gru = GRU(num_inputs=len(vocab), num_hiddens=32)\n",
        "net6 = RNNLM(gru, vocab_size=len(vocab))\n",
        "trainer = torch.optim.Adadelta(net6.parameters(), lr=4)\n",
        "loss = loss_NLP\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    L = 0.0\n",
        "    N = 0\n",
        "    TestN = 0\n",
        "    TestL = 0\n",
        "    for X, Y in train_iter:\n",
        "        l = loss(net6(X), Y)\n",
        "        trainer.zero_grad()\n",
        "        l.mean().backward()\n",
        "        clip_gradients(grad_clip_val = 1, model = net6)\n",
        "        trainer.step()\n",
        "        L += l.sum()\n",
        "        N += l.numel()\n",
        "    for X, Y in test_iter:\n",
        "        TestL += l.sum()\n",
        "        TestN += Y.numel()\n",
        "    print(f'epoch {epoch + 1}')\n",
        "    print(f'    loss {float(L/N):f},')\n",
        "    print(f'    train perplexity {torch.exp((L/N)):f},')\n",
        "    print(f'    test perplexity {torch.exp((TestL/TestN)):f}.')\n",
        "    print()"
      ],
      "id": "5dda1a27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:57:40.090900Z",
          "iopub.status.busy": "2022-07-13T08:57:40.090564Z",
          "iopub.status.idle": "2022-07-13T08:57:40.109666Z",
          "shell.execute_reply": "2022-07-13T08:57:40.108875Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "80bcd4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ead0479-ce0f-49f7-f80a-e7a4266f3f96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'avellaneda se hallar de la manc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "net6.predict('avellaneda ', 20, vocab)"
      ],
      "id": "80bcd4e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "DDclW7Malvrv"
      },
      "source": [
        "# Redes bidireccionales y profundas\n",
        "\n",
        "En general, vamos a ver que muchas veces tener una variable oculta lineal o generada por una sola capa, puede no capturar toda la complejidad de nuestro modelo. Es por esto que debemos tener alguna herramienta que nos permita generar un estado oculto mucho más complejo. Para esto tenemos las redes recurrentes profundas.\n",
        "\n",
        "En líneas generales, la idea será usar sucesivos redes recurrentes a la salida de nuestra primera capa con estados ocultos. Como muestra la figura\n",
        "\n",
        "![](https://d2l.ai/_images/deep-rnn.svg)\n",
        "\n",
        "Afortunadamente este tipo de arquitecturas podemos llamarlas solo agregando un parametro a nuestro código."
      ],
      "id": "DDclW7Malvrv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.550820Z",
          "iopub.status.busy": "2022-07-13T08:40:58.550482Z",
          "iopub.status.idle": "2022-07-13T08:40:58.556222Z",
          "shell.execute_reply": "2022-07-13T08:40:58.555331Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "Z7i0-uiSlvrv"
      },
      "outputs": [],
      "source": [
        "class LSTMDeep(RNN):\n",
        "    def __init__(self, num_inputs, num_hiddens,num_layers):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.rnn = torch.nn.LSTM(num_inputs, num_hiddens,num_layers)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        return self.rnn(inputs, H_C)"
      ],
      "id": "Z7i0-uiSlvrv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.559697Z",
          "iopub.status.busy": "2022-07-13T08:40:58.559412Z",
          "iopub.status.idle": "2022-07-13T08:41:51.293011Z",
          "shell.execute_reply": "2022-07-13T08:41:51.292187Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "catZCoP8lvrw"
      },
      "outputs": [],
      "source": [
        "lstmD = LSTMDeep(num_inputs=len(vocab), num_hiddens=32, num_layers=2)\n",
        "net7 = RNNLM(lstm, vocab_size=len(vocab))"
      ],
      "id": "catZCoP8lvrw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más adelante, veremos que en el problema de traducción, tener información del futuro puede resultar util para los estados presentes. El ejemplo más sencillo es como las palabras se ordenan de manera distinta segun el idioma:\n",
        "\n",
        ">This is a red pencil\n",
        "\n",
        ">Este un lápiz es rojo\n",
        "\n",
        "Por esto también es útil tener los llamados modelos bidireccionales. Estos modelos duplican el número de parametros, pues en esencia entrenan tanto \"hacia adelante\" temporalmente como \"hacia atras\""
      ],
      "metadata": {
        "id": "blBxeJZ3nYkC"
      },
      "id": "blBxeJZ3nYkC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.550820Z",
          "iopub.status.busy": "2022-07-13T08:40:58.550482Z",
          "iopub.status.idle": "2022-07-13T08:40:58.556222Z",
          "shell.execute_reply": "2022-07-13T08:40:58.555331Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "fSQB7xqrosxD"
      },
      "outputs": [],
      "source": [
        "class LSTMBidir(RNN):\n",
        "    def __init__(self, num_inputs, num_hiddens, num_layers, bidirectional):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.rnn = torch.nn.LSTM(num_inputs,\n",
        "                                 num_hiddens, num_layers,\n",
        "                                 bidirectional=bidirectional)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        if bidirectional: self.num_hiddens *= 2\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        return self.rnn(inputs, H_C)"
      ],
      "id": "fSQB7xqrosxD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:40:58.559697Z",
          "iopub.status.busy": "2022-07-13T08:40:58.559412Z",
          "iopub.status.idle": "2022-07-13T08:41:51.293011Z",
          "shell.execute_reply": "2022-07-13T08:41:51.292187Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "uQ999iQ3osxF"
      },
      "outputs": [],
      "source": [
        "lstmD = LSTMBidir(num_inputs=len(vocab), num_hiddens=32,\n",
        "                  num_layers=2, bidirectional=True)\n",
        "net8 = RNNLM(lstm, vocab_size=len(vocab))"
      ],
      "id": "uQ999iQ3osxF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cabe destacar que las redes bidireccionales son un mal modelo para el problema a \"aprender a escribir español letra por letra\". La razón, de alguna manera es la siguiente:\n",
        "\n",
        "* Dada una letra \"q\", ¿cuál es la letra anterior en un texto?\n",
        "* Dada una letra \"q\", ¿cuál es la letra siguiente en un texto?\n",
        "\n",
        "A diferencia de la traducción automática, el deletro tiene mucha más información en la dirección \"hacia adelante\" que hacia atras."
      ],
      "metadata": {
        "id": "hPjdrqGPp5S5"
      },
      "id": "hPjdrqGPp5S5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Introduccion2_y_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}