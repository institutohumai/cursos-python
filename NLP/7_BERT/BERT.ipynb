{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/7_BERT/BERT.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "6xBj2-DpQOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "\n",
        "El aÃ±o 2018 fue  un importante punto de inflexiÃ³n para los modelos de aprendizaje automÃ¡tico que manejan texto (o, mÃ¡s exactamente, Procesamiento del Lenguaje Natural o NLP para abreviar). Nuestra comprensiÃ³n de la mejor forma de representar palabras y oraciones comprendiendo los significados y las relaciones subyacentes estÃ¡ evolucionando rÃ¡pidamente. AdemÃ¡s, la comunidad de NLP ha estado presentando componentes increÃ­blemente poderosos que puedes descargar y usar libremente en tus propios modelos y versiones \n",
        "\n",
        "![](https://jalammar.github.io/images/transformer-ber-ulmfit-elmo.png)\n",
        "\n",
        "ULM-FiT no tiene nada que ver con Cookie Monster pero no se me ocurriÃ³ nada mejor ðŸ™‚\n",
        "\n",
        "Uno de los Ãºltimos hitos en este desarrollo es el lanzamiento de BERT, un evento descrito como el comienzo de una nueva era en la NLP. BERT es un modelo que rompiÃ³ varios rÃ©cords relativos a la forma en la que estos modelos pueden manejar tareas basadas en el lenguaje. Poco despuÃ©s del lanzamiento del documento que presenta Bert, el equipo liberÃ³ el cÃ³digo del modelo y puso a libre disposiciÃ³n la descarga de versiones del modelo pre-entrenadas con conjuntos de datos masivos. Este es un desarrollo trascendental, ya que permite a cualquier persona desarrollar un modelo de aprendizaje automÃ¡tico que involucre procesamiento del lenguaje, usando este motor como un componente fÃ¡cilmente disponible, ahorrando el tiempo, la energÃ­a, el conocimiento y los recursos que habrÃ­a tenido que destinar a entrenar un modelo de procesamiento del lenguaje construido desde cero.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-transfer-learning.png)\n",
        "Estos son los 2 pasos a seguir para usar BERT. Primero descargas el modelo previamente entrenado con datos no anotados (PASO 1) y luego te concentras en ajustarlo (PASO 2). [Fuente de la imagen].\n",
        "\n",
        "BERT se basa en algunas buenas ideas que han ido surgiendo recientemente en la comunidad de NLP, y que incluyen, entre otros, el aprendizaje semi-supervisado (de Andrew Dai y Quoc Le), ELMo (de Matthew Peters e investigadores de AI2 y UW CSE ), ULMFiT (del fundador de fast.ai Jeremy Howard y Sebastian Ruder), transformer OpenAI (de los investigadores de OpenAI Radford, Narasimhan, Salimans y Sutskever) y los Transformer (de Vaswani et alia.) .\n",
        "\n",
        "Conviene conocer bien algunos conceptos esenciales para comprender correctamente quÃ© es BERT. Pero comencemos por ver las formas en que puedes usar BERT, antes de ver los conceptos involucrados en el modelo en sÃ­.\n",
        "\n",
        "## ClasificaciÃ³n de oraciones\n",
        "\n",
        "La forma mÃ¡s directa de emplear BERT es usarlo para clasificar un fragmento de texto. El modelo tendrÃ­a este aspecto:\n",
        "\n",
        "![](https://jalammar.github.io/images/BERT-classification-spam.png)\n",
        "\n",
        "Para entrenar un modelo de este tipo, principalmente se tiene que entrenar el clasificador, con cambios mÃ­nimos en el modelo BERT durante la fase de entrenamiento. Este proceso de entrenamiento se llama Fine-Tunning y tiene sus raÃ­ces en el Aprendizaje Secuencial Semi-supervisado y ULM-FiT.\n",
        "\n",
        "Para las personas que no estÃ¡n versadas en el tema, dado que hablamos de clasificadores, estamos en el dominio del aprendizaje supervisado, dentro del campo del aprendizaje automÃ¡tico. Lo que significarÃ­a que necesitamos un conjunto de datos etiquetados para entrenar dicho modelo. Para este ejemplo de clasificador de spam, el conjunto de datos etiquetado serÃ­a una lista de mensajes de correo electrÃ³nico y una etiqueta (â€œspamâ€ o â€œno spamâ€ para cada mensaje).\n",
        "\n",
        "![](https://jalammar.github.io/images/spam-labeled-dataset.png)\n",
        "\n",
        "Otros ejemplos de este uso podrÃ­an incluir:\n",
        "\n",
        "* AnÃ¡lisis de sentimientos\n",
        "  * Entrada: ReseÃ±a de pelÃ­cula/producto. Salida: Â¿la revisiÃ³n es positiva o negativa?\n",
        "  * Conjunto de datos de ejemplo: SST\n",
        "* ComprobaciÃ³n de hechos\n",
        "  * Entrada: oraciÃ³n. Salida: â€œDeclaraciÃ³nâ€ o â€œNo declaraciÃ³nâ€\n",
        "  * Ejemplo mÃ¡s ambicioso/futurista:\n",
        "      * Entrada: DeclaraciÃ³n. Salida: â€œVerdaderaâ€ o â€œFalsaâ€\n",
        "\n",
        "## Arquitectura del modelo\n",
        "\n",
        "Ahora que sabemos en que podemos aplica BERT, echemos un vistazo mÃ¡s de cerca a cÃ³mo funciona.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-base-bert-large.png)\n",
        "\n",
        "El paper original del BERT presenta dos tamaÃ±os del modelo:\n",
        "\n",
        "* BERT BASE: comparable en tamaÃ±o al Transformer OpenAI (para comparar rendimiento)\n",
        "* BERT LARGE: un modelo ridÃ­culamente enorme que logrÃ³ los increÃ­bles resultados reseÃ±ados en el paper original\n",
        "\n",
        "BERT es bÃ¡sicamente una pila de transformers encoders entrenados. Es posible que sea necesario revisar la clase de Transformes para continuar \n",
        "\n",
        "![](https://jalammar.github.io/images/bert-base-bert-large-encoders.png)\n",
        "\n",
        "Ambos tamaÃ±os de modelos BERT tienen una gran cantidad de capas de encoder: 12 para la versiÃ³n bÃ¡sica y 24 para la versiÃ³n grande. Estos tambiÃ©n tienen densas mÃ¡s grandes (768 y 1024 unidades ocultas respectivamente) y mÃ¡s cabezales de atenciÃ³n (12 y 16 respectivamente) que la configuraciÃ³n predeterminada en la implementaciÃ³n de referencia del Transformer en el paper inicial (6 capas de codificador, 512 unidades ocultas, y 8 cabezales de atenciÃ³n).\n",
        "\n",
        "|CaracterÃ­sticas|Transformer|BERT base|BERT large|\n",
        "|:--|:-:|:-:|:-:|\n",
        "|Cantidad de capas|6|12|24|\n",
        "|Longitud de estados ocultos|512|768|1024|\n",
        "|Cantidad de cabezales de atenciÃ³n|8|12|16|\n",
        "\n",
        "## Entradas del modelo\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-input-output.png)\n",
        "\n",
        "El primer token de entrada viene con un token [CLS] especial por motivos que se aclararÃ¡n mÃ¡s adelante. CLS aquÃ­ significa ClasificaciÃ³n.\n",
        "\n",
        "Al igual que el encoder vanilla del transformer, BERT toma una secuencia de palabras como entrada que avancia hacia arriba en la pila. Cada capa aplica auto atenciÃ³n, pasa sus resultados a travÃ©s de una red de avance y luego los devuelve al siguiente encoder.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-encoders-input.png)\n",
        "\n",
        "En tÃ©rminos de arquitectura, es idÃ©ntica a la del Transformer hasta este punto (aparte del tamaÃ±o, que no deja de ser una configuraciones a elecciÃ³n de cada uno). Es en la salida donde empezamos a ver cÃ³mo cambian las cosas.\n",
        "\n",
        "## Salidas del modelo\n",
        "\n",
        "Cada posiciÃ³n genera un vector de tamaÃ±o hidden_size (768 en BERT Base). Para el ejemplo de clasificaciÃ³n de oraciones que vimos anteriormente, nos enfocamos en la salida de solo la primera posiciÃ³n (a la que le pasamos el token [CLS] especial).\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-output-vector.png)\n",
        "\n",
        "Ese vector ahora se puede usar como entrada para un clasificador de nuestra elecciÃ³n. El documento logra excelentes resultados usando Ãºnicamente una red neuronal de una sola capa como clasificador.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-classifier.png)\n",
        "\n",
        "Si tienes mÃ¡s etiquetas (por ejemplo, si es un servicio de correo electrÃ³nico que etiqueta los correos electrÃ³nicos como â€œspamâ€, â€œno spamâ€, â€œsocialâ€ y â€œpromociÃ³nâ€), simplemente modifica la red clasificadora para tener mÃ¡s neuronas de salida que luego pasar por softmax.\n",
        "\n",
        "## Una nueva era de integraciÃ³n\n",
        "\n",
        "Estos nuevos desarrollos traen consigo un nuevo cambio en la forma en que se codifican las palabras. Hasta ahora, los embeddings de palabras han sido una gran herramienta en la forma en que los principales modelos de NLP tratan el lenguaje. MÃ©todos como Word2Vec y Glove se han utilizado ampliamente para este tipo de tareas. Recapitulemos cÃ³mo se usan antes de seÃ±alar lo que ahora ha cambiado.\n",
        "\n",
        "Resumen de embeddings de palabras\n",
        "\n",
        "Para que las palabras sean procesadas por modelos de aprendizaje automÃ¡tico, necesitan alguna forma de representaciÃ³n numÃ©rica que los modelos puedan usar en sus cÃ¡lculos. Word2Vec demostrÃ³ que podemos usar un vector (una lista de nÃºmeros) para representar correctamente las palabras de una manera que captura las relaciones semÃ¡nticas o relacionadas con el significado (por ejemplo, la capacidad de saber si las palabras son similares u opuestas, o que un par de palabras como â€œEstocolmoâ€ y â€œSueciaâ€ tienen entre ellos la misma relaciÃ³n que tienen entre ellos â€œEl Cairoâ€ y â€œEgiptoâ€), asÃ­ como relaciones sintÃ¡cticas o basadas en la gramÃ¡tica (por ejemplo, la relaciÃ³n entre â€œtenÃ­aâ€ y â€œtengoâ€ es la mismo que entre â€œeraâ€ y â€œesâ€).\n",
        "\n",
        "La comunidad rÃ¡pidamente se dio cuenta de que era mucho mejor idea usar embeddings previamente entrenados con grandes cantidades de datos de texto, en lugar de entrenarlas junto con el modelo en lo que con frecuencia eran pequeÃ±os conjuntos de datos. De esta  manera uno descarga una lista de palabras y sus embeddings a partir del entrenamiento previo con Word2Vec o GloVe. Este es un ejemplo del embedding GloVe de la palabra â€œpaloâ€ (con un tamaÃ±o de vector de embedding de 200)\n",
        "\n",
        "![](https://jalammar.github.io/images/glove-embedding.png)\n",
        "\n",
        "Embedding GloVe de la palabra â€œpaloâ€: un vector de 200 floats (redondeados a dos decimales). ContinÃºa con mÃ¡s de doscientos valores.\n",
        "\n",
        "Dada su longitud y la gran cantidad de nÃºmeros, en las ilustraciones de mis publicaciones utilizo esta cuadrÃ­cula simplificada para representar vectores:\n",
        "\n",
        "![](https://jalammar.github.io/images/vector-boxes.png)\n",
        "\n",
        "## ELMo: el contexto importa\n",
        "\n",
        "Si estamos usando esta representaciÃ³n GloVe, entonces la palabra â€œpaloâ€ estarÃ­a representada por este vector sin importar el contexto. Sin embargo, es claro que esto es un problema.(Peters et. al., 2017 , McCann et. al., 2017 , y una vez mÃ¡s Peters et. al., 2018 en el artÃ­culo de ELMo ). \"Palo\" tiene mÃºltiples significados dependiendo de dÃ³nde se use. Â¿Por quÃ© no darle un embedding basado en el contexto en el que se usa, tanto para capturar el significado de la palabra en ese contexto como para otra informaciÃ³n contextual?â€. Y asÃ­ nacieron los embeddings de palabras contextualizadas .\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
        "\n",
        "Los embeddings de palabras contextualizadas pueden dar a las palabras embeddings diferentes segÃºn el significado que tengan en el contexto de la oraciÃ³n.\n",
        "\n",
        "En lugar de usar un embedding fijo para cada palabra, ELMo analiza la oraciÃ³n completa antes de asignarle un embedding a cada palabra. Utiliza un LSTM bidireccional entrenado en una tarea especÃ­fica para poder crear esos embedding.\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-word-embedding.png)\n",
        "\n",
        "ELMo supuso un paso significativo hacia la formaciÃ³n previa en el contexto de la NLP. El ELMo LSTM se entrenarÃ­a en un conjunto de datos masivo en el idioma de nuestro conjunto de datos, y luego podemos usarlo como componente en otros modelos que necesitan manejar el idioma.\n",
        "\n",
        "Â¿CuÃ¡l es el secreto de ELMo?\n",
        "\n",
        "ELMo adquiriÃ³ su comprensiÃ³n del idioma al ser entrenado para predecir la siguiente palabra en una secuencia de palabras, una tarea llamada **Modelado del Lenguaje**. Esto es muy interesante porque tenemos grandes cantidades de datos de texto de los que dicho modelo puede aprender sin necesidad de etiquetas.\n",
        "\n",
        "![](https://jalammar.github.io/images/Bert-language-modeling.png)\n",
        "\n",
        "Uno de los pasos en el proceso de pre-entrenamiento de ELMo: dado â€œLets stick toâ€ como entrada, predecir la siguiente palabra mÃ¡s probable: una tarea de Modelado de Lenguaje . Cuando se entrena en un gran conjunto de datos, el modelo comienza a captar patrones de lenguaje. Es poco probable que adivine con precisiÃ³n la siguiente palabra en este ejemplo. De manera mÃ¡s realista, despuÃ©s de una palabra como â€œpasarâ€, asignarÃ¡ una mayor probabilidad a una palabra como â€œtiempoâ€ (para conformar â€œpasar tiempoâ€) que a â€œcÃ¡maraâ€.\n",
        "\n",
        "Podemos entrever cada uno de los pasos de LSTM asomando por detrÃ¡s de la cabeza de ELMo. Resultan muy Ãºtiles para generar embeddings de que realizar este entrenamiento previo.\n",
        "\n",
        "ELMo en realidad va un paso mÃ¡s allÃ¡ y entrena un LSTM bidireccional, de modo que su modelo de lenguaje no solo se hace una idea de la palabra siguiente, sino tambiÃ©n de la palabra anterior.\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png)\n",
        "\n",
        "ELMo llega al  embedding contextualizado mediante la agrupaciÃ³n de los estados ocultos (y el embedding inicial) de cierta manera (concatenaciÃ³n seguida de suma ponderada).\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-embedding.png)\n",
        "\n",
        "## ULM-FiT: Transferencia de Aprendizaje en PNLP\n",
        "\n",
        "ULM-FiT introdujo nuevos mÃ©todos para utilizar de manera mÃ¡s efectiva mucho de lo que el modelo aprende durante el entrenamiento previo, mÃ¡s allÃ¡ de los meros embeddings e embeddings contextualizados. ULM-FiT introdujo un nuevo modelo de lenguaje y un proceso para ajustar efectivamente ese modelo de lenguaje para resolver varias tareas.\n",
        "\n",
        "## El Transformer: yendo mÃ¡s allÃ¡ que los LSTM\n",
        "\n",
        "La publicaciÃ³n del paper y el cÃ³digo de Transformer, y los resultados que logrÃ³ en tareas como la traducciÃ³n automÃ¡tica, comenzaron a hacer que algunos pensaran en ellos como un reemplazo de los LSTM. Esto se vio agravado por el hecho de que los Transformers manejan las dependencias a largo plazo mejor que los LSTM.\n",
        "\n",
        "La estructura Encoder-Decoder del Transformer lo hizo perfecto para la traducciÃ³n automÃ¡tica. Pero, Â¿cÃ³mo lo usarÃ­as para la clasificaciÃ³n de oraciones? Â¿CÃ³mo lo usarÃ­a para entrenar previamente un modelo de idioma que se puede ajustar para otras tareas (tareas posteriores es lo que el campo llama tareas de aprendizaje supervisado que utilizan un modelo o componente entrenado previamente).\n",
        "\n",
        "## OpenAI Transformer: pre-entrenando un decoder de Transformer para Modelado de Lenguaje\n",
        "\n",
        "Resulta que no necesitamos un Transformer completo para adoptar transferencia del aprendizaje y un modelo de lenguaje ajustable para tareas de NLP. Podemos hacerlo solo con el decoder del Transformer. El decoder es una buena opciÃ³n porque es una opciÃ³n natural para el **modelado del lenguaje** (predecir la siguiente palabra) ya que estÃ¡ diseÃ±ado para **enmascarar tokens futuros**, una caracterÃ­stica valiosa cuando genera una traducciÃ³n palabra por palabra.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-1.png)\n",
        "\n",
        "El transformer OpenAI estÃ¡ integrado por la pila de decoders del Transformer\n",
        "\n",
        "El modelo apilÃ³ doce capas de decoder. Dado que no hay encoder en esta configuraciÃ³n, estas capas de decoder no tendrÃ­an la subcapa de atenciÃ³n de encoder-decoder que tienen las capas de decoder de Transformer estÃ¡ndar. Sin embargo, todavÃ­a tendrÃ­a la capa de auto atenciÃ³n (enmascarada para que no alcance su punto mÃ¡ximo en tokens futuros).\n",
        "\n",
        "Con esta estructura, podemos proceder a entrenar el modelo en la misma tarea de modelado de lenguaje: predecir la siguiente palabra utilizando conjuntos de datos masivos **sin etiquetar**. Â¡MÃ©tele 7.000 libros y que aprenda! Los libros son excelentes para este tipo de tareas, ya que permiten que el modelo aprenda a asociar informaciÃ³n relacionada, incluso si estÃ¡n separados por mucho texto, algo que no se obtiene, por ejemplo, cuando se entrena con tweets o artÃ­culos.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-language-modeling.png)\n",
        "\n",
        "OpenAI Transformer listo para ser entrenado para predecir la siguiente palabra, con un conjunto de datos compuesto por 7.000 libros.\n",
        "\n",
        "Transferencia del Aprendizaje a tareas posteriores\n",
        "\n",
        "Ahora que el Transformer OpenAI estÃ¡ preentrenado y sus capas se han ajustado para manejar razonablemente el lenguaje, podemos comenzar a usarlo para tareas posteriores. Primero veamos la clasificaciÃ³n de oraciones (clasificar un mensaje de correo electrÃ³nico como â€œspamâ€ o â€œno spamâ€):\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-sentence-classification.png)\n",
        "\n",
        "CÃ³mo usar un Transformer OpenAI preentrenado para clasificar oraciones\n",
        "\n",
        "El documento de OpenAI describe una serie de transformaciones de entrada para manejar las entradas para diferentes tipos de tareas. La siguiente imagen del paper muestra las estructuras de los modelos y las transformaciones de entrada para llevar a cabo diferentes tareas.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-input%20transformations.png)\n",
        "\n",
        "Muy inteligente, no?\n",
        "\n",
        "### BERT: de Decodificadores a Codificadores\n",
        "\n",
        "El Transformer openAI nos brindÃ³ un modelo preentrenado ajustable con precisiÃ³n basado en el Transformer. Pero faltaba algo en esta transiciÃ³n de LSTM a Transformers. El modelo de lenguaje de ELMo era bidireccional, pero el Transformer openAI solo entrena un modelo de lenguaje directo. Â¿PodrÃ­amos construir un modelo basado en Transformers cuyo modelo de lenguaje mire tanto hacia adelante como hacia atrÃ¡s (en la jerga tÃ©cnica, â€œestÃ© condicionado tanto en el contexto izquierdo como en el derechoâ€)?\n",
        "\n",
        "Modelo de lenguaje enmascarado\n",
        "\n",
        "Todo el mundo sabe que el condicionamiento bidireccional permitirÃ­a que cada palabra se viera indirectamente en un contexto de varias capas. Por eso no se usa encoders para esta tarea... Â¿Pero que pasa si usamos mÃ¡scaras?\n",
        "\n",
        "![](https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png)\n",
        "\n",
        "El proceso de modelado de lenguaje inteligente de BERT enmascara el 15% de las palabras en la entrada y le pide al modelo que prediga la palabra que falta.\n",
        "\n",
        "Es literalmente la tarea de cualquier libro de texto de lengua extranjera. Completar con la preposiciÃ³n correcta, con la conjugaciÃ³n correcta. Solo que ahora es como si le dijeramos al alumno. Podes usar todo el diccionario.\n",
        "\n",
        "Encontrar la tarea correcta para entrenar una pila de codificadores de Transformer es un obstÃ¡culo complejo que BERT resuelve adoptando un concepto de â€œmodelo de lenguaje enmascaradoâ€ de la literatura anterior (donde se denomina tarea Cloze).\n",
        "\n",
        "MÃ¡s allÃ¡ de enmascarar el 15% de la entrada, BERT tambiÃ©n mezcla un poco las cosas para mejorar la forma en que el modelo se ajusta mÃ¡s tarde. A veces **reemplaza aleatoriamente una palabra con otra palabra** y le pide al modelo que prediga la palabra correcta en esa posiciÃ³n.\n",
        "\n",
        "## Tareas de dos oraciones\n",
        "\n",
        "Si miramos atrÃ¡s en las transformaciones de entrada que hace el Transformer OpenAI para gestionar diferentes tareas, notaremos que algunas tareas requieren que el modelo diga algo inteligente sobre dos oraciones (por ejemplo, Â¿son simplemente versiones parafraseadas una de la otra? Dada una entrada de wikipedia como entrada, y una pregunta sobre esa entrada como otra entrada, Â¿podemos responder esa pregunta?).\n",
        "\n",
        "Para hacer que BERT sea mejor en el manejo de relaciones entre mÃºltiples oraciones, el proceso de entrenamiento previo incluye una tarea adicional: dadas dos oraciones (A y B), Â¿es probable que B sea la oraciÃ³n que sigue a A, o no?\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-next-sentence-prediction.png)\n",
        "\n",
        "La segunda tarea en la que BERT estÃ¡ pre-entrenado es una tarea de clasificaciÃ³n de dos oraciones. La tokenizaciÃ³n esta muy simplificada en este grÃ¡fico, ya que BERT en realidad usa WordPieces como tokens en lugar de palabras, por lo que algunas palabras se dividen en fragmentos mÃ¡s pequeÃ±os.\n",
        "\n",
        "## Modelos especÃ­ficos de tareas\n",
        "\n",
        "El paper de BERT muestra varias formas de usar BERT para diferentes tareas.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-tasks.png)\n",
        "\n",
        "BERT para extracciÃ³n de caracterÃ­sticas\n",
        "\n",
        "El enfoque de Fine-Tuning no es la Ãºnica forma de utilizar BERT. Al igual que ELMo, se puede usar BERT previamente entrenado para crear embeddings de palabras contextualizadas. Luego, se puede alimentar estos embeddings a un modelo existente, un proceso que el paper muestra que produce resultados no muy lejanos del Fine-Tuning de BERT en tareas como el reconocimiento de entidades.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-contexualized-embeddings.png)\n",
        "\n",
        "Â¿QuÃ© vector funciona mejor como embeddig contextualizada?Depende de la tarea. El documento examina seis opciones (en comparaciÃ³n con el modelo ajustado que logrÃ³ una puntuaciÃ³n de 96,4):\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "PXuRSh6PLnAs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAqZLs3lwhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14698aa7-c073-4de1-dd81-27afd9872027"
      },
      "source": [
        "# Fist install the library and download the models from github\n",
        "\n",
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "--2022-09-05 17:31:41--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409871727 (391M) [application/x-gzip]\n",
            "Saving to: â€˜pytorch_weights.tar.gzâ€™\n",
            "\n",
            "pytorch_weights.tar 100%[===================>] 390.88M  9.05MB/s    in 81s     \n",
            "\n",
            "2022-09-05 17:33:04 (4.83 MB/s) - â€˜pytorch_weights.tar.gzâ€™ saved [409871727/409871727]\n",
            "\n",
            "--2022-09-05 17:33:04--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 242120 (236K) [text/plain]\n",
            "Saving to: â€˜vocab.txtâ€™\n",
            "\n",
            "vocab.txt           100%[===================>] 236.45K   253KB/s    in 0.9s    \n",
            "\n",
            "2022-09-05 17:33:07 (253 KB/s) - â€˜vocab.txtâ€™ saved [242120/242120]\n",
            "\n",
            "--2022-09-05 17:33:07--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313 [application/json]\n",
            "Saving to: â€˜config.jsonâ€™\n",
            "\n",
            "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-05 17:33:08 (41.7 MB/s) - â€˜config.jsonâ€™ saved [313/313]\n",
            "\n",
            "pytorch/\n",
            "pytorch/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_1ucCCUnXqa"
      },
      "source": [
        "# import the necessary\n",
        "\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCpOxoXkoGFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8fbee8-dd57-41d8-f6ff-f9b1e432acb3"
      },
      "source": [
        "# create the tokenizer and the model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
        "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
        "e = model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KXo6-ahoJoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ab651c-af3a-40ea-fa66-b3742b84d4c8"
      },
      "source": [
        "# Now test it\n",
        "\n",
        "# Miguel de Cervantes, El ingenioso hidalgo Don Quijote de la Mancha\n",
        "\n",
        "text = \"[CLS] En un lugar [MASK] la [MASK] de cuyo nombre no quiero [MASK] [SEP]\"\n",
        "masked_indxs = (4,6,12)\n",
        "\n",
        "\n",
        "# JosÃ© Hernandez, Martin Fierro\n",
        "\"\"\"\n",
        "text = \"[CLS] Los hermanos sean [MASK] [SEP] porque esa es la ley primera; [SEP] \\\n",
        "tengan uniÃ³n verdadera [SEP] en cualquier tiempo que [MASK], [SEP] \\\n",
        "porque si entre [MASK] pelean [SEP] los devoran los de afuera. [SEP]\"\n",
        "masked_indxs = (4,22,28)\n",
        "\"\"\"\n",
        "# Alejandro Dolina, Lo que me costÃ³ el amor de Laura.\n",
        "#text = \"[CLS] se ha dicho que los [MASK], hacen todo lo que [MASK] con el Ãºnico fin de [MASK] mujeres\"\n",
        "#masked_indxs = (6,12,18)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = model(tokens_tensor)[0]\n",
        "\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASK 0 : ['de', 'en', 'como', 'a', ',']\n",
            "MASK 1 : ['tierra', 'ciudad', 'Tierra', 'gente', 'patria']\n",
            "MASK 2 : ['hablar', '.', 'ser', ',', 'decir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
        "\n",
        "model2 = BertForNextSentencePrediction.from_pretrained(\"pytorch/\")\n",
        "\n",
        "text = \"[CLS] MÃ­rela bien.  [SEP] Ya no la verÃ¡ nunca mÃ¡s. \"\n",
        "\n",
        "text = \"[CLS] Si el espacio es infinito estamos en cualquier punto del espacio. [SEP] Si el tiempo es infinito estamos en cualquier punto del tiempo.\"\n",
        "\n",
        "text = \"[CLS] Si el espacio es infinito estamos en cualquier punto del espacio.  [SEP] Ya no la verÃ¡ nunca mÃ¡s. \"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = model2(tokens_tensor)\n",
        "\n",
        "print(torch.argmax(predictions.logits))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X1xdDDov-jk",
        "outputId": "b486a63c-6e85-4154-f8df-9200948233b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_ner = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"mrm8488/RuPERTa-base-finetuned-pos\",\n",
        "    tokenizer=\"mrm8488/RuPERTa-base-finetuned-pos\")\n",
        "\n",
        "text = 'tras el temporal, el joven encontro un empleo temporal'\n",
        "text = 'Hacia el este fue donde este sujeto se dirigio y se topo con este'\n",
        "\n",
        "\n",
        "outputs = nlp_ner(text)\n",
        "\n",
        "for i in outputs:\n",
        "  print(i['word'], i['entity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1XwLc2o63zq",
        "outputId": "756aa647-81d9-4f8d-b972-36c004002c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hacia VERB\n",
            "Ä el DET\n",
            "Ä este NOUN\n",
            "Ä fue AUX\n",
            "Ä donde PRON\n",
            "Ä este DET\n",
            "Ä sujeto NOUN\n",
            "Ä se PRON\n",
            "Ä diri VERB\n",
            "gio VERB\n",
            "Ä y CCONJ\n",
            "Ä se PRON\n",
            "Ä topo VERB\n",
            "Ä con ADP\n",
            "Ä este PRON\n"
          ]
        }
      ]
    }
  ]
}