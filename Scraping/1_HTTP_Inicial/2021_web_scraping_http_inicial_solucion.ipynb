{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "2021_web_scraping_http_inicial_solucion.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/auto/Scraping/1_HTTP_Inicial/2021_web_scraping_http_inicial_solucion.ipynb\" target=\"_parent\"><img src=\"https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping: Extrayendo datos de Internet**"
      ],
      "metadata": {
        "id": "dJikR8QwA0Zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **¬øQu√© es el web scraping?**  ü§î\n",
        "\n",
        "*La pr√°ctica de **recopilar datos** a trav√©s de cualquier medio que no sea un programa que interact√∫a con una API o un humano que usa un navegador web. **Normalmente mediante un programa automatizado** que consulta un servidor web, solicita datos (generalmente en forma de HTML y otros archivos que componen las p√°ginas web) y luego analiza esos datos para extraer la informaci√≥n necesaria.*\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://images-na.ssl-images-amazon.com/images/I/517z2NUzcEL._SX198_BO1,204,203,200_QL40_ML2_.jpg\">\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "*Por otro lado, el **web crawling o indexaci√≥n** se utiliza para indexar la informaci√≥n de la p√°gina mediante bots tambi√©n conocidos como crawlers (lo que hacen los motores de b√∫squeda). Se trata de ver una p√°gina como un todo e indexarla. Cuando un bot rastrea un sitio web, **recorre todas las p√°ginas y todos los enlaces**, hasta la √∫ltima l√≠nea del sitio web, en busca de **CUALQUIER informaci√≥n**.*"
      ],
      "metadata": {
        "id": "bTg_uMsf2sXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Antes de empezar** ‚ö†Ô∏è\n",
        "\n",
        "1. *Aspectos √©ticos y legales del web scraping*\n",
        "  * El web scraping es una forma autom√°tica de guardar informaci√≥n que se presenta en nuestro navegador muy utilizada tanto en la industria como en la academia, sus aspectos legales depender√°n de cada sitio y de cada estado. Respecto a la √©tica es importante que nos detengamos a pensar si estamos o no generando algun perjuicio. En ambos casos el debate est√° abierto y hay mucha bibliograf√≠a al respecto como por ejemplo [este trabajo](https://www.researchgate.net/profile/Vlad-Krotov/publication/324907302_Legality_and_Ethics_of_Web_Scraping/links/5aea622345851588dd8287dc/Legality-and-Ethics-of-Web-Scraping.pdf)\n",
        "\n",
        "2. *No reinventar la rueda*\n",
        "  * Emprender un proyecto de web scraping a veces es rapido y sencillo, pero normalmente requiere tiempo y esfuerzo. Siempre es aconsejable asegurarse de que valga la pena y antes iniciar hacerse algunas preguntas:<br>\n",
        "    - ¬øLa informacion que necesito ya se encuentra disponible? (ej: APIs)\n",
        "    - ¬øVale la pena automatizarlo o es algo que lleva poco trabajo a mano?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wDfhvFueahvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conceptos b√°sicos sobre la web** \n",
        "\n",
        "#### HTML, CSS y JavaScript son los tres lenguajes principales con los que est√° hecho la parte de la web que vemos (*front-end*).\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.nicepng.com/png/detail/142-1423886_html5-css3-js-html-css-javascript.png\" width=\"400\">\n",
        "\n",
        "<img src=\"https://geekflare.com/wp-content/uploads/2019/12/css-gif.gif\" width=\"243\">\n",
        "\n",
        "\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "| ESTRUCTURA  | ESTILO | FUNCIONALIDAD|\n",
        "|-----|----------------| ---------- |\n",
        "|HTML| CSS | JAVASCRIPT|\n",
        "\n",
        "<font color=\"gray\">\n",
        "Fuente de las im√°genes: <br>\n",
        "https://geekflare.com/es/css-formatting-optimization-tools/ <br>\n",
        "https://www.nicepng.com/ourpic/u2q8i1o0e6q8r5t4_html5-css3-js-html-css-javascript/\n",
        "</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "JodftJTr_eGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducci√≥n a HTML"
      ],
      "metadata": {
        "id": "UvoyAdrL2pGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El lenguaje principal de la internet es HTML, cuando nosotros vemos algo as√≠:\n",
        "\n",
        "![](https://github.com/institutohumai/cursos-python/blob/master/Scraping/1_HTTP_Inicial/multimedia/hello-world.jpeg?raw=1)\n",
        "\n",
        "Eso se genera a partir de una c√≥digo que luce as√≠\n",
        "\n",
        "```html\n",
        "<html>\n",
        "  <header>\n",
        "    <title>Web Scraping - Instituto Humai</title>\n",
        "  </header>\n",
        "  <body>\n",
        "    <h1>¬°Hola!</h1>\n",
        "    <p>Esto es un sitio web</p>\n",
        "  </body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "**_Nota_**: Para saber m√°s sobre HTML pod√©s consultar [ac√°](https://www.w3schools.com/TAGS/default.ASP) la lista de etiquetas de este lenguaje."
      ],
      "metadata": {
        "id": "WZ8PvQ87A0Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```html\n",
        "  <head>\n",
        "    <title>Mi primer pagina</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1 id='titulo'>Hola</h1>\n",
        "    <h2 style='color:red;'>Subtitulo en rojo</h2>\n",
        "    <p>Primer parrafo</p>\n",
        "    <hr>\n",
        "    <img src=\"https://i.pinimg.com/564x/8f/14/25/8f142555ef5006abd82d8c5c7f9f8570.jpg\" alt=\"gato\" width=400>\n",
        "  </body>\n",
        "```\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/9pqvGSv/HTML-gatito.png\"  width=800> <br>\n",
        "<h3>Probar el c√≥digo: <a>https://codepen.io/GEJ1/pen/GRmVNPb</a></h3>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "B6sSeFHDGY26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DOM (Document Object Model)\n",
        "\n",
        "\n",
        "Interfaz independiente del lenguaje que trata un documento XML o HTML como una estructura de tipo **√°rbol**\n",
        "<figure>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/DOM-model.svg/330px-DOM-model.svg.png\" width=\"500\">\n",
        "\n",
        "```html\n",
        "<html>\n",
        "  <head>\n",
        "    <title>My title</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1>A heading</h1>\n",
        "    <a href>Link text</a>\n",
        "  </body>\n",
        "</html>\n",
        "```\n",
        "</figure>\n",
        "\n",
        "<font color=\"gray\"> Fuente: https://en.wikipedia.org/wiki/Document_Object_Model\n",
        "<br>Autor: Birger Eriksson\n",
        "</font>\n"
      ],
      "metadata": {
        "id": "MrnbZl5CETFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¬øC√≥mo consigo el c√≥digo HTML?\n",
        "\n",
        "Ahora que sabemos cu√°l es el componente principal de los sitios webs podemos intentar programar a nuestra computadora para leer HTML y extraer informaci√≥n √∫til.\n",
        "\n",
        "Para conseguir el c√≥digo de un sitio web podemos presionar `ctrl+u` en el navegador.\n",
        "\n",
        "Para hacer lo mismo desde Python podemos hacer lo siguiente:"
      ],
      "metadata": {
        "id": "B3s4n2eqA0Z3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Importamos la libreria necesaria\n",
        "import requests\n",
        "\n",
        "un_sitio_web = \"https://es.wikipedia.org/wiki/HTML\"\n",
        "\n",
        "# esto descarga la informaci√≥n del sitio web\n",
        "# Es similar a lo que hace un navegador web antes de mostrar el contenido de forma amigable para un humano\n",
        "resultado = requests.get(un_sitio_web)\n",
        "\n",
        "# accedemos al c√≥digo a trav√©s del atributo \"text\" del resultado\n",
        "codigo_html = resultado.text\n",
        "print(codigo_html[:1000])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs\" lang=\"es\" dir=\"ltr\">\n",
            "<head>\n",
            "<meta charset=\"UTF-8\"/>\n",
            "<title>HTML - Wikipedia, la enciclopedia libre</title>\n",
            "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\",\\t.\",\"¬†\\t,\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"enero\",\"febrero\",\"marzo\",\"abril\",\"mayo\",\"junio\",\"julio\",\"agosto\",\"septiembre\",\"octubre\",\"noviembre\",\"diciembre\"],\"wgRequestId\":\"29fc7366-2b54-483e-82ab-b76ae6eb8438\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"HTML\",\"wgTitle\":\"HTML\",\"wgCurRevisionId\":138298734,\"wgRevisionId\":138298734,\"wgArticleId\":1366,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Wikipedia:Art√≠culos con datos por trasladar a Wikidata\",\"Wikipedia:Art√≠culos con identificadores BNE\",\"Wikipedia:Art√≠culos con identificadores BNF\",\"Wikipedia:Art√≠cu\n"
          ]
        }
      ],
      "metadata": {
        "id": "lCA6aQ1gA0Z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5835ff58-4cab-4ac0-8b24-69c2319703b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øQu√© acabamos de hacer?\n",
        "\n",
        "Veamos algunos detalles m√°s sobre c√≥mo descargar el contenido de un sitio web (O c√≥mo se le suele decir en la jerga de la programaci√≥n _realizar un request_). Como dijimos, en python se puede utilizar la funci√≥n get de la libreria requests para hacer esto, veamos con mayor profundidad c√≥mo se utiliza."
      ],
      "metadata": {
        "id": "fyGIaAQNA0Z7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "# httpbin es una pagina para testear pedidos HTTP, en particular la siguiente URL nos devuelve nuestro header.\n",
        "url = 'http://httpbin.org/headers' \n",
        "resp = requests.get(url)\n",
        "\n",
        "print('------------------------------')\n",
        "print('Respuesta sin headers')\n",
        "print(resp.text)\n",
        "\n",
        "print('------------------------------')\n",
        "print('Respuesta con headers')\n",
        "nuestros_headers = {\n",
        "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'\n",
        "    }\n",
        "resp_con_headers = requests.get(url, headers = nuestros_headers)\n",
        "print(resp_con_headers.text)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Respuesta sin headers\n",
            "{\n",
            "  \"headers\": {\n",
            "    \"Accept\": \"*/*\", \n",
            "    \"Accept-Encoding\": \"gzip, deflate\", \n",
            "    \"Host\": \"httpbin.org\", \n",
            "    \"User-Agent\": \"python-requests/2.23.0\", \n",
            "    \"X-Amzn-Trace-Id\": \"Root=1-61561f08-0c46c3b434fb43e715333b44\"\n",
            "  }\n",
            "}\n",
            "\n",
            "------------------------------\n",
            "Respuesta con headers\n",
            "{\n",
            "  \"headers\": {\n",
            "    \"Accept\": \"*/*\", \n",
            "    \"Accept-Encoding\": \"gzip, deflate\", \n",
            "    \"Host\": \"httpbin.org\", \n",
            "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36\", \n",
            "    \"X-Amzn-Trace-Id\": \"Root=1-61561f08-4afb069b3aab2efa415e83a7\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "ekwXCdmkA0Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d45032f-fe4d-4ec8-d6d1-e8ca976c8fa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parte de la _url_, muchas veces se especifican los _headers_, estos son objetos que proveen datos sobre nuestro _request_, por ejemplo en el campo user-agent brindamos detalles sobre quienes somos (Nuestro sistema operativo, navegador web y dem√°s). En este caso, como no estamos usando un navegador sino que hacemos el _request_ desde Python normalmente se omite este campo, o en caso de ser obligatorio se puede inventar, ya que algunos sitios nos van a ignorar a menos que especifiquemos este campo.\n",
        "\n",
        "Pueden ver m√°s en esta [Lista de Headers](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields)\n",
        "\n",
        "- Consultas\n",
        "    - ¬øPor qu√© los sitios te podr√≠an bloquear/ignorar?\n",
        "    - ¬øDe donde saco un user-agent?\n"
      ],
      "metadata": {
        "id": "XhxvusaLA0Z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Protocolo HTTP"
      ],
      "metadata": {
        "id": "MdRjN5y1CuBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La web utiliza ampliamente el protocolo HTTP (de _Hypertext Transfer Protocol_) para interactuar con sus recursos. Este protocolo indica c√≥mo estructurar un mensaje de texto que describa la petici√≥n (**request**) del usuario a un servidor. Hay distintos tipos de peticios que un usuario puede realizar, algunas de ellas son:\n",
        "\n",
        "* **GET**: Solicita una representaci√≥n de un recurso alojado en el servidor.\n",
        "* **POST**: Env√≠a datos al servidor para crear un recurso nuevo.\n",
        "* **PUT**: Crea o modifica un recurso del servidor.\n",
        "* **DELETE**: Elimina un recurso del servidor.\n",
        "\n",
        "Existen otros m√©todos que no nos van a ser relevantes por ahora.\n",
        "\n",
        "Cada vez que vamos al navegador y escribimos la direcci√≥n de una p√°gina web, **estamos haciendo un GET request** a un servidor. \n",
        "Esto es una petici√≥n para adquirir el c√≥digo de un recurso que queremos visualizar en el navegador. "
      ],
      "metadata": {
        "id": "2w07ITLwHmi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos antes la funci√≥n `get` retorna un objeto, el cual llamamos _resp_, este es un elemento de la clase **_Response_** y tiene distintos atributos a los que podemos acceder.\n",
        "\n",
        "El objeto **_Response_** de requests tiene los siguientes elementos principales:\n",
        "\n",
        "- **.text**: devuelve el contenido como string.\n",
        "- **.content**: devuelve el contenido en bytes.\n",
        "- **.json()**: el contenido en formato JSON, si es posible.\n",
        "- **.status_code**: el c√≥digo de respuesta.\n",
        "\n",
        "\n",
        "El c√≥digo de status (*status code*) nos informa del estado de nuestra *request*\n",
        "\n",
        "C√≥digos posibles:\n",
        "\n",
        "- 1xx Mensaje de informaci√≥n\n",
        "- 2xx √âxito\n",
        "- 3xx Redirigir a otra URL\n",
        "- 4xx Error del cliente\n",
        "- 5xx Error del servidor\n",
        "\n",
        "<center>\n",
        "<img alt=\"http-status-codes\" src=\"https://miro.medium.com/max/1400/1*w_iicbG7L3xEQTArjHUS6g.jpeg\" width=\"500\"> <br>\n",
        "<font color='gray'>Fuente: https://www.youtube.com/watch?v=LtNSd_4txVc\n",
        "</font>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "U_wuF5Mq1ikJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Vemos el c√≥digo de estado\n",
        "# 200 es que esta todo bien, 5xx o 4xx es que esta todo mal (Por ejemplo el clasico 404)\n",
        "resp.status_code"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "BJoW-QxnA0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937f0449-5e68-42d8-a573-f85d29aaa35b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Vemos los headers que enviamos\n",
        "resp.request.headers"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "LyzTCE9lA0Z-",
        "outputId": "ebd2d06b-21ac-43d8-979e-9b11961ce684"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El atributo que nos interesa particularmente es resp.text, que guardan el contenido de la p√°gina.\n",
        "\n",
        "Como vamos a descargar el codigo de un sitio frecuentemente armamos una funcion para no reescribir lo mismo muchas veces"
      ],
      "metadata": {
        "id": "tgd3ifvlA0Z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def codigo_html(url):\n",
        "    headers = {\n",
        "        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'\n",
        "        }\n",
        "    resp = requests.get(url, headers = headers)\n",
        "    return resp.text"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZtjjqrYdA0Z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Tambien podemos scrapear otro tipo de a que no sea texto\n",
        "import requests\n",
        "\n",
        "# defino la URL\n",
        "image_url = 'https://www.octoparse.com/media/7179/find-data.jpg'\n",
        "\n",
        "# Hago una peticion y guardo la respuesta\n",
        "image_response = requests.get(image_url)\n",
        "\n",
        "# Accedemos al contenido de la imagen en bytes\n",
        "image_response_content = image_response.content\n",
        "\n",
        "print(f'Este es el contenido en bytes: \\n {image_response_content[:100]}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Este es el contenido en bytes: \n",
            " b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\n\\x07\\x07\\x08\\x07\\x06\\n\\x08\\x08\\x08\\x0b\\n\\n\\x0b\\x0e\\x18\\x10\\x0e\\r\\r\\x0e\\x1d\\x15\\x16\\x11\\x18#\\x1f%$\"\\x1f\"!&+7/&)4)!\"0A149;>>>%.DIC<H7=>;\\xff\\xdb\\x00C\\x01\\n\\x0b\\x0b\\x0e\\r\\x0e'\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp8KOmTafohn",
        "outputId": "5220527c-3b25-4e89-f13d-fd6fd930b6ee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Importamos librerias para manejar imagenes (no tienen nada que ver con el scrapeo)\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Mostramos la imagen\n",
        "image_from_url = Image.open(BytesIO(image_response_content))\n",
        "print('Esta imagen la bajamos de internet usando Python! \\n ')\n",
        "image_from_url"
      ],
      "outputs": [],
      "metadata": {
        "id": "XCeM77vEf70c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentaci√≥n\n",
        "La funci√≥n get y la clase Response fueron desarrolladas por lxs programadores que crearon la librer√≠a requests. Si quieren saber mas sobre alg√∫n detalle siempre es recomendable buscar en la [documentaci√≥n oficial de la librer√≠a](https://docs.python-requests.org/en/latest/)."
      ],
      "metadata": {
        "id": "h1cGseCPA0aA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¬øC√≥mo extraigo datos √∫tiles del c√≥digo HTML?\n",
        "\n",
        "- Veamos un ejemplo inspeccionando con chrome un sitio web:\n",
        "1. Nos posicionamos sobre el elemento que nos interesa.\n",
        "2. Presionamos click derecho -> *inspeccionar elemento* para abrir las *herramientas de desarrollo* (o presionando `CTRL + SHIFT + I`)\n",
        "4. Esto nos da acceso al codigo de HTML correspondiente al elemento de la pagina que nos interesa.\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/1RSNcs5/inspect.png\" alt=\"inspect\" width=\"1100\">\n",
        "\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "En **Elements** vamos a poder inspeccionar el c√≥digo HTML, para ubicar los datos de nuestro inter√©s. Para identificar la ubicaci√≥n de uno, podemos posicionarnos con el cursor sobre el sitio, hacer click derecho, y seleccionar \"Inspeccionar elemento\".\n",
        "\n",
        "En la solapa **Network** podemos ver todos los paquetes HTTP que realiza nuestro navegador interactuando con un sitio. Identificando los paquetes de las APIs que traen los datos, podemos _scrapear_ datos m√°s facilmente. **[Esto lo ver√°n en la clase de APIs]**\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src='https://i2.wp.com/abodeqa.com/wp-content/uploads/2019/02/Inspect-Element-Using-Select-Tool.gif'>\n",
        "\n"
      ],
      "metadata": {
        "id": "NYNXMrDgA0aA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **M√©todo 1: Expresiones regulares**"
      ],
      "metadata": {
        "id": "j208C_1eH8Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegEx para los amigos. Son un mini lenguaje de programaci√≥n dise√±ado para realizar b√∫squedas en strings."
      ],
      "metadata": {
        "id": "IIprwSHYA0aA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las funciones principales de la librer√≠a re son:\n",
        "- re.findall(pattern, string) para encontrar todos los resultados de una b√∫squeda\n",
        "- re.search(pattern, string) para encontrar el primer resultado que coincida\n",
        "- re.sub(pattern, replace, string) para substituir un texto por otro\n",
        "\n",
        "Recursos √∫tiles\n",
        "\n",
        "- [Testeo de regex online](https://regex101.com/)\n",
        "- [CheatSheet](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)"
      ],
      "metadata": {
        "id": "BhrBctRSA0aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplicandolo a la web\n",
        "##### Ejemplo 1: Usamos regex para extraer los t√≠tulos del diario La Prensa.\n",
        "\n",
        "\n",
        "```html\n",
        "<h2 class=\"entry__title\"><a href=\"http://www.laprensa.com.ar/491843-Dilemas-de-la-batalla-cultural-I.note.aspx\" target=\"_self\" onclick=\"javascript:if(typeof(_gaq)!='undefined'){_gaq.push(['_trackEvent', 'Notas', 'Cultura', 'Dilemas de la batalla cultural (I)'])};\">Dilemas de la batalla cultural (I)</a></h2>\n",
        "```\n"
      ],
      "metadata": {
        "id": "XIxDi3ZiA0aG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Usamos el navegador para identificar la estructura de los datos que queremos extraer y creamos el patr√≥n de b√∫squeda\n",
        "regla_de_busqueda = r'_self\">(.+)</a></h2>'"
      ],
      "outputs": [],
      "metadata": {
        "id": "kz1d61KNA0aH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Usamos findall para encontrar todas las coincidencias\n",
        "import re\n",
        "import requests\n",
        "titles = [m for m in re.findall(regla_de_busqueda, codigo_html(\"http://www.laprensa.com.ar/\"))]"
      ],
      "outputs": [],
      "metadata": {
        "id": "rTtKkAsfA0aH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(titles)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Vidal cuestion√≥ los \"parches y manotazos de ahogado\" del Gobierno ', 'Tres gobiernos, ning√∫n gobierno', 'Insultaron y agredieron a Leandro Santoro en un caf√© cerca del Cabildo', 'El Gobierno elimina las retenciones a las exportaciones de servicios a partir de 2022', 'Libros aburridos', 'El ataque al Convento de las Catalinas']\n"
          ]
        }
      ],
      "metadata": {
        "id": "Ssh-aWNdA0aH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f20fd7-d2b1-4508-f907-ff27cf3f0aed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  <font color='red'> Ejercicio </font>\n",
        "\n",
        "####  Modifiquen la regla de b√∫squeda para que descargue los links a las notas en vez del t√≠tulo"
      ],
      "metadata": {
        "id": "H8oKjJkHFj2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Resoluci√≥n\n",
        "\n",
        "#Usamos el navegador para identificar la estructura de los datos que queremos extraer y creamos el patr√≥n de b√∫squeda\n",
        "regla_de_busqueda = r'<a href=\"(.+)\" target=\"_self\">'\n",
        "\n",
        "links = [link for link in re.findall(regla_de_busqueda, codigo_html(\"http://www.laprensa.com.ar/\"))]\n",
        "print(links)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.laprensa.com.ar/507044-Vidal-cuestiono-los-parches-y-manotazos-de-ahogado-del-Gobierno-.note.aspx', 'https://www.laprensa.com.ar/507044-Vidal-cuestiono-los-parches-y-manotazos-de-ahogado-del-Gobierno-.note.aspx', 'https://www.laprensa.com.ar/507043-Tres-gobiernos-ningun-gobierno.note.aspx\" class=\"thumb-url', 'https://www.laprensa.com.ar/507043-Tres-gobiernos-ningun-gobierno.note.aspx', 'https://www.laprensa.com.ar/507045-Insultaron-y-agredieron-a-Leandro-Santoro-en-un-cafe-cerca-del-Cabildo.note.aspx\" class=\"thumb-url', 'https://www.laprensa.com.ar/507045-Insultaron-y-agredieron-a-Leandro-Santoro-en-un-cafe-cerca-del-Cabildo.note.aspx', 'https://www.laprensa.com.ar/507025-El-Gobierno-elimina-las-retenciones-a-las-exportaciones-de-servicios-a-partir-de-2022.note.aspx\" class=\"thumb-url', 'https://www.laprensa.com.ar/507025-El-Gobierno-elimina-las-retenciones-a-las-exportaciones-de-servicios-a-partir-de-2022.note.aspx', 'https://www.laprensa.com.ar/506993-Segundo-tiempo-con-otro-equipo-y-otro-capitan.note.aspx', 'https://www.laprensa.com.ar/506993-Segundo-tiempo-con-otro-equipo-y-otro-capitan.note.aspx', 'https://www.laprensa.com.ar/506990-Julieta-Diaz-la-maternidad-y-la-vida.note.aspx', 'https://www.laprensa.com.ar/506990-Julieta-Diaz-la-maternidad-y-la-vida.note.aspx', 'https://www.laprensa.com.ar/506992-Libros-aburridos.note.aspx', 'https://www.laprensa.com.ar/506992-Libros-aburridos.note.aspx', 'https://www.laprensa.com.ar/506991-El-ataque-al-Convento-de-las-Catalinas.note.aspx', 'https://www.laprensa.com.ar/506991-El-ataque-al-Convento-de-las-Catalinas.note.aspx', 'https://www.laprensa.com.ar/506905-El-Centro-Industrial-de-Ford-en-Pacheco-cumple-60-anos.note.aspx', 'https://www.laprensa.com.ar/506905-El-Centro-Industrial-de-Ford-en-Pacheco-cumple-60-anos.note.aspx', 'https://www.laprensa.com.ar/506814-El-neopuritanismo-alimenticio-contra-los-tomates-asesinos.note.aspx', 'https://www.laprensa.com.ar/506814-El-neopuritanismo-alimenticio-contra-los-tomates-asesinos.note.aspx']\n"
          ]
        }
      ],
      "metadata": {
        "id": "-Q2ms9IQFvBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6979f7-faa5-48d7-aa07-e3970f4ab2e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **M√©todo 2: BeautifulSoup**\n",
        "* Esta librer√≠a provee un *parser* de html, o sea un programa que entiende el c√≥digo, permitiendonos hacer consultas m√°s sofisticadas de forma simple, por ejemplo \"buscame todos los titulos h2 del sitio\".\n",
        "\n",
        "\n",
        "* Se usa para extraer los datos de archivos HTML. Crea un √°rbol de an√°lisis a partir del c√≥digo fuente de la p√°gina que se puede utilizar para extraer datos de forma jer√°rquica y m√°s legible.\n",
        "\n",
        "<center>\n",
        "<img alt=\"\" width=\"700\" role=\"presentation\" src=\"https://miro.medium.com/max/700/0*ETFzXPCNHkPpqNv_.png\"> <br>\n",
        "\n",
        "<font color=\"gray\">\n",
        "Fuente: https://medium.com/milooproject/python-simple-crawling-using-beautifulsoup-8247657c2de5\n",
        "<font>\n",
        "</center>"
      ],
      "metadata": {
        "id": "ahxVvZazA0aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalidades"
      ],
      "metadata": {
        "id": "BGXRo8BWtIDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "import requests\n",
        "\n",
        "# Vamos a jugar un poco con la pagina de Exactas\n",
        "url_base = 'https://exactas.uba.ar/'\n",
        "endpoint_calendario = 'calendario-academico/'\n",
        "html_obtenido = requests.get(url_base + endpoint_calendario)\n",
        "soup = BeautifulSoup(html_obtenido.text, \"html.parser\")\n",
        "# print(soup)\n",
        "# print(type(soup))\n",
        "# print(soup.prettify())\n",
        "\n",
        "## Tambien podemos mostrar renderizar el html aca en Colab :D\n",
        "# HTML(html_obtenido.text)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bs4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141592/74427014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Vamos a jugar un poco con la pagina de Exactas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
          ]
        }
      ],
      "metadata": {
        "id": "YLnDRuoftSDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Si queremos quedarnos con un tag\n",
        "\n",
        "# El m√©todo \"find\" busca el primer elemento de la pagina con ese tag\n",
        "primer_h3 = soup.find('h3')\n",
        "print(primer_h3)\n",
        "\n",
        "# equivalente a:\n",
        "# print(soup.h3.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h3>CURSO DE VERANO 2021 (7 semanas)</h3>\n"
          ]
        }
      ],
      "metadata": {
        "id": "Jo7AkWAhh23W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a59c11-911f-4d1e-813d-f4451d216238"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# El m√©todo \"find_all\" busca TODOS los elementos de la pagina con ese tag y devuelve una lista que los contiene (en realidad devuelve un objeto de la clase \"bs4.element.ResultSet\")\n",
        "h3_todos = soup.find_all('h3')\n",
        "print(h3_todos)\n",
        "\n",
        "# Si usamos el parametro limit = 1, emulamos al metodo find:\n",
        "# h3_uno_solo = soup.find_all('h3',limit=1)\n",
        "# print(h3_uno_solo)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<h3>CURSO DE VERANO 2021 (7 semanas)</h3>, <h3>INSCRIPCI√ìN PRIMER CUATRIMESTRE 2021</h3>, <h3>EX√ÅMENES DE FEBRERO-MARZO 2021</h3>, <h3>PRIMER BIMESTRE 2021 (8 semanas)</h3>, <h3>SEGUNDO BIMESTRE 2021 (8 semanas)</h3>, <h3>CURSO DE INVIERNO 2021</h3>, <h3>INSCRIPCI√ìN SEGUNDO CUATRIMESTRE 2021</h3>, <h3>INSCRIPCI√ìN A DOCTORADO 2021</h3>, <h3>SEMANAS DE LAS CIENCIAS</h3>, <h3>ACTOS DE COLACI√ìN DE GRADO Y POSGRADO</h3>, <h3><strong>FERIADOS</strong></h3>, <h3 class=\"widget-title\"><a href=\"https://exactas.uba.ar/agenda/\">Agenda ‚Üí</a></h3>]\n"
          ]
        }
      ],
      "metadata": {
        "id": "Az5VWD8qh5XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02266c23-c07d-4395-a874-ba3a8f35429c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# podemos iterar sobre el objeto\n",
        "for fecha in h3_todos[:-1]:\n",
        "  # Extraemos el texto que se encuentra dentro del tag\n",
        "  print(fecha.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CURSO DE VERANO 2021 (7 semanas)\n",
            "INSCRIPCI√ìN PRIMER CUATRIMESTRE 2021\n",
            "EX√ÅMENES DE FEBRERO-MARZO 2021\n",
            "PRIMER BIMESTRE 2021 (8 semanas)\n",
            "SEGUNDO BIMESTRE 2021 (8 semanas)\n",
            "CURSO DE INVIERNO 2021\n",
            "INSCRIPCI√ìN SEGUNDO CUATRIMESTRE 2021\n",
            "INSCRIPCI√ìN A DOCTORADO 2021\n",
            "SEMANAS DE LAS CIENCIAS\n",
            "ACTOS DE COLACI√ìN DE GRADO Y POSGRADO\n",
            "FERIADOS\n"
          ]
        }
      ],
      "metadata": {
        "id": "g_pjutuZh9wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8d647b-e906-47f2-c953-d706ff0fca9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Busco por clase, escribo class_ porque \"class\" es una palabra reservada en Python\n",
        "eventos_proximos = soup.find('aside', class_ = 'widget_my_calendar_upcoming_widget')\n",
        "for evento in eventos_proximos:\n",
        "  print(evento.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agenda ‚Üí\n",
            "\n",
            "30 septiembre, 2021,  : Becas de ayuda econ√≥mica Sarmiento  | + INFO\n",
            "30 septiembre, 2021,  : Concurso de cargo t√©cnico IGEBA CONICET  | + INFO\n",
            "30 septiembre, 2021, 14.00: Coloquio del Departamento de F√≠sica  | + INFO\n",
            "30 septiembre, 2021, 15.00: Charlas virtuales de carrera de septiembre-octubre  | + INFO\n",
            "1 octubre, 2021,  : Proyectos UBACYT de Desarrollo Estrat√©gico 2022  | + INFO\n",
            "1 octubre, 2021, 13.00: Coloquio de los viernes \"Cannabis Medicinal: Mitos y realidades de la marihuana en el siglo XXI\"  | + INFO\n",
            "1 octubre, 2021, 17.00: Coloquios del DCAO/CIMA  | + INFO\n",
            "3 octubre, 2021,  : Meteorolog√≠a del Espacio  | + INFO\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "2KTxwctZiAC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e6b16d-8533-4f13-fa1a-6d60540ca1e2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Todos los links. Esto podr√≠a ser √∫til para seguir scrapeando todo el sitio haciendo requests en ellos\n",
        "a_todos = soup.find_all('a', href=True)\n",
        "for a in a_todos:\n",
        "  print(f\"{a.text}: {a['href']}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "fVxip8F0iEYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd08db4-40f6-4282-e57d-ea5918866770"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Podemos tambien scrapear un tabla y traernos los feriados\n",
        "tabla_feriados = soup.find_all('td')\n",
        "\n",
        "# Con 'attr' podemos acceder a cualquier atributo de a etiqueta usando un diccionario\n",
        "dias = soup.find_all('td', attrs={'style':'width: 74px;'}) \n",
        "fechas = soup.find_all('td', attrs={'style':'width: 127px;'}) \n",
        "eventos = soup.find_all('td', attrs={'style':'width: 438px;'}) \n",
        "# print(tabla_feriados)\n",
        "\n",
        "for pos in range(len(dias)):\n",
        "  print(f\" Dia: {dias[pos].text.strip()} | fecha: {fechas[pos].text.strip()} | evento: {eventos[pos].text.strip()} \")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dia: Viernes | fecha: 1 de enero | evento: A√±o Nuevo \n",
            " Dia: Lunes | fecha: 15 de febrero | evento: Carnaval \n",
            " Dia: Martes | fecha: 16 de febrero | evento: Carnaval \n",
            " Dia: Mi√©rcoles | fecha: 24 de marzo | evento: D√≠a Nacional de la Memoria por la Verdad y la Justicia \n",
            " Dia: Domingo | fecha: 28 de marzo | evento: Pascua Jud√≠a \n",
            " Dia: Lunes | fecha: 29 de marzo | evento: Pascua Jud√≠a \n",
            " Dia: Jueves | fecha: 1 de abril | evento: Jueves Santo \n",
            " Dia: Viernes | fecha: 2 de abril | evento: Viernes Santo y D√≠a del Veterano y de los Ca√≠dos en la Guerra de Malvinas \n",
            " Dia: S√°bado | fecha: 3 de abril | evento: Pascua Jud√≠a \n",
            " Dia: Domingo | fecha: 4 de abril | evento: Pascua Jud√≠a \n",
            " Dia: S√°bado | fecha: 24 de abril | evento: D√≠a de Acci√≥n por la Tolerancia y el Respeto entre los Pueblos \n",
            " Dia: S√°bado | fecha: 1 de mayo | evento: D√≠a del Trabajador \n",
            " Dia: Jueves | fecha: 13 de mayo | evento: Fiesta de la Ruptura del Ayuno del Sagrado Mes de Ramad√°n \n",
            " Dia: Lunes | fecha: 24 de mayo | evento: Feriado con fines tur√≠sticos \n",
            " Dia: Martes | fecha: 25 de mayo | evento: D√≠a de la Revoluci√≥n de Mayo \n",
            " Dia: Domingo | fecha: 20 de junio | evento: Paso a la Inmortalidad del General Belgrano \n",
            " Dia: Lunes | fecha: 21 de junio | evento: Paso a la Inmortalidad del General G√ºemes \n",
            " Dia: Viernes | fecha: 9 de julio | evento: D√≠a de la Independencia \n",
            " Dia: Martes | fecha: 20 de julio | evento: Fiesta del Sacrificio \n",
            " Dia: Domingo | fecha: 8 de agosto | evento: A√±o Nuevo Isl√°mico \n",
            " Dia: Lunes | fecha: 16 de agosto | evento: Paso a la Inmortalidad del General San Mart√≠n \n",
            " Dia: Martes | fecha: 7 de septiembre | evento: A√±o Nuevo Jud√≠o \n",
            " Dia: Mi√©rcoles | fecha: 8 de septiembre | evento: A√±o Nuevo Jud√≠o \n",
            " Dia: Jueves | fecha: 16 de septiembre | evento: D√≠a del Perd√≥n \n",
            " Dia: Martes | fecha: 21 de septiembre | evento: D√≠a del Estudiante \n",
            " Dia: Viernes | fecha: 8 de octubre | evento: Feriado con fines tur√≠sticos \n",
            " Dia: Lunes | fecha: 11 de octubre | evento: D√≠a del Respeto a la Diversidad Cultural \n",
            " Dia: S√°bado | fecha: 20 de noviembre | evento: D√≠a de la Soberan√≠a Nacional \n",
            " Dia: Lunes | fecha: 22 de noviembre | evento: Feriado con fines tur√≠sticos \n",
            " Dia: Mi√©rcoles | fecha: 8 de diciembre | evento: Inmaculada Concepci√≥n de Mar√≠a \n",
            " Dia: S√°bado | fecha: 25 de diciembre | evento: Navidad \n"
          ]
        }
      ],
      "metadata": {
        "id": "HuLP4tVviG-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25e19ba-dbb7-4fa5-abce-3e5c15cf5b0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Obtener la √∫ltima actualizaci√≥n\n",
        "\n",
        "# Para exprresiones regulares\n",
        "import re\n",
        "# Para manejo de fechas\n",
        "from datetime import datetime\n",
        "\n",
        "def calendario_ultima_actualizacion():\n",
        "  ultima_actualizacion = soup.select('#post-256') \n",
        "  ultima_actualizacion= ultima_actualizacion[0].text\n",
        "  # Expresion que busca algo del estilo x/x/xxxx (donde x es un n√∫mero)\n",
        "  match = re.search(r'\\d/\\d/\\d{4}', ultima_actualizacion)\n",
        "  fecha = match.group()\n",
        "  fecha_datetime = datetime.strptime(fecha, '%d/%m/%Y').date()\n",
        "  return fecha_datetime\n",
        "\n",
        "fecha_datetime = calendario_ultima_actualizacion()\n",
        "print(f\"√öltima actualizaci√≥n: {fecha_datetime.day}/{fecha_datetime.month}/{fecha_datetime.year} \")\n",
        "\n",
        "# Ahora podemos crear un programa que nos avise si se actualiza el calendario\n",
        "\n",
        "def avisame_si_actualizaron(fecha_previa):\n",
        "  # Esto esta solo para cortar el while\n",
        "  contador_anti_explosion = 0\n",
        "  fecha_datetime = calendario_ultima_actualizacion()\n",
        "  fecha_previa = f'{fecha_previa.day}/{fecha_previa.month}/{fecha_previa.year}'\n",
        "\n",
        "  while True:\n",
        "    fecha_actual = f'{fecha_datetime.day}/{fecha_datetime.month}/{fecha_datetime.year}'\n",
        "\n",
        "    if fecha_actual == fecha_previa and contador_anti_explosion < 5:\n",
        "      url_base = 'https://exactas.uba.ar/'\n",
        "      endpoint_calendario = 'calendario-academico/'\n",
        "      html_obtenido = requests.get(url_base + endpoint_calendario)\n",
        "      soup = BeautifulSoup(html_obtenido.text, \"html.parser\")\n",
        "      fecha_datetime = calendario_ultima_actualizacion()\n",
        "      print('igual')\n",
        "      contador_anti_explosion += 1\n",
        "\n",
        "    else:\n",
        "      print('Actualizaron!! \\n')\n",
        "      from google.colab import output\n",
        "      print(f'Se actualizo el: {fecha_datetime.day}/{fecha_datetime.month}/{fecha_datetime.year}')\n",
        "      # Para que ladre un perro avisandonos \n",
        "      output.eval_js('new Audio(\"https://assets.mixkit.co/sfx/preview/mixkit-dog-barking-twice-1.mp3\").play()')\n",
        "      break\n",
        "\n",
        "fecha_previa = calendario_ultima_actualizacion()\n",
        "\n",
        "# Corremos la funcion\n",
        "avisame_si_actualizaron(fecha_previa)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√öltima actualizaci√≥n: 3/6/2021 \n",
            "igual\n",
            "igual\n",
            "igual\n",
            "igual\n",
            "igual\n",
            "Actualizaron!! \n",
            "\n",
            "Se actualizo el: 3/6/2021\n"
          ]
        }
      ],
      "metadata": {
        "id": "8H_4LzMqk218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "507ac860-6d4c-467a-c2d5-338f65da6d19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Ejercicio</font>\n",
        "\n",
        "* Generar diccionario cuyas claves sean los nombres de las carreras de grado vigentes en Exactas y sus valores el link asociado a cada una de ellas. https://exactas.uba.ar/ensenanza/carreras-de-grado/\n",
        "\n",
        "**¬°A trabajar!**\n",
        "\n",
        "<img src=\"https://img.icons8.com/ios/452/spade.png\" width=\"80\" height=\"auto\"/>\n"
      ],
      "metadata": {
        "id": "wfxO0wO5ubqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Resoluci√≥n\n",
        "\n",
        "url = 'https://exactas.uba.ar/ensenanza/carreras-de-grado/'\n",
        "\n",
        "html_obtenido = requests.get(url)\n",
        "soup = BeautifulSoup(html_obtenido.text, \"html.parser\")\n",
        "\n",
        "titulos = soup.find_all('h2', class_ = 'titulo')\n",
        "uls = soup.find('ul', class_='listado carreras grado')\n",
        "lis = uls.find_all('li')\n",
        "\n",
        "lista_titulos = []\n",
        "lista_paginas = []\n",
        "\n",
        "for li in lis:\n",
        "  a = li.find('a', href=True)\n",
        "  lista_paginas.append(a['href'])\n",
        "  titulo = li.find('h2')\n",
        "  lista_titulos.append(titulo.text)\n",
        "\n",
        "diccionario_exactas = dict(zip(lista_titulos, lista_paginas))\n",
        "display(diccionario_exactas)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(diccionario_exactas.values(), index=list(diccionario_exactas.keys()), columns=['url'])\n",
        "display(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WCmp2i2a5nqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejemplo 2: Cortazar"
      ],
      "metadata": {
        "id": "GNkLPmfyA0aJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Creo carpeta donde voy a guardar los cuentos\n",
        "!mkdir -p multimedia/cortazar/\n",
        "\n",
        "import re\n",
        "codigo_html_crudo = codigo_html('http://ciudadseva.com/autor/julio-cortazar/cuentos/')\n",
        "\n",
        "regla_para_url_de_un_cuento = r'(https://ciudadseva.com/texto/.+/)'\n",
        "\n",
        "for url_de_un_cuento in re.findall(regla_para_url_de_un_cuento, codigo_html_crudo):\n",
        "    codigo_html_interpretado = BeautifulSoup(codigo_html(url_de_un_cuento), 'html.parser')\n",
        "    elem = codigo_html_interpretado.find(\"div\", { \"class\" : \"text-justify\" })\n",
        "    cuento = elem.text\n",
        "    \n",
        "    # Asi podemos guardar los resultados\n",
        "    nombre_del_archivo = url_de_un_cuento.split('/')[-2]\n",
        "    with open (f\"multimedia/cortazar/{nombre_del_archivo}.txt\", 'w') as out:\n",
        "        print(f'Guardando {nombre_del_archivo}')\n",
        "        out.write(cuento)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vgHsDKT6A0aJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pr√°ctica: Mercadolibre"
      ],
      "metadata": {
        "id": "pVzzO2iXA0aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'> Ejercicio </font>\n",
        "\n",
        "Descarg√° y calcul√° el promedio de los precios que aparecen en la primer p√°gina de mercado libre al buscar gibson"
      ],
      "metadata": {
        "id": "oUGQfsEwSMXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Resoluci√≥n\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def precios_gibson():\n",
        "    url = \"https://listado.mercadolibre.com.ar/gibson\"\n",
        "    soup = BeautifulSoup(codigo_html(url), 'html.parser')\n",
        "    prices = []\n",
        "    # COMPLETAR\n",
        "    for precio in soup.find_all('span', class_ = 'price-tag-fraction'):\n",
        "      precio = precio.text\n",
        "      print(precio)\n",
        "      prices.append(int(precio.replace('.','')))\n",
        "\n",
        "    return prices\n",
        "\n",
        "precios = precios_gibson()\n",
        "print(precios)\n",
        "import numpy as np\n",
        "print(f\"El precio promedio es {int(np.nanmean(precios))}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "VqFNpq-tA0aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando cookies üç™\n",
        "\n",
        "Las [*cookies*](https://es.wikipedia.org/wiki/Cookie_(inform%C3%A1tica)) son bloques de datos creados por un servidor con informaci√≥n enviada por un sitio web y almacenada en el navegador del usuario, de manera que el sitio web puede consultar la actividad previa del navegador. \n",
        "\n",
        "Sus principales funciones son:\n",
        "\n",
        "* Recordar accesos para saber si ya se ha visitado la p√°gina (ejemplo: cuando nos *loggeamos* se guardan cookies).\n",
        "\n",
        "* Conocer informaci√≥n sobre los h√°bitos de navegaci√≥n.\n",
        "\n",
        "Tambi√©n hay otro tipo de informaci√≥n que se guarda en algunas p√°ginas que son las *sessions*, b√°sicamente es un dato similar a una cookie pero que se guarda en el servidor en lugar de hacerlo en el cliente (nuestro navegador).\n",
        "\n",
        "Para algunos proyectos de *web scraping* puede ser √∫til interactuar con ellas."
      ],
      "metadata": {
        "id": "QAOTO4i5VFqg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import requests\n",
        "from IPython.display import HTML\n",
        "\n",
        "response = requests.get('https://www.kaggle.com/')\n",
        "\n",
        "# Obtenemos el atributo cookies\n",
        "cookies = response.cookies\n",
        "print(type(cookies))\n",
        "print(cookies[:100])"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9ToN5a0JswW",
        "outputId": "2c084848-3908-4fd6-eb5d-695a7949a11a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "# iteramos sobre las cookies en el cookie jar\n",
        "for cookie in cookies:\n",
        "  print('domain: ' ,cookie.domain)\n",
        "  print('name: ', cookie.name)\n",
        "  print('value: ', cookie.value)\n",
        "  print('------------------------')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "domain:  www.kaggle.com\n",
            "name:  CLIENT-TOKEN\n",
            "value:  eyJhbGciOiJub25lIiwidHlwIjoiSldUIn0.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.\n",
            "------------------------\n",
            "domain:  www.kaggle.com\n",
            "name:  CSRF-TOKEN\n",
            "value:  CfDJ8LdUzqlsSWBPr4Ce3rb9VL8ZAvOxaIXRSNBQa4LDjb5ajR4LbGFqXlsl1yHPrV3-4aCQ9zQJsspddpRP-u_1VB_mh9mhEOlCPyuF7C5tzGyPmO5005DM4S1KXCGe9Su9jvie5MPmMzGhJmDcYEyVtVM\n",
            "------------------------\n",
            "domain:  www.kaggle.com\n",
            "name:  GCLB\n",
            "value:  CJXT5cu1xKWCJQ\n",
            "------------------------\n",
            "domain:  www.kaggle.com\n",
            "name:  XSRF-TOKEN\n",
            "value:  CfDJ8LdUzqlsSWBPr4Ce3rb9VL8ngJLc_OhYwFZd-CQxmyZnGEOVwga5FCx9v39D0ziPmJK1gV_aBrZQjwCPaxqiYKpKgUbfyo9OoBPYTjY5jbRdrq2xMpa8bso__32bhpu0nD7dsPqMG_tdxfdClHPO1Rs\n",
            "------------------------\n",
            "domain:  www.kaggle.com\n",
            "name:  ka_sessionid\n",
            "value:  f87ccffa4c82c115675514b69dac9406\n",
            "------------------------\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpPlQ7brg1__",
        "outputId": "4362a5f2-3ca0-4ce7-c62f-39c4a11f2725"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos enviar cookies junto con nuestro request. Esto puede ser util para ciertos sitios que usan la ausencia de cookies como criterio para bloquear el acceso."
      ],
      "metadata": {
        "id": "Jziv4geBHFmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "source": [
        "url = 'https://www.kaggle.com/'\n",
        "\n",
        "mis_cookies = {\n",
        "    'name':'mi nombre',\n",
        "    'password':'superSeguro1234'\n",
        "    }\n",
        "print(mis_cookies)\n",
        "\n",
        "# Mando mis propias cookies\n",
        "respuesta = requests.get(url, cookies=mis_cookies)\n",
        "\n",
        "respuesta"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'mi nombre', 'password': 'superSeguro1234'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CM8ZB9cG0T8",
        "outputId": "8a7c0e39-1b4f-448f-c93d-6a4884934787"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¬øEntonces me puedo descargar todo internet?\n",
        "\n",
        "En la pr√≥ximas clases veremos algunas limitaciones de este m√©todo y sus alternativas. Mas all√° de eso es importante ponerse de vez en cuando en el lugar del sitio del cual estamos descargando datos.\n",
        "\n",
        "\n",
        "Muchas veces las p√°ginas web obtienen sus ingresos a partir del uso de usuarios tradicionales (humanos) pero no de los scrapers (m√°quinas). Por lo que estos no generan ganancias al sitio y encima pueden causar congesti√≥n en los servidores (Pudiendo causar incluso la rotura del sitio similar a lo que pasa con los [ataques DDOS](https://es.wikipedia.org/wiki/Ataque_de_denegaci%C3%B3n_de_servicio)).\n",
        "\n",
        "Por esta raz√≥n los sitios webs suelen tener una pagina [/robots.txt](https://es.wikipedia.org/wiki/Est%C3%A1ndar_de_exclusi%C3%B3n_de_robots) donde especifican que tipo de scrapeo prefieren evitar para poder mantener su sitio funcionando correctamente sin problemas.\n",
        "\n",
        "Pueden ver, como ejemplos:\n",
        "\n",
        "- https://www.google.com/robots.txt\n",
        "- https://en.wikipedia.org/robots.txt"
      ],
      "metadata": {
        "id": "w52rjRnsA0aK"
      }
    }
  ]
}